{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sLaWNa64xAz"
   },
   "source": [
    "# 한국어 BERT 모델 학습\n",
    "\n",
    "> 작성자      \n",
    "```\n",
    "* 김성현 (bananaband657@gmail.com)  \n",
    "1기 멘토\n",
    "김바다 (qkek983@gmail.com)\n",
    "박상희 (parksanghee0103@gmail.com)  \n",
    "이정우 (jungwoo.l2.rs@gmail.com)\n",
    "2기 멘토\n",
    "박상희 (parksanghee0103@gmail.com)  \n",
    "이정우 (jungwoo.l2.rs@gmail.com)\n",
    "이녕우 (leenw2@gmail.com)\n",
    "박채훈 (qkrcogns2222@gmail.com)\n",
    "3, 4, 5기 멘토\n",
    "이녕우 (leenw2@gmail.com)\n",
    "박채훈 (qkrcogns2222@gmail.com)\n",
    "```\n",
    "[CC BY-NC-ND](https://creativecommons.org/licenses/by-nc-nd/2.0/kr/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNn_C9oG8pft"
   },
   "source": [
    "###**콘텐츠 라이선스**\n",
    "\n",
    "<font color='red'><b>**WARNING**</b></font> : **본 교육 콘텐츠의 지식재산권은 재단법인 네이버커넥트에 귀속됩니다. 본 콘텐츠를 어떠한 경로로든 외부로 유출 및 수정하는 행위를 엄격히 금합니다.** 다만, 비영리적 교육 및 연구활동에 한정되어 사용할 수 있으나 재단의 허락을 받아야 합니다. 이를 위반하는 경우, 관련 법률에 따라 책임을 질 수 있습니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItbY5oL-V5DW"
   },
   "source": [
    "## 학습 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wYqha-sZ4KAS"
   },
   "outputs": [],
   "source": [
    "# !mkdir my_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQ7EGqFiJ9EH"
   },
   "source": [
    "실습을 위해서 아주 작은 wiki 코퍼스를 가져와보겠습니다 :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2828,
     "status": "ok",
     "timestamp": 1680014452544,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "_z0pedAT44Gl",
    "outputId": "64b0416d-878d-443b-bb35-d1c51ce319f0"
   },
   "outputs": [],
   "source": [
    "# !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" > /dev/null\n",
    "# !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" -o my_data/wiki_20190620_small.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYXccFQeKB7F"
   },
   "source": [
    "물론 한국어 wiki 전체 코퍼스도 공유드릴게요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13480,
     "status": "ok",
     "timestamp": 1680014466022,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "Eq_xSrYBKFuv",
    "outputId": "4b144117-c87d-4eae-ea5e-65ee684e8ed9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  476M  100  476M    0     0  10.1M      0  0:00:46  0:00:46 --:--:-- 15.1M\n"
     ]
    }
   ],
   "source": [
    "# !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1_F5fziHjUM-jKr5Pwcx1we6g_J2o70kZ\" > /dev/null\n",
    "# !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1_F5fziHjUM-jKr5Pwcx1we6g_J2o70kZ\" -o my_data/wiki_20190620.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gI3vg0Q2KW6K"
   },
   "source": [
    "이제 본격적으로 BERT 학습을 위한 준비를 해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12696,
     "status": "ok",
     "timestamp": 1680014478711,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "id8FcYRa48Gc",
    "outputId": "4e2d9d00-8847-4ced-d853-988e156f8926"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIEUFdT4WIqd"
   },
   "source": [
    "## Tokenizer 만들기\n",
    "\n",
    "corpus를 이용하 wordPiece tokenizer를 만들어보겠습니다.   \n",
    "이제는 익숙하시죠? :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFTgG7yv4_n7"
   },
   "outputs": [],
   "source": [
    "# !mkdir wordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 491,
     "status": "ok",
     "timestamp": 1680014634466,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "MGFiNRpV5Ban",
    "outputId": "5f1a0351-69bd-4e73-95f3-20b5f57ce0cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['wordPieceTokenizer/my_tokenizer-vocab.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# Initialize an empty tokenizer\n",
    "wp_tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,   # [\"이순신\", \"##은\", \" \", \"조선\"] ->  [\"이순신\", \"##은\", \"조선\"]\n",
    "    # if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "    handle_chinese_chars=True,  # 한자는 모두 char 단위로 쪼게버립니다.\n",
    "    strip_accents=False,    # True: [YehHamza] -> [Yep, Hamza]\n",
    "    lowercase=False,    # Hello -> hello\n",
    ")\n",
    "\n",
    "# And then train\n",
    "wp_tokenizer.train(\n",
    "    files=\"my_data/wiki_20190620_small.txt\",\n",
    "    vocab_size=20000,   # vocab size 를 지정해줄 수 있습니다.\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    wordpieces_prefix=\"##\"\n",
    ")\n",
    "\n",
    "# Save the files\n",
    "wp_tokenizer.save_model(\"wordPieceTokenizer\", \"my_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1680014635735,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "ngd6Z3aJ57VR",
    "outputId": "839048db-6d13-4c7c-9a31-0a1c32cf5d63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "print(wp_tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1680014636228,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "fnPe0_4a5C-a",
    "outputId": "a4ff392f-3f46-4f11-847f-dc9dde165baf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '##순', '##신은', '조선', '중', '##기의', '무신', '##이다', '.']\n",
      "[706, 1504, 7583, 2000, 754, 2605, 13160, 1895, 16]\n"
     ]
    }
   ],
   "source": [
    "text = \"이순신은 조선 중기의 무신이다.\"\n",
    "tokenized_text = wp_tokenizer.encode(text)\n",
    "print(tokenized_text.tokens)\n",
    "print(tokenized_text.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLE2ZQEp5GBq"
   },
   "source": [
    "## BERT 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2468,
     "status": "ok",
     "timestamp": 1680014640486,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "lrihNAMK5EsC",
    "outputId": "28b25677-5707-4a2b-df70-6b3bbcbf681e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zye2l6fP6igt"
   },
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForPreTraining, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnx2l-RVZOfk"
   },
   "source": [
    "위 과정에서 생성한 tokenizer를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0Wtm3umi6geG"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast(\n",
    "    vocab_file='./wordPieceTokenizer/my_tokenizer-vocab.txt',\n",
    "    max_len=128,\n",
    "    do_lower_case=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1680014642130,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "CvjurG9_bWND",
    "outputId": "a2ad1fb4-fe3f-43a1-efd3-aea993e273bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[', 'M', '##AS', '##K', ']', '조선', '중', '##기의', '무신', '##이다', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"뷁은 [MASK] 조선 중기의 무신이다.\")) # 아직 [MASK] 토큰을 special token으로 처리하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 질문\n",
    "\n",
    "* 아래 셀 코드에 tokenizer.special_tokens_map에 [CLS]랑 [MASK]가 잘 있는데 왜 tokenizing을 못하는지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1484,
     "status": "ok",
     "timestamp": 1680014643611,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "Y8rzC1StJHWZ",
    "outputId": "fbd23c85-2a5e-431b-d1a6-13e2bdaafeb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "['이', '##순', '##신은', '[', 'C', '##L', '##S', ']', '중', '##기의', '무신', '##이다', '.']\n",
      "{'input_ids': [2, 706, 1504, 7583, 59, 35, 1566, 1276, 61, 754, 2605, 13160, 1895, 16, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[CLS] 이순신은 [ MASK ] 중기의 무신이다. [SEP]\n",
      "1492 [CLS]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.special_tokens_map) # 여기에 [CLS]랑 [MASK]가 잘 있는데 왜 tokenizing을 못하지?\n",
    "print(tokenizer.tokenize(\"이순신은 [CLS] 중기의 무신이다.\"))\n",
    "print(tokenizer(\"이순신은 [CLS] 중기의 무신이다.\"))\n",
    "encoded_input = tokenizer(\"이순신은 [MASK] 중기의 무신이다.\")\n",
    "print(tokenizer.decode(encoded_input[\"input_ids\"]))\n",
    "for i,j in enumerate(tokenizer.vocab):\n",
    "  if j == '[CLS]':\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1680014643611,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "gpmBH_F5XYxf",
    "outputId": "5bc7295d-b811-4c98-967e-001224385e30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '##순', '##신은', '[MASK]', '중', '##기의', '무신', '##이다', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'mask_token':'[MASK]'})\n",
    "print(tokenizer.tokenize(\"이순신은 [MASK] 중기의 무신이다.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2423,
     "status": "ok",
     "timestamp": 1680014646032,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "xH7JzbMv5xBt",
    "outputId": "b2d7d30a-53f7-49c9-9e1f-f303223db407"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101720098"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForPreTraining\n",
    "\n",
    "config = BertConfig(    # https://huggingface.co/transformers/model_doc/bert.html#bertconfig\n",
    "    vocab_size=20000,\n",
    "    # hidden_size=512,\n",
    "    # num_hidden_layers=12,    # layer num\n",
    "    # num_attention_heads=8,    # transformer attention head number\n",
    "    # intermediate_size=3072,   # transformer 내에 있는 feed-forward network의 dimension size\n",
    "    # hidden_act=\"gelu\",\n",
    "    # hidden_dropout_prob=0.1,\n",
    "    # attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=128,    # embedding size 최대 몇 token까지 input으로 사용할 것인지 지정. 장문이라면 1024까지도 가능\n",
    "    # type_vocab_size=2,    # token type ids의 범위 (BERT는 segmentA, segmentB로 2종류)\n",
    "    # pad_token_id=0,\n",
    "    # position_embedding_type=\"absolute\"\n",
    ")\n",
    "\n",
    "model = BertForPreTraining(config=config)\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Sn7SnD0VjUqi"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_Ka2mjYRuKzo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from typing import Dict, List, Optional\n",
    "import os\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from filelock import FileLock\n",
    "\n",
    "from transformers.utils import logging\n",
    "\n",
    "logger = logging.get_logger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNGbscSSZzLv"
   },
   "source": [
    "BERT의 학습에서 가장 중요한 요소 중 하나는, 다음 문장을 예측하는 것 입니다.   \n",
    "아래 코드를 통해 다음 문장을 예측하기 위한 dataset을 구성합니다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3ekdXKzlt2qO"
   },
   "outputs": [],
   "source": [
    "class TextDatasetForNextSentencePrediction(Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach soon.\n",
    "    구글의 BERT GitHub 코드를 그대로 가져옴.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        file_path: str,\n",
    "        block_size: int,\n",
    "        overwrite_cache=False,\n",
    "        short_seq_probability=0.1,\n",
    "        nsp_probability=0.5,\n",
    "    ):\n",
    "        # 여기 부분은 학습 데이터를 caching하는 부분입니다 :-)\n",
    "        assert os.path.isfile(file_path), f\"Input file path {file_path} not found\"\n",
    "\n",
    "        self.block_size = block_size - tokenizer.num_special_tokens_to_add(pair=True)\n",
    "        self.short_seq_probability = short_seq_probability\n",
    "        self.nsp_probability = nsp_probability\n",
    "\n",
    "        directory, filename = os.path.split(file_path)\n",
    "        cached_features_file = os.path.join(\n",
    "            directory,\n",
    "            \"cached_nsp_{}_{}_{}\".format(\n",
    "                tokenizer.__class__.__name__,\n",
    "                str(block_size),\n",
    "                filename,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "\n",
    "        # Input file format:\n",
    "        # (1) One sentence per line. These should ideally be actual sentences, not\n",
    "        # entire paragraphs or arbitrary spans of text. (Because we use the\n",
    "        # sentence boundaries for the \"next sentence prediction\" task).\n",
    "        # (2) Blank lines between documents. Document boundaries are needed so\n",
    "        # that the \"next sentence prediction\" task doesn't span between documents.\n",
    "        #\n",
    "        # Example:\n",
    "        # I am very happy.\n",
    "        # Here is the second sentence.\n",
    "        #\n",
    "        # A new document.\n",
    "\n",
    "        with FileLock(lock_path):\n",
    "            if os.path.exists(cached_features_file) and not overwrite_cache:\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"rb\") as handle:\n",
    "                    self.examples = pickle.load(handle)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
    "                )\n",
    "            else:\n",
    "                logger.info(f\"Creating features from dataset file at {directory}\")\n",
    "                ##########################################################################\n",
    "                # 여기서부터 본격적으로 dataset을 만듭니다.\n",
    "                self.documents = [[]] # document 단위로(서로 다른 document의 마지막 문장 - 첫 문장이 붙은 경우 틀린 NSP 학습)\n",
    "                with open(file_path, encoding=\"utf-8\") as f:\n",
    "                    while True: # 일단 문장을 읽고\n",
    "                        line = f.readline()\n",
    "                        if not line:\n",
    "                            break\n",
    "                        line = line.strip()\n",
    "\n",
    "                        # 이중 띄어쓰기가 발견된다면, 나왔던 문장들을 모아 하나의 문서로 묶어버립니다.\n",
    "                        # 즉, 문단 단위로 데이터를 저장합니다.\n",
    "                        if not line and len(self.documents[-1]) != 0:\n",
    "                            self.documents.append([])\n",
    "                        tokens = tokenizer.tokenize(line)\n",
    "                        tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "                        if tokens:\n",
    "                            self.documents[-1].append(tokens)\n",
    "                ###########################################################################\n",
    "                # 이제 코퍼스 전체를 읽고, 문서 데이터를 생성했습니다! :-)\n",
    "                logger.info(f\"Creating examples from {len(self.documents)} documents.\") # 코퍼스를 다 읽었다는 log 남김\n",
    "                self.examples = []\n",
    "                # 본격적으로 학습을 위한 데이터로 변형시켜볼까요?\n",
    "                for doc_index, document in enumerate(self.documents):\n",
    "                    self.create_examples_from_document(document, doc_index) # 함수로 가봅시다.\n",
    "\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"wb\") as handle:\n",
    "                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                logger.info(\n",
    "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                )\n",
    "\n",
    "    def create_examples_from_document(self, document: List[List[int]], doc_index: int):\n",
    "        \"\"\"Creates examples for a single document.\"\"\"\n",
    "        # 문장의 앞, 뒤에 [CLS], [SEP] token이 부착되기 때문에, 내가 지정한 size에서 2 만큼 빼줍니다.\n",
    "        # 예를 들어 128 token 만큼만 학습 가능한 model을 선언했다면, 학습 데이터로부터는 최대 126 token만 가져오게 됩니다.\n",
    "        max_num_tokens = self.block_size - self.tokenizer.num_special_tokens_to_add(pair=True)\n",
    "\n",
    "        # We *usually* want to fill up the entire sequence since we are padding\n",
    "        # to `block_size` anyways, so short sequences are generally wasted\n",
    "        # computation. However, we *sometimes*\n",
    "        # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
    "        # sequences to minimize the mismatch between pretraining and fine-tuning.\n",
    "        # The `target_seq_length` is just a rough target however, whereas\n",
    "        # `block_size` is a hard limit.\n",
    "\n",
    "        # 여기가 재밌는 부분인데요!\n",
    "        # 위에서 설명했듯이, 학습 데이터는 126 token(128-2)을 채워서 만들어지는게 목적입니다.\n",
    "        # 하지만 나중에 BERT를 사용할 때, 126 token 이내의 짧은 문장을 테스트하는 경우도 분명 많을 것입니다.\n",
    "        # 오히려 모든 토큰 개수를 꽉 채워서만 학습하면, 짧은 문장에 대해 학습하는 능력이 떨어지는 경우도 발생합니다.\n",
    "        # 그래서 short_seq_probability 만큼의 데이터에서는 2-126 사이의 random 값으로 학습 데이터를 만들게 됩니다.\n",
    "        target_seq_length = max_num_tokens\n",
    "        if random.random() < self.short_seq_probability: # short_seq_probability에 따라서\n",
    "            target_seq_length = random.randint(2, max_num_tokens) # 2~max_num_tokens 사이의 임의 정수로 시퀀스 길이를 제한\n",
    "\n",
    "        current_chunk = []  # a buffer stored current working segments\n",
    "        current_length = 0\n",
    "        i = 0\n",
    "\n",
    "        # 데이터 구축의 단위는 document 입니다\n",
    "        # 이 때, 무조건 문장_1[SEP]문장_2 이렇게 만들어지는 것이 아니라,\n",
    "        # 126 token을 꽉 채울 수 있게 문장_1+문장_2[SEP]문장_3+문장_4 형태로 만들어질 수 있습니다.\n",
    "        while i < len(document):\n",
    "            segment = document[i]\n",
    "            current_chunk.append(segment)\n",
    "            current_length += len(segment)\n",
    "            if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "                if current_chunk:\n",
    "                    # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "                    # (first) sentence.\n",
    "                    a_end = 1\n",
    "                    # 여기서 문장_1+문장_2 가 이루어졌을 때, 길이를 random하게 짤라버립니다 :-)\n",
    "                    if len(current_chunk) >= 2:\n",
    "                        a_end = random.randint(1, len(current_chunk) - 1)\n",
    "                    tokens_a = []\n",
    "                    for j in range(a_end):\n",
    "                        tokens_a.extend(current_chunk[j])\n",
    "                    # 이제 [SEP] 뒷 부분인 segmentB를 살펴볼까요?\n",
    "                    tokens_b = []\n",
    "                    #######################################################################\n",
    "                    # Next Sentence Prediction (NSP)\n",
    "                    # 50%의 확률로 랜덤하게 다른 문장을 선택하거나, 다음 문장을 학습데이터로 만듭니다.\n",
    "                    if len(current_chunk) == 1 or random.random() < self.nsp_probability:\n",
    "                        is_random_next = True\n",
    "                        target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "                        # This should rarely go for more than one iteration for large\n",
    "                        # corpora. However, just to be careful, we try to make sure that\n",
    "                        # the random document is not the same as the document\n",
    "                        # we're processing.\n",
    "                        for _ in range(10):\n",
    "                            random_document_index = random.randint(0, len(self.documents) - 1)\n",
    "                            if random_document_index != doc_index:\n",
    "                                break\n",
    "                        # 여기서 랜덤하게 선택합니다 :-)\n",
    "                        random_document = self.documents[random_document_index]\n",
    "                        random_start = random.randint(0, len(random_document) - 1)\n",
    "                        for j in range(random_start, len(random_document)):\n",
    "                            tokens_b.extend(random_document[j])\n",
    "                            if len(tokens_b) >= target_b_length:\n",
    "                                break\n",
    "                        # We didn't actually use these segments so we \"put them back\" so\n",
    "                        # they don't go to waste.\n",
    "                        num_unused_segments = len(current_chunk) - a_end\n",
    "                        i -= num_unused_segments\n",
    "                    # Actual next\n",
    "                    else:\n",
    "                        is_random_next = False\n",
    "                        for j in range(a_end, len(current_chunk)):\n",
    "                            tokens_b.extend(current_chunk[j])\n",
    "                    ########################################################################\n",
    "\n",
    "                    # 이제 126 token을 넘는다면 truncation을 해야합니다.\n",
    "                    # 이 때, 126 token 이내로 들어온다면 행위를 멈추고,\n",
    "                    # 만약 126 token을 넘는다면, segmentA와 segmentB에서 랜덤하게 하나씩 제거합니다.\n",
    "                    def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens):\n",
    "                        \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "                        while True:\n",
    "                            total_length = len(tokens_a) + len(tokens_b)\n",
    "                            if total_length <= max_num_tokens:\n",
    "                                break\n",
    "                            trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "                            assert len(trunc_tokens) >= 1\n",
    "                            # We want to sometimes truncate from the front and sometimes from the\n",
    "                            # back to add more randomness and avoid biases.\n",
    "                            if random.random() < 0.5:\n",
    "                                del trunc_tokens[0] # 절반의 확률로 맨 앞 토큰 제거\n",
    "                            else:\n",
    "                                trunc_tokens.pop() # 절반의 확률로 맨 뒤 토큰 제거\n",
    "\n",
    "                    truncate_seq_pair(tokens_a, tokens_b, max_num_tokens)\n",
    "\n",
    "                    assert len(tokens_a) >= 1\n",
    "                    assert len(tokens_b) >= 1\n",
    "\n",
    "                    # add special tokens\n",
    "                    input_ids = self.tokenizer.build_inputs_with_special_tokens(tokens_a, tokens_b)\n",
    "                    # add token type ids, 0 for sentence a, 1 for sentence b\n",
    "                    token_type_ids = self.tokenizer.create_token_type_ids_from_sequences(tokens_a, tokens_b)\n",
    "                    \n",
    "                    # 드디어 아래 항목에 대한 데이터셋이 만들어졌습니다! :-)\n",
    "                    # 즉, segmentA[SEP]segmentB, [0, 0, .., 0, 1, 1, ..., 1], NSP 데이터가 만들어진 것입니다 :-)\n",
    "                    # 그럼 다음은.. 이 데이터에 [MASK] 를 씌워야겠죠?\n",
    "                    example = {\n",
    "                        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                        \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                        \"next_sentence_label\": torch.tensor(1 if is_random_next else 0, dtype=torch.long),\n",
    "                    }\n",
    "\n",
    "                    self.examples.append(example)\n",
    "\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.examples[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1886,
     "status": "ok",
     "timestamp": 1680014649188,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "cvlBlEmF52ed",
    "outputId": "c3410738-3047-40bf-bc2b-196fac154b0c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (137 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "dataset = TextDatasetForNextSentencePrediction(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='./my_data/wiki_20190620_small.txt',\n",
    "    block_size=128,\n",
    "    overwrite_cache=False,\n",
    "    short_seq_probability=0.1,\n",
    "    nsp_probability=0.5,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(    # [MASK] 를 씌우는 것은 저희가 구현하지 않아도 됩니다! :-)\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15 # mlm=True로 masked language modeling 구현\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jt52m1cUbCIl"
   },
   "source": [
    "이렇게 만들어진 학습 데이터를 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1680014649579,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "sZGsKlvDQBBI",
    "outputId": "9585ac39-6d2a-4767-a8df-cd6332209b47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([    2,  9437,  2489,  2428,  2780,  1968,  5379,  3120,  1940,  2407,\n",
      "           16,  5497, 10311, 16247,   553,  1163,   823,  1284,  1246,   931,\n",
      "        16493, 12286,  1035,  3666,    16,  6531,  8936,  1023,  2677,  1906,\n",
      "           16,   175,   985,  4020,  1007,  8598,   728,  1200,    93,  7743,\n",
      "           93, 10415,  1017, 18365,  3483, 18886,    16,  6437,  1968,  4020,\n",
      "          278,  3363,   658,  1143,  2105,  1933, 17661,    93,   438,  1046,\n",
      "         2137,     1,  2023,  4086, 17980,    16,  2062,   495,  2735,     5,\n",
      "        17661, 12973,     5,   378,  7719,    16,  4186,  6531,   750,   542,\n",
      "         1008,  2795,  4860,  5152,  4783,   175, 11843, 15561,   655,  2786,\n",
      "         9395,  1945,  2370,  2895,  2053,    14,  9875,  6531,   750,   762,\n",
      "         1075,  6461,  5152,  2823,  3950,  6531,   750,   762,  2092,  6205,\n",
      "            3,    21,    95,    21,  2203,  5645,  4194,   701,  3095, 16208,\n",
      "         5307,  1971,  4019,    16,     3]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1]), 'next_sentence_label': tensor(1)}\n"
     ]
    }
   ],
   "source": [
    "for example in dataset.examples[0:1]:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdwJsZDdWpni"
   },
   "source": [
    "[MASK]를 부착하는 data collator의 역할은 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1680014650913,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "QpeY7jjMWsTW",
    "outputId": "6f2ad602-bb10-4278-90dd-6c729600ab3c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2,  9437,  2489,  ...,  4019,    16,     3],\n",
      "        [    2,   728, 16247,  ...,  1899,    16,     3],\n",
      "        [    2, 10311,  4632,  ...,     4,     4,     3],\n",
      "        ...,\n",
      "        [    2, 17805,  3870,  ..., 16791,  1887,     3],\n",
      "        [    2, 18359,   958,  ...,  1017,  3000,     3],\n",
      "        [    2, 12576, 13168,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'next_sentence_label': tensor([1, 0, 0,  ..., 1, 1, 0]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  2030,    16,  -100],\n",
      "        ...,\n",
      "        [ -100, 10150,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100]])}\n"
     ]
    }
   ],
   "source": [
    "print(data_collator(dataset.examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 454,
     "status": "ok",
     "timestamp": 1680014652163,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "GMMUbKr_WvhG",
    "outputId": "756e16be-ea1c-4b0c-f8ee-27bb8c8fa857"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    2,  9437,  2489,  2428,  2780,  1968,  5379,     4,  1940,  2407,\n",
      "           16,  5497, 10311, 16247,   553,  1163,     4,  1284,  1246,   931,\n",
      "        16493, 12286,  1035,  3666,    16,  6531,  5979,     4,  2677,  1906,\n",
      "           16,   175,   985,  4020,  1007,  8598,   728,  1200,    93,  7743,\n",
      "           93, 10415,  1017, 18365,  3483, 18886,    16,  6437,  1968,  4020,\n",
      "          278,  3363,   658,     4,  2105,     4, 17661,    93,   438, 11045,\n",
      "         2137,     1,  2023,  4086, 17980,    16, 13619,     4,     4,     5,\n",
      "        17661, 12973,     5,   378,  7719,     4,  4186,  6531,   750,   542,\n",
      "         1008,  2795,  4860,     4,  4783,   175, 11843, 18291,     4,  2786,\n",
      "         9395,  1945,     4,  2895,  2053,    14,  9875,  6531,   750,   762,\n",
      "         1075,  6461,  5152,  2823,  3950,  6531,   750,   762,  2092,  6205,\n",
      "            3,    21,    95,    21,  2203,     4,  4194,   701,  3095,     4,\n",
      "         5307,  1971,  4019,    16,     3])\n"
     ]
    }
   ],
   "source": [
    "print(data_collator(dataset.examples)['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWbI_dEJeguW"
   },
   "source": [
    "collator 함수가 실행되면, 입력 문장에 [MASK] 가 부착됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "executionInfo": {
     "elapsed": 547,
     "status": "ok",
     "timestamp": 1680014653072,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "5lzsg6RIVIIN",
    "outputId": "482d3937-9c8d-4a02-c0f4-c0142a9fa3cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 주니 [MASK] 민주당 [MASK] 미국 39번째 [MASK] 이다. [MASK] 카터는 조지아주 [MASK]터 카운티 [MASK]학에 마을에서 태어났다. 조지아 [MASK]를 졸업하였다. 그 [MASK] 해군에 들어가 [MASK]함 · 원자력 · 잠수함의 승무원으로 일하였다. 1953년심의 해군 대위로 예편하였고 이후 땅콩 · 면화 등을 [UNK] 많은 돈을 [MASK]. 그의 별 [MASK] \" 땅콩 농부 \" 로 알려졌다. 1962년 조지아 [MASK] 상원 [MASK] 선거에서 낙선하나 그 선거가 부정선거 였 [MASK] 입증하게 [MASK] 당선되고, 1966년 조지아 주 지사 선거에 낙선하지만 1970년 조지아 주 지 짓 역임 [SEP] 3 × 3 실수 행렬의 행렬식은 은 스칼라 삼중곱의 [MASK]기도 [MASK]. [SEP]'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data_collator(dataset.examples)['input_ids'][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6434,
     "status": "ok",
     "timestamp": 1680014660311,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "AEGfg7JL7KWJ",
    "outputId": "708be4f3-f6b9-46d0-b910-f42b3563d569"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='model_output',\n",
    "    overwrite_output_dir=True, # 모델이 계속 학습됨에 따라서 overwrite를 할 것인지?\n",
    "    num_train_epochs=10,\n",
    "    per_gpu_train_batch_size=32,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2, # 마지막 두 개만 저장하고 그 이전 것들을 전부 삭제\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "executionInfo": {
     "elapsed": 609014,
     "status": "ok",
     "timestamp": 1680015269323,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "wwdMy08j7u-F",
    "outputId": "d6d88fd2-1cff-4092-b928-bafe17d310c6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "/home/kingstar/anaconda3/envs/ml2/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtraintogpb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kingstar/workspace/klue_practice/wandb/run-20230503_010617-7yc5tcqa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/traintogpb/huggingface/runs/7yc5tcqa' target=\"_blank\">volcanic-sun-1</a></strong> to <a href='https://wandb.ai/traintogpb/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/traintogpb/huggingface' target=\"_blank\">https://wandb.ai/traintogpb/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/traintogpb/huggingface/runs/7yc5tcqa' target=\"_blank\">https://wandb.ai/traintogpb/huggingface/runs/7yc5tcqa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='720' max='720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [720/720 04:09, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>9.601000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>9.179200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>9.089200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>8.991800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>8.888600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>8.798300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>8.717300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=720, training_loss=9.027583736843534, metrics={'train_runtime': 260.9678, 'train_samples_per_second': 88.287, 'train_steps_per_second': 2.759, 'total_flos': 1490577258240000.0, 'train_loss': 9.027583736843534, 'epoch': 10.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() # wiki 전체 데이터로 학습 시, 1 epoch에 9시간 정도 소요됩니다!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "6F-dEL8TZ1eo"
   },
   "outputs": [],
   "source": [
    "trainer.save_model('./model_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtuDt--Yb1Bx"
   },
   "source": [
    "BERT 학습이 완료되었습니다! :-)   \n",
    "아래 코드를 실행해서 [MASK] 테스트도 가능합니다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "rrKvxFAMVg9I"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1254,
     "status": "ok",
     "timestamp": 1680015272122,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "gCg6A9rMVsIY",
    "outputId": "01cd94d7-bf0d-4d57-dff3-2928bb4b7f08"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at model_output were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "my_model = BertForMaskedLM.from_pretrained('model_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1680015272539,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "1e1pqEqKWgGT",
    "outputId": "ece64ddc-8147-444f-984c-852c4e8bfaa9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이', '##순', '##신은', '[MASK]', '중', '##기의', '무신', '##이다', '.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('이순신은 [MASK] 중기의 무신이다.') # 습관처럼 tokenizer 정확한지 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "RVYg7-PmVzvH"
   },
   "outputs": [],
   "source": [
    "nlp_fill = pipeline('fill-mask', top_k=5, model=my_model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1680015272540,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "IPn9azZoV7aC",
    "outputId": "834b0c98-c1af-4f20-c8dc-b42984949d4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.03782593086361885,\n",
       "  'token': 14,\n",
       "  'token_str': ',',\n",
       "  'sequence': '[CLS] 이순신은, 중기의 무신이다. [SEP]'},\n",
       " {'score': 0.02568073570728302,\n",
       "  'token': 16,\n",
       "  'token_str': '.',\n",
       "  'sequence': '[CLS] 이순신은. 중기의 무신이다. [SEP]'},\n",
       " {'score': 0.010853766463696957,\n",
       "  'token': 1017,\n",
       "  'token_str': '##의',\n",
       "  'sequence': '[CLS] 이순신은의 중기의 무신이다. [SEP]'},\n",
       " {'score': 0.007862213999032974,\n",
       "  'token': 1033,\n",
       "  'token_str': '##이',\n",
       "  'sequence': '[CLS] 이순신은이 중기의 무신이다. [SEP]'},\n",
       " {'score': 0.007134279236197472,\n",
       "  'token': 1043,\n",
       "  'token_str': '##을',\n",
       "  'sequence': '[CLS] 이순신은을 중기의 무신이다. [SEP]'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_fill('이순신은 [MASK] 중기의 무신이다.') # 사실 wiki-small 데이터로 학습하면 잘 안됨ㅠㅠ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1680015272540,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "6kYenPbpZqhP",
    "outputId": "69e5f19d-8e8b-4f02-e337-c91e02538993"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.01725221425294876,\n",
       "  'token': 16,\n",
       "  'token_str': '.',\n",
       "  'sequence': '[CLS]. 는 조선 중기의 무신이다. [SEP]'},\n",
       " {'score': 0.016912033781409264,\n",
       "  'token': 14,\n",
       "  'token_str': ',',\n",
       "  'sequence': '[CLS], 는 조선 중기의 무신이다. [SEP]'},\n",
       " {'score': 0.007854515686631203,\n",
       "  'token': 706,\n",
       "  'token_str': '이',\n",
       "  'sequence': '[CLS] 이 는 조선 중기의 무신이다. [SEP]'},\n",
       " {'score': 0.007497415412217379,\n",
       "  'token': 9,\n",
       "  'token_str': \"'\",\n",
       "  'sequence': \"[CLS]'는 조선 중기의 무신이다. [SEP]\"},\n",
       " {'score': 0.0072351801209151745,\n",
       "  'token': 1017,\n",
       "  'token_str': '##의',\n",
       "  'sequence': '[CLS]의 는 조선 중기의 무신이다. [SEP]'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_fill('[MASK]는 조선 중기의 무신이다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AvZG_qmQKBL"
   },
   "source": [
    "끗!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
