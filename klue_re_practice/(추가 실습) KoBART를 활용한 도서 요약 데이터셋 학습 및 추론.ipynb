{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7f9e40e403224b86847e455e42f78df0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_914ea2cd69fe4793aa8126ddf5a91f0e","IPY_MODEL_eb0ef7780b6c4b73b255e5a41f4ac181","IPY_MODEL_6360e0981cca4aec943b08314ceb9894"],"layout":"IPY_MODEL_b25ebacc0e634eddb2de479992c6d15f"}},"914ea2cd69fe4793aa8126ddf5a91f0e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f11fee25b2e74211bcb341a8464c89aa","placeholder":"​","style":"IPY_MODEL_3f8569ded07e46fb8ffd30267de0d768","value":"100%"}},"eb0ef7780b6c4b73b255e5a41f4ac181":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6425d2f0ad34f63a6b19998cfffedd0","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b423a25e812a40b4abdd6356e9b26d79","value":1}},"6360e0981cca4aec943b08314ceb9894":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf7444ee5ff248b1958e54a854f1975d","placeholder":"​","style":"IPY_MODEL_9f2d9860e8d44dfcb1c005beeaade2c9","value":" 1/1 [00:00&lt;00:00,  2.46ba/s]"}},"b25ebacc0e634eddb2de479992c6d15f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f11fee25b2e74211bcb341a8464c89aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f8569ded07e46fb8ffd30267de0d768":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e6425d2f0ad34f63a6b19998cfffedd0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b423a25e812a40b4abdd6356e9b26d79":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cf7444ee5ff248b1958e54a854f1975d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f2d9860e8d44dfcb1c005beeaade2c9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd032244eb90451c97e021e5181dd6db":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_150abb3389ac46babe0625a510a670ac","IPY_MODEL_1bbb0471fbd342a3942d3cbc7e20a17c","IPY_MODEL_481ebaee27b24330be93c311f272d9d1"],"layout":"IPY_MODEL_78f8f878e5554d28ad64c88fd66d73f9"}},"150abb3389ac46babe0625a510a670ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e92dedcbf4284a20ae7fabb744af83c2","placeholder":"​","style":"IPY_MODEL_e57e46f51f7e4297bc03e69c3c364b36","value":"100%"}},"1bbb0471fbd342a3942d3cbc7e20a17c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7f5df14b3944fc1b571a28b7c386ff1","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b56b9d6c4c3540069bbece9769b0cbe5","value":1}},"481ebaee27b24330be93c311f272d9d1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb518dae285d4b71b789922826bfc967","placeholder":"​","style":"IPY_MODEL_9fa962093a9a4a728f95064358063618","value":" 1/1 [00:00&lt;00:00,  7.40ba/s]"}},"78f8f878e5554d28ad64c88fd66d73f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e92dedcbf4284a20ae7fabb744af83c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e57e46f51f7e4297bc03e69c3c364b36":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a7f5df14b3944fc1b571a28b7c386ff1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b56b9d6c4c3540069bbece9769b0cbe5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cb518dae285d4b71b789922826bfc967":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9fa962093a9a4a728f95064358063618":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9ffa39bb956c46f2919ab729e71c9fe7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e777ea5574d94484aadd67260eaea55b","IPY_MODEL_3a4d555e28134460b385080107fb7c09","IPY_MODEL_1bcca7e4d74545c58ae1918c49cefdd9"],"layout":"IPY_MODEL_7d4fee9b16fc4fe29e71b9ca0e87e47b"}},"e777ea5574d94484aadd67260eaea55b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13f5ca4f44ab488887b10dd01d5f48fb","placeholder":"​","style":"IPY_MODEL_3cdc2371fc994164bf72e9b84d7d3593","value":"Downloading builder script: "}},"3a4d555e28134460b385080107fb7c09":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6cafa589135a4bc8944970abbdc31369","max":2161,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d20558e834b04804a54a8e41edbe3026","value":2161}},"1bcca7e4d74545c58ae1918c49cefdd9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_098762e9fa0f452f805d07f6f5ce6f7b","placeholder":"​","style":"IPY_MODEL_8e931efbeb9246e38998d0958fe1002a","value":" 5.60k/? [00:00&lt;00:00, 164kB/s]"}},"7d4fee9b16fc4fe29e71b9ca0e87e47b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13f5ca4f44ab488887b10dd01d5f48fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cdc2371fc994164bf72e9b84d7d3593":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6cafa589135a4bc8944970abbdc31369":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d20558e834b04804a54a8e41edbe3026":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"098762e9fa0f452f805d07f6f5ce6f7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e931efbeb9246e38998d0958fe1002a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a082bc86607a4a85872992a4724cd85e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9fd0a5f00d694dbf8b9fcc8e3132bbe0","IPY_MODEL_8a223078f03c43469a8f4cd3e9760a1b","IPY_MODEL_20252d53407f4307abaf3fabed31a650"],"layout":"IPY_MODEL_1aea91d0775f48ea8097829e1d776ff3"}},"9fd0a5f00d694dbf8b9fcc8e3132bbe0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a57f4b0402f44a55b55bfa34fcc5cd74","placeholder":"​","style":"IPY_MODEL_d4ed231c28394b2ba16cdb357ba93015","value":"Downloading: 100%"}},"8a223078f03c43469a8f4cd3e9760a1b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bdf2ac73e8d94d70b7bd81de523d8942","max":1454,"min":0,"orientation":"horizontal","style":"IPY_MODEL_884ea3af92424c989e5e9ee2ea96214c","value":1454}},"20252d53407f4307abaf3fabed31a650":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_661b721f79b744569ca7bb2d0790dfaf","placeholder":"​","style":"IPY_MODEL_0816960475f3441b9f3a1deea53e33b2","value":" 1.42k/1.42k [00:00&lt;00:00, 42.2kB/s]"}},"1aea91d0775f48ea8097829e1d776ff3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a57f4b0402f44a55b55bfa34fcc5cd74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4ed231c28394b2ba16cdb357ba93015":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bdf2ac73e8d94d70b7bd81de523d8942":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"884ea3af92424c989e5e9ee2ea96214c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"661b721f79b744569ca7bb2d0790dfaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0816960475f3441b9f3a1deea53e33b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"383ad6b1014a4f4b9b94a7d09511b4de":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_681c5aa1c9674439ab7530d3ece30358","IPY_MODEL_36bd089604454611bdf8b52b0d734727","IPY_MODEL_75c3fc729cb54eb89df070c37dd70a0b"],"layout":"IPY_MODEL_1d98995015224dfc96d540ccc8dd0857"}},"681c5aa1c9674439ab7530d3ece30358":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_285d4ee2dd9b48b18220ad28d8ddce65","placeholder":"​","style":"IPY_MODEL_70a89dd0ae77481fb814fefa6a987722","value":"Downloading: 100%"}},"36bd089604454611bdf8b52b0d734727":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f624d3e463e04d038cdf4f7a2a74e757","max":495657017,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a447474a318b47e48e40afcfd6a0c3a4","value":495657017}},"75c3fc729cb54eb89df070c37dd70a0b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6bcfa88b4d174d05b2a4c7c66d4b0f7f","placeholder":"​","style":"IPY_MODEL_f4b1dce6c0944a0fadcb68916b30b747","value":" 473M/473M [00:38&lt;00:00, 8.36MB/s]"}},"1d98995015224dfc96d540ccc8dd0857":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"285d4ee2dd9b48b18220ad28d8ddce65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70a89dd0ae77481fb814fefa6a987722":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f624d3e463e04d038cdf4f7a2a74e757":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a447474a318b47e48e40afcfd6a0c3a4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6bcfa88b4d174d05b2a4c7c66d4b0f7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4b1dce6c0944a0fadcb68916b30b747":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# KoBART를 활용한 도서 요약 데이터셋 학습 및 추론\n","\n","\n","[CC BY-NC-ND](https://creativecommons.org/licenses/by-nc-nd/2.0/kr/)"],"metadata":{"id":"imdZJYyq0FQk"}},{"cell_type":"markdown","source":["###**콘텐츠 라이선스**\n","\n","<font color='red'><b>**WARNING**</b></font> : **본 교육 콘텐츠의 지식재산권은 재단법인 네이버커넥트에 귀속됩니다. 본 콘텐츠를 어떠한 경로로든 외부로 유출 및 수정하는 행위를 엄격히 금합니다.** 다만, 비영리적 교육 및 연구활동에 한정되어 사용할 수 있으나 재단의 허락을 받아야 합니다. 이를 위반하는 경우, 관련 법률에 따라 책임을 질 수 있습니다."],"metadata":{"id":"FHpggSaF0seo"}},{"cell_type":"markdown","source":["이번 실습에서는 사전학습된 한국어 언어모델인 KoBART를 활용하여 도서 문서를 요약하는 요약모델을 학습해봅시다!"],"metadata":{"id":"V--vuXQcV3Tj"}},{"cell_type":"markdown","source":["## 1. dataset download"],"metadata":{"id":"yrEBqELRZ559"}},{"cell_type":"markdown","source":["AI Hub의 도서 요약 데이터셋을 활용합니다.   \n","전처리가 완료된 데이터셋은 [링크](https://drive.google.com/file/d/1SBKsb9hy9vmOm5Cfi0GDOrpm-_kqXOyS/view?usp=share_link)에서 받으실 수 있으며,\n","\n","해당 데이터셋에 대한 이용 약관은 [여기](https://www.aihub.or.kr/useStplat.do?currMenu=110&topMenu=110)에 명시되어 있습니다."],"metadata":{"id":"zT_7j7qkaGY3"}},{"cell_type":"code","source":["data = open('/content/drive/MyDrive/summary/modi_output.txt', 'r', encoding='utf-8')\n","lines = data.readlines()\n","passages = []\n","summaries = []\n","\n","for line in lines:\n","    line = line.strip()\n","    if len(line.split('\\t')) < 2:\n","        print(line)\n","        break\n","    passages.append('<s>' + line.split('\\t')[0] + '</s>')\n","    # kobart 학습시에는 앞 뒤로 '<s>'와 '</s>'를 삽입해야 잘 나옵니다.. 왜그런지 몰?루..\n","    summaries.append('<s>' + line.split('\\t')[1] + '</s>')\n","    # 주의! 이렇게 끝에 </s>만 붙여서 tokenizing하시면 나중에 max_len에 의해 </s>가 truncation될 수 있습니다!\n","    # 요약을 위한 모델이기 때문에, passage가 model의 max_len을 넘어가는 경우엔 학습 데이터에서 제거하는 것이 좋습니다.\n","\n","data.close()"],"metadata":{"id":"cBG6utCzaQIa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"passages:\", passages[0])\n","print(\"summaries:\", summaries[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"so2CHdx1aXPV","outputId":"c50a11cb-9ddf-4e62-d1a1-d874e0be0a8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["passages: <s>자신의 생각을 불명확하게 표현하는 사람들은 생각도 불명확할 가능성이 높다. 따라서 생각을 명료하게 정리하는 것이 명확한 글쓰기의 시작이다. 존 케네스 갈브레이드(John Kenneth Galbraith) 교수는 “아무리 복잡하고 어려운 주제라고 명쾌한 언어로 설명이 가능하다. 하지만 필자가 그 주제에 대해 완전히 이해하지 못하면 절대로 명확하게 쓸 수 없다.”고 했다. 명료한 글은 명료한 생각에서 나온다. 그렇지만 글을 고치다보면 생각도 명료하게 된다. 명료하게 쓰려면 내용을 단순화해야 한다. 많은 것을 전달하려는 욕심을 버리고 한두가지로 선택하고 거기 집중해야 한다. 앞에서 이야기한 간결성은 명확성에 도움이 된다. 명료함은 복잡한 설득기법을 사용하는 것보다 더 중요하다. 판사는 이해할 수 없는 사실관계 진술로는 설득되지 않는다. 설득기법이 명료함을 해치는 때에는 그 기법을 사용하지 않는다.</s>\n","summaries: <s>주제가 아무리 복잡하고 어려울 지라도 필자가 주제에 대해 완벽하게 이해하고 있다면 명확한 글쓰기는 당연 가능하다. 명확한 글쓰기를 할 때 가장 중요한 점은 내용의 단순화 및 간결성이고 많은 것을 전달하려는 욕심을 버리는 것이다.</s>\n"]}]},{"cell_type":"code","source":["print(len(passages))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I_uV2V_1_eOY","outputId":"b73bb2a4-4003-4e09-c34e-52424981e242"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["160002\n"]}]},{"cell_type":"markdown","source":["## 2. make dataset"],"metadata":{"id":"r_9Lw95N1pPA"}},{"cell_type":"markdown","source":["Huggingface에서 데이터를 쉽게 다루기 위해 datasets로 묶어줍니다."],"metadata":{"id":"KquhfzgK27jC"}},{"cell_type":"code","source":["!pip install transformers\n","!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YejeK2M92QRn","outputId":"9742f99e-25f3-4152-aa6c-d958fb913dea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.0.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n","Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.2.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split"],"metadata":{"id":"jR95jrbc1koq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train, x_test, y_train, y_test = train_test_split(\n","    passages[0:1000],   # 16만 데이터를 모두 사용하면, 학습 시간이 1 epoch에 약 2시간 30분 정도 소요됩니다.\n","    summaries[0:1000],  # 그래서 실습에서는 1000개만 사용하겠습니다.\n","    test_size=0.2,\n","    shuffle=True\n","    )"],"metadata":{"id":"TJYI9Qle1vbO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(x_train))\n","print(len(x_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sX9UPJSi2EHj","outputId":"13adc6ed-7d75-4e96-f716-2e5dba034a61"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["800\n","200\n"]}]},{"cell_type":"code","source":["from datasets import Dataset, DatasetDict"],"metadata":{"id":"KSOeeWy22JRT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = DatasetDict(\n","    {\n","        'train': Dataset.from_dict({'passage':x_train, 'summary':y_train}),\n","        'valid': Dataset.from_dict({'passage':x_test, 'summary':y_test}),\n","    }\n",")"],"metadata":{"id":"2z5EUobt2PCJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(dataset['train'][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7yi9c03G2nhi","outputId":"e12f81a5-17fd-4733-bb32-e53aa125b807"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'passage': '<s>생활지도 문항 중에서 부모 합산하여 점수가 가장 높은 문항은 아이가 공공 장소에서 다른 사람에게 피해주는 일이 없도록 신경 씀(4.1점), 아이의 긍정적인 행동에 대해 충분히 칭찬함(4.1점)으로 나타났다. 이는 모두 실행역량 문항에 포함되는 것으로 인식, 성장역량 문항에 비해서 점수가 높았다. 전체적으로 문항별 점수가 크게 다르지 않으나 통계적으로 유의한 4개의 문항에 대해 부의 점수가 모에 비해 0.15점 낮은 것으로 나타났다. 또한 다른 영역과 달리 생활지도 영역의 실행역량의 경우, 부모 간의 역량 차이가 크지 않은 것으로 나타났으나 인식과 성장역량에서 다른 영역과 마찬가지로 부모 간 역량 차이가 나 부에 비해 모의 역량이 더 높은 것으로 확인되었다.</s>', 'summary': '<s>생활지도 영역에서는 실행역량의 두 문항이 부모 합산 점수가 가장 높았다. 문항별 점수 중 통계적으로 유의한 4개 문항이나 생활지도 영역의 인식, 성장역량은 모의 점수가 부보다 높게 나타났다.</s>'}\n"]}]},{"cell_type":"markdown","source":["## 3. tokenizing datasets"],"metadata":{"id":"O7oDJodK22eV"}},{"cell_type":"markdown","source":["먼저 kobart-base-v2의 tokenizer를 다운받습니다."],"metadata":{"id":"aVaasN2I3PuE"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"gogamza/kobart-base-v2\")"],"metadata":{"id":"WmBUzjTN2qya"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.special_tokens_map)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ChBAwSCmE-cy","outputId":"6030b04f-df01-45b2-c659-b58b4a66208a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}\n"]}]},{"cell_type":"code","source":["tokenizer.tokenize('<s>')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B1-2Y3Z7r0eU","outputId":"67a29aac-d6f8-47ae-9b81-c04220774e07"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<s>']"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["sents = [\"이순신은 조선 중기의 무신이다.\", \"이순신 짱\"]"],"metadata":{"id":"4bvT3S-k3byG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_sents = tokenizer(sents)"],"metadata":{"id":"SpIij45J3iwc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for keys in tokenized_sents.keys():\n","    print(keys, ':', tokenized_sents[keys]) # 길이가 다릅니다"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CUc9zAuh4xGQ","outputId":"c1463c17-6ea1-48d9-c4c2-197defb2d9f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["input_ids : [[22577, 19293, 15015, 14059, 15525, 14113, 11467, 16247], [22577, 11467, 1700, 12371]]\n","token_type_ids : [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0]]\n","attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1]]\n"]}]},{"cell_type":"markdown","source":["tokenizing 옵션을 줄 수 있습니다."],"metadata":{"id":"loknaNoa4iAP"}},{"cell_type":"code","source":["tokenized_sents = tokenizer(sents, max_length=6, truncation=True, padding=True)\n","for keys in tokenized_sents.keys():\n","    print(keys, ':', tokenized_sents[keys])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"huSH-jLF4he7","outputId":"72fad6c7-5e80-41cb-e0f7-c30ae2205994"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["input_ids : [[22577, 19293, 15015, 14059, 15525, 14113], [22577, 11467, 1700, 12371, 3, 3]]\n","token_type_ids : [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]\n","attention_mask : [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0]]\n"]}]},{"cell_type":"markdown","source":["전체 데이터셋을 tokenizing하여 저장하겠습니다."],"metadata":{"id":"aLl_pSic4A0-"}},{"cell_type":"code","source":["max_input_length = 512\n","max_target_length = 512\n","\n","def preprocess_function(examples):\n","    inputs = [doc for doc in examples[\"passage\"]]\n","    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)  # 단, padding은 주지 않습니다.\n","    # padding은 data_collator가 만들어줄겁니다.\n","\n","    # Setup the tokenizer for targets\n","    with tokenizer.as_target_tokenizer():   # kobart의 경우, 동일한 tokenizer를 사용합니다.\n","        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"],"metadata":{"id":"m2CZL-AW3kcM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preprocessed_dataset_sample = preprocess_function(dataset['train'][0:1])\n","\n","for keys in preprocessed_dataset_sample.keys():\n","    print(keys, ':', preprocessed_dataset_sample[keys])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0NtMyQud5VV0","outputId":"f913b006-5304-435c-ec1e-f9adeffc0c7d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["input_ids : [[0, 14923, 14931, 14111, 13603, 18609, 15714, 14350, 11211, 14330, 14244, 15026, 14377, 14904, 14111, 13603, 12005, 19736, 15641, 14137, 18290, 14355, 19977, 14783, 14785, 15029, 22382, 17219, 1700, 11667, 15672, 15275, 12136, 14505, 22497, 20316, 29211, 14225, 17050, 20624, 13599, 15672, 15275, 12136, 18247, 19743, 14130, 15002, 14422, 19349, 11803, 10256, 14111, 13603, 11786, 14740, 14343, 14173, 16083, 243, 14966, 11803, 10256, 14111, 13603, 11786, 14085, 14668, 14244, 15026, 14276, 18332, 26335, 14111, 13603, 10872, 14244, 15026, 15003, 27386, 14076, 14665, 19672, 14157, 23900, 13590, 14136, 15957, 14111, 13603, 11786, 14225, 27104, 14244, 15026, 14069, 11786, 15685, 27305, 252, 12136, 16867, 14173, 19743, 14130, 14638, 14355, 17601, 9120, 15616, 14923, 14931, 17601, 12024, 19349, 11803, 18306, 18543, 15714, 18406, 23563, 17828, 24107, 14687, 14173, 19743, 14665, 16083, 9120, 14966, 11803, 10256, 14030, 14355, 17601, 9120, 19177, 15714, 14313, 23563, 17828, 14054, 14062, 11786, 15685, 25538, 14201, 16191, 14166, 14904, 14173, 14715, 15568, 14130, 1]]\n","token_type_ids : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n","attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n","labels : [[0, 14923, 14931, 17601, 14275, 19349, 11803, 18306, 14196, 14111, 13603, 12034, 15714, 14350, 11211, 14244, 15026, 14377, 14276, 18332, 14111, 13603, 10872, 25880, 14059, 19672, 14157, 23900, 13590, 17912, 14111, 13603, 14303, 14923, 14931, 17601, 12024, 16083, 243, 14966, 11803, 18448, 25538, 14244, 15026, 14062, 14310, 21122, 19743, 14130, 1]]\n"]}]},{"cell_type":"code","source":["print(tokenizer.decode(preprocessed_dataset_sample['input_ids'][0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A7SU7_oA5cT7","outputId":"8d650bdc-d233-4f47-fdad-9550209bf272"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<s> 생활지도 문항 중에서 부모 합산하여 점수가 가장 높은 문항은 아이가 공공 장소에서 다른 사람에게 피해주는 일이 없도록 신경 씀(4.1점), 아이의 긍정적인 행동에 대해 충분히 칭찬함(4.1점)으로 나타났다. 이는 모두 실행역량 문항에 포함되는 것으로 인식, 성장역량 문항에 비해서 점수가 높았다. 전체적으로 문항별 점수가 크게 다르지 않으나 통계적으로 유의한 4개의 문항에 대해 부의 점수가 모에 비해 0.15점 낮은 것으로 나타났다. 또한 다른 영역과 달리 생활지도 영역의 실행역량의 경우, 부모 간의 역량 차이가 크지 않은 것으로 나타났으나 인식과 성장역량에서 다른 영역과 마찬가지로 부모 간 역량 차이가 나 부에 비해 모의 역량이 더 높은 것으로 확인되었다.</s>\n"]}]},{"cell_type":"code","source":["print(tokenizer.decode(preprocessed_dataset_sample['labels'][0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GqFZqSNb550b","outputId":"8a13fe9f-e755-4e22-bb91-2c0efb8f7139"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<s> 생활지도 영역에서는 실행역량의 두 문항이 부모 합산 점수가 가장 높았다. 문항별 점수 중 통계적으로 유의한 4개 문항이나 생활지도 영역의 인식, 성장역량은 모의 점수가 부보다 높게 나타났다.</s>\n"]}]},{"cell_type":"markdown","source":["동작을 확인했으니, 학습 데이터를 tokenizing해서 저장하겠습니다."],"metadata":{"id":"Ujw-kKhc6UDP"}},{"cell_type":"code","source":["tokenized_datasets = dataset.map(preprocess_function, batched=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["7f9e40e403224b86847e455e42f78df0","914ea2cd69fe4793aa8126ddf5a91f0e","eb0ef7780b6c4b73b255e5a41f4ac181","6360e0981cca4aec943b08314ceb9894","b25ebacc0e634eddb2de479992c6d15f","f11fee25b2e74211bcb341a8464c89aa","3f8569ded07e46fb8ffd30267de0d768","e6425d2f0ad34f63a6b19998cfffedd0","b423a25e812a40b4abdd6356e9b26d79","cf7444ee5ff248b1958e54a854f1975d","9f2d9860e8d44dfcb1c005beeaade2c9","fd032244eb90451c97e021e5181dd6db","150abb3389ac46babe0625a510a670ac","1bbb0471fbd342a3942d3cbc7e20a17c","481ebaee27b24330be93c311f272d9d1","78f8f878e5554d28ad64c88fd66d73f9","e92dedcbf4284a20ae7fabb744af83c2","e57e46f51f7e4297bc03e69c3c364b36","a7f5df14b3944fc1b571a28b7c386ff1","b56b9d6c4c3540069bbece9769b0cbe5","cb518dae285d4b71b789922826bfc967","9fa962093a9a4a728f95064358063618"]},"id":"CIFKmign6Bez","outputId":"230cbf44-8c33-4aa7-e767-17ee60baf288"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f9e40e403224b86847e455e42f78df0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd032244eb90451c97e021e5181dd6db"}},"metadata":{}}]},{"cell_type":"code","source":["for keys in tokenized_datasets['train'][0].keys():\n","    print(keys, ':', tokenized_datasets['train'][0][keys])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZQPea8ur6OHJ","outputId":"e0817b00-62c8-462c-90e5-1e24d0861355"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["passage : <s>생활지도 문항 중에서 부모 합산하여 점수가 가장 높은 문항은 아이가 공공 장소에서 다른 사람에게 피해주는 일이 없도록 신경 씀(4.1점), 아이의 긍정적인 행동에 대해 충분히 칭찬함(4.1점)으로 나타났다. 이는 모두 실행역량 문항에 포함되는 것으로 인식, 성장역량 문항에 비해서 점수가 높았다. 전체적으로 문항별 점수가 크게 다르지 않으나 통계적으로 유의한 4개의 문항에 대해 부의 점수가 모에 비해 0.15점 낮은 것으로 나타났다. 또한 다른 영역과 달리 생활지도 영역의 실행역량의 경우, 부모 간의 역량 차이가 크지 않은 것으로 나타났으나 인식과 성장역량에서 다른 영역과 마찬가지로 부모 간 역량 차이가 나 부에 비해 모의 역량이 더 높은 것으로 확인되었다.</s>\n","summary : <s>생활지도 영역에서는 실행역량의 두 문항이 부모 합산 점수가 가장 높았다. 문항별 점수 중 통계적으로 유의한 4개 문항이나 생활지도 영역의 인식, 성장역량은 모의 점수가 부보다 높게 나타났다.</s>\n","input_ids : [0, 14923, 14931, 14111, 13603, 18609, 15714, 14350, 11211, 14330, 14244, 15026, 14377, 14904, 14111, 13603, 12005, 19736, 15641, 14137, 18290, 14355, 19977, 14783, 14785, 15029, 22382, 17219, 1700, 11667, 15672, 15275, 12136, 14505, 22497, 20316, 29211, 14225, 17050, 20624, 13599, 15672, 15275, 12136, 18247, 19743, 14130, 15002, 14422, 19349, 11803, 10256, 14111, 13603, 11786, 14740, 14343, 14173, 16083, 243, 14966, 11803, 10256, 14111, 13603, 11786, 14085, 14668, 14244, 15026, 14276, 18332, 26335, 14111, 13603, 10872, 14244, 15026, 15003, 27386, 14076, 14665, 19672, 14157, 23900, 13590, 14136, 15957, 14111, 13603, 11786, 14225, 27104, 14244, 15026, 14069, 11786, 15685, 27305, 252, 12136, 16867, 14173, 19743, 14130, 14638, 14355, 17601, 9120, 15616, 14923, 14931, 17601, 12024, 19349, 11803, 18306, 18543, 15714, 18406, 23563, 17828, 24107, 14687, 14173, 19743, 14665, 16083, 9120, 14966, 11803, 10256, 14030, 14355, 17601, 9120, 19177, 15714, 14313, 23563, 17828, 14054, 14062, 11786, 15685, 25538, 14201, 16191, 14166, 14904, 14173, 14715, 15568, 14130, 1]\n","token_type_ids : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","labels : [0, 14923, 14931, 17601, 14275, 19349, 11803, 18306, 14196, 14111, 13603, 12034, 15714, 14350, 11211, 14244, 15026, 14377, 14276, 18332, 14111, 13603, 10872, 25880, 14059, 19672, 14157, 23900, 13590, 17912, 14111, 13603, 14303, 14923, 14931, 17601, 12024, 16083, 243, 14966, 11803, 18448, 25538, 14244, 15026, 14062, 14310, 21122, 19743, 14130, 1]\n"]}]},{"cell_type":"markdown","source":["## 4. Fine-tuning using KoBART"],"metadata":{"id":"hVPyfLIF7GZ2"}},{"cell_type":"markdown","source":["이제 본격적으로 kobart 모델을 불러와 학습을 해보겠습니다."],"metadata":{"id":"Cl-Bzejr7K-e"}},{"cell_type":"code","source":["from transformers import BartForConditionalGeneration"],"metadata":{"id":"QR-M4x3q6quP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-base-v2')"],"metadata":{"id":"hIcLuAqw7TcM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["모델 구조를 확인해봅니다."],"metadata":{"id":"oEGRaVEhAAX9"}},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H1jNSgTb7e3q","outputId":"3bb1869c-d70e-416b-c77f-b7300fb18a16"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BartForConditionalGeneration(\n","  (model): BartModel(\n","    (shared): Embedding(30000, 768, padding_idx=3)\n","    (encoder): BartEncoder(\n","      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n","      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n","      (layers): ModuleList(\n","        (0): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (1): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (2): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (3): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (4): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (5): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (decoder): BartDecoder(\n","      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n","      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n","      (layers): ModuleList(\n","        (0): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (1): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (2): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (3): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (4): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (5): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",")"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments"],"metadata":{"id":"aSP5RXwY7jNb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 16\n","args = Seq2SeqTrainingArguments(\n","    output_dir = \"model_output\",\n","    evaluation_strategy = \"steps\",\n","    learning_rate = 2e-5,\n","    per_device_train_batch_size = batch_size,\n","    per_device_eval_batch_size = batch_size,\n","    save_total_limit = 2,\n","    num_train_epochs = 2,\n","    predict_with_generate = True,\n","    save_steps = 1000,\n","    eval_steps = 500\n",")"],"metadata":{"id":"qMKL9cuE760u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n","# https://github.com/huggingface/transformers/blob/master/src/transformers/data/data_collator.py\n","# Data collator that will dynamically pad the inputs received, as well as the labels."],"metadata":{"id":"v5PP2bnk8g9v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이제 evaluation을 위한 metric을 넣어보겠습니다."],"metadata":{"id":"uGa3gECG83ZO"}},{"cell_type":"code","source":["!pip install rouge_score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6oijplnd83BM","outputId":"f449397d-4b8c-4bc9-a232-afeffed70356"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rouge_score\n","  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.0.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.21.5)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge_score) (3.2.5)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.15.0)\n","Installing collected packages: rouge-score\n","Successfully installed rouge-score-0.0.4\n"]}]},{"cell_type":"code","source":["from datasets import load_metric\n","\n","metric = load_metric(\"rouge\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["9ffa39bb956c46f2919ab729e71c9fe7","e777ea5574d94484aadd67260eaea55b","3a4d555e28134460b385080107fb7c09","1bcca7e4d74545c58ae1918c49cefdd9","7d4fee9b16fc4fe29e71b9ca0e87e47b","13f5ca4f44ab488887b10dd01d5f48fb","3cdc2371fc994164bf72e9b84d7d3593","6cafa589135a4bc8944970abbdc31369","d20558e834b04804a54a8e41edbe3026","098762e9fa0f452f805d07f6f5ce6f7b","8e931efbeb9246e38998d0958fe1002a"]},"id":"T0GveSk48xtY","outputId":"77d8a763-2434-44c2-c204-d5a187590a74"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/2.16k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ffa39bb956c46f2919ab729e71c9fe7"}},"metadata":{}}]},{"cell_type":"code","source":["!pip install kss==2.1.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T9aOXmbj9HEI","outputId":"c1f33b39-0a82-4861-bd85-585ee61f6891"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: kss==2.1.0 in /usr/local/lib/python3.7/dist-packages (2.1.0)\n"]}]},{"cell_type":"code","source":["import kss\n","import numpy as np\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    \n","    # Rouge expects a newline after each sentence\n","    decoded_preds = [\"\\n\".join(kss.split_sentences(pred.strip())) for pred in decoded_preds]\n","    decoded_labels = [\"\\n\".join(kss.split_sentences(label.strip())) for label in decoded_labels]\n","    \n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    # Extract a few results\n","    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n","    \n","    # Add mean generated length\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    \n","    return {k: round(v, 4) for k, v in result.items()}"],"metadata":{"id":"YQusxlMt8mJ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"valid\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")"],"metadata":{"id":"nocRhMA69Xa5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"WUWu_2479dB-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.save_model()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0YQEwMIp9gxW","outputId":"ef86a6f1-99a9-4a05-bcd4-689e97d95ac6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to model_output\n","Configuration saved in model_output/config.json\n","Model weights saved in model_output/pytorch_model.bin\n","tokenizer config file saved in model_output/tokenizer_config.json\n","Special tokens file saved in model_output/special_tokens_map.json\n"]}]},{"cell_type":"code","source":["!cp /content/model_output/pytorch_model.bin /content/drive/MyDrive/summary/.\n","!cp /content/model_output/config.json /content/drive/MyDrive/summary/."],"metadata":{"id":"wUIpVkl7M87i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Inference"],"metadata":{"id":"wkN81m7EK30i"}},{"cell_type":"markdown","source":["이제 학습된 모델로 inference를 날려보겠습니다."],"metadata":{"id":"5C_pVtqCNR2j"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, BartForConditionalGeneration\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"gogamza/kobart-base-v2\")\n","model = BartForConditionalGeneration.from_pretrained('MrBananaHuman/kobart-base-v2-summarization').to('cuda')\n","model.eval()"],"metadata":{"id":"TKhfJvQALMU1","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["a082bc86607a4a85872992a4724cd85e","9fd0a5f00d694dbf8b9fcc8e3132bbe0","8a223078f03c43469a8f4cd3e9760a1b","20252d53407f4307abaf3fabed31a650","1aea91d0775f48ea8097829e1d776ff3","a57f4b0402f44a55b55bfa34fcc5cd74","d4ed231c28394b2ba16cdb357ba93015","bdf2ac73e8d94d70b7bd81de523d8942","884ea3af92424c989e5e9ee2ea96214c","661b721f79b744569ca7bb2d0790dfaf","0816960475f3441b9f3a1deea53e33b2","383ad6b1014a4f4b9b94a7d09511b4de","681c5aa1c9674439ab7530d3ece30358","36bd089604454611bdf8b52b0d734727","75c3fc729cb54eb89df070c37dd70a0b","1d98995015224dfc96d540ccc8dd0857","285d4ee2dd9b48b18220ad28d8ddce65","70a89dd0ae77481fb814fefa6a987722","f624d3e463e04d038cdf4f7a2a74e757","a447474a318b47e48e40afcfd6a0c3a4","6bcfa88b4d174d05b2a4c7c66d4b0f7f","f4b1dce6c0944a0fadcb68916b30b747"]},"outputId":"1a99b5d9-c813-48d3-92d5-0741d54af0db"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/gogamza/kobart-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/54a37e9385f90886428b084042f151c1a699203416d41765d94aac4cddb5fd5c.d098ef3866c1da94bdfaa5c1f24ecb7c5c16b37423b79263fbd3668d2ae61f91\n","Model config BartConfig {\n","  \"_name_or_path\": \"gogamza/kobart-base-v2\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"BartModel\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 1,\n","  \"classif_dropout\": 0.1,\n","  \"classifier_dropout\": 0.1,\n","  \"d_model\": 768,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 3072,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 6,\n","  \"decoder_start_token_id\": 1,\n","  \"do_blenderbot_90_layernorm\": false,\n","  \"dropout\": 0.1,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 3072,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 6,\n","  \"eos_token_id\": 1,\n","  \"extra_pos_embeddings\": 2,\n","  \"force_bos_token_to_be_generated\": false,\n","  \"forced_eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"NEGATIVE\",\n","    \"1\": \"POSITIVE\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"kobart_version\": 2.0,\n","  \"label2id\": {\n","    \"NEGATIVE\": 0,\n","    \"POSITIVE\": 1\n","  },\n","  \"max_position_embeddings\": 1026,\n","  \"model_type\": \"bart\",\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": true,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 3,\n","  \"scale_embedding\": false,\n","  \"static_position_embeddings\": false,\n","  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 30000\n","}\n","\n","loading file https://huggingface.co/gogamza/kobart-base-v2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/f94202e1dad4fcfcb282aff4c6865b6119e03c87c6fa9e5886abe93835c41ecd.dc2013f8bbecd755468e2c44397f53dc624be5451d0190744397caf61a20383f\n","loading file https://huggingface.co/gogamza/kobart-base-v2/resolve/main/added_tokens.json from cache at /root/.cache/huggingface/transformers/7c75331e2f4b5767db997fbb489f1408eb36a3217beb3057ae8d04bd2b3f97ba.04312f398a3bbda664297588800a86e0fda9d4ef4f0749cd9d96f88043daad39\n","loading file https://huggingface.co/gogamza/kobart-base-v2/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/a87d2ed77831bb40ce806a97c04126addf5ecc82b3e23ecf916b2a4acdb9c29a.c23d5e62137984cf842a885705037b25b156747d145406702932d5f5d5e7c88e\n","loading file https://huggingface.co/gogamza/kobart-base-v2/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/gogamza/kobart-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/54a37e9385f90886428b084042f151c1a699203416d41765d94aac4cddb5fd5c.d098ef3866c1da94bdfaa5c1f24ecb7c5c16b37423b79263fbd3668d2ae61f91\n","Model config BartConfig {\n","  \"_name_or_path\": \"gogamza/kobart-base-v2\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"BartModel\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 1,\n","  \"classif_dropout\": 0.1,\n","  \"classifier_dropout\": 0.1,\n","  \"d_model\": 768,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 3072,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 6,\n","  \"decoder_start_token_id\": 1,\n","  \"do_blenderbot_90_layernorm\": false,\n","  \"dropout\": 0.1,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 3072,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 6,\n","  \"eos_token_id\": 1,\n","  \"extra_pos_embeddings\": 2,\n","  \"force_bos_token_to_be_generated\": false,\n","  \"forced_eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"NEGATIVE\",\n","    \"1\": \"POSITIVE\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"kobart_version\": 2.0,\n","  \"label2id\": {\n","    \"NEGATIVE\": 0,\n","    \"POSITIVE\": 1\n","  },\n","  \"max_position_embeddings\": 1026,\n","  \"model_type\": \"bart\",\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": true,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 3,\n","  \"scale_embedding\": false,\n","  \"static_position_embeddings\": false,\n","  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 30000\n","}\n","\n","https://huggingface.co/MrBananaHuman/kobart-base-v2-summarization/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpq1_h4fbc\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a082bc86607a4a85872992a4724cd85e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/MrBananaHuman/kobart-base-v2-summarization/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/2a767070f7e98353903ccb1ac5bfb4080195079cbb731542a8f0a22c55512b70.d4626372fd5133c852a84193d02699c123aa2f4700d3ed2d1cb7ec98570fa7bd\n","creating metadata file for /root/.cache/huggingface/transformers/2a767070f7e98353903ccb1ac5bfb4080195079cbb731542a8f0a22c55512b70.d4626372fd5133c852a84193d02699c123aa2f4700d3ed2d1cb7ec98570fa7bd\n","loading configuration file https://huggingface.co/MrBananaHuman/kobart-base-v2-summarization/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/2a767070f7e98353903ccb1ac5bfb4080195079cbb731542a8f0a22c55512b70.d4626372fd5133c852a84193d02699c123aa2f4700d3ed2d1cb7ec98570fa7bd\n","Model config BartConfig {\n","  \"_name_or_path\": \"gogamza/kobart-base-v2\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"BartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 1,\n","  \"classif_dropout\": 0.1,\n","  \"classifier_dropout\": 0.1,\n","  \"d_model\": 768,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 3072,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 6,\n","  \"decoder_start_token_id\": 1,\n","  \"do_blenderbot_90_layernorm\": false,\n","  \"dropout\": 0.1,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 3072,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 6,\n","  \"eos_token_id\": 1,\n","  \"extra_pos_embeddings\": 2,\n","  \"force_bos_token_to_be_generated\": false,\n","  \"forced_eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"NEGATIVE\",\n","    \"1\": \"POSITIVE\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"kobart_version\": 2.0,\n","  \"label2id\": {\n","    \"NEGATIVE\": 0,\n","    \"POSITIVE\": 1\n","  },\n","  \"max_position_embeddings\": 1026,\n","  \"model_type\": \"bart\",\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": true,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 3,\n","  \"scale_embedding\": false,\n","  \"static_position_embeddings\": false,\n","  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 30000\n","}\n","\n","https://huggingface.co/MrBananaHuman/kobart-base-v2-summarization/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpflwwg52n\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/473M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"383ad6b1014a4f4b9b94a7d09511b4de"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/MrBananaHuman/kobart-base-v2-summarization/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/8f6bd1db12121bd5dba9e20202216e9d29f7c21ec822a2abe7194d4374d23674.86be1e574dbb785d7fdbe6a0d3a19060a1f9d82acc7d00ab197cb10b3ed3d293\n","creating metadata file for /root/.cache/huggingface/transformers/8f6bd1db12121bd5dba9e20202216e9d29f7c21ec822a2abe7194d4374d23674.86be1e574dbb785d7fdbe6a0d3a19060a1f9d82acc7d00ab197cb10b3ed3d293\n","loading weights file https://huggingface.co/MrBananaHuman/kobart-base-v2-summarization/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8f6bd1db12121bd5dba9e20202216e9d29f7c21ec822a2abe7194d4374d23674.86be1e574dbb785d7fdbe6a0d3a19060a1f9d82acc7d00ab197cb10b3ed3d293\n","All model checkpoint weights were used when initializing BartForConditionalGeneration.\n","\n","All the weights of BartForConditionalGeneration were initialized from the model checkpoint at MrBananaHuman/kobart-base-v2-summarization.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"]}]},{"cell_type":"markdown","source":["디코딩 전략에 대해선 아래 링크를 참고해주세요    \n","https://colab.research.google.com/drive/1yUGVmQ0nj8Hd3h0YV6PemQx0FtzpefGB?usp=sharing"],"metadata":{"id":"I7WjGwwcLyhs"}},{"cell_type":"code","source":["import torch\n","\n","def summarization(text):\n","    tokenized_text = tokenizer(text, return_tensors='pt', truncation=False).to('cuda')\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            tokenized_text['input_ids'],\n","            do_sample = True,\n","            eos_token_id = tokenizer.eos_token_id,\n","            max_length = 512,\n","            # top_p = 0.7,\n","            # top_k = 20,\n","            num_beams=20,\n","            num_return_sequences = 1,\n","            no_repeat_ngram_size = 2,\n","            early_stopping = True\n","        )\n","        return tokenizer.decode(outputs[0], skip_special_tokens = True)"],"metadata":{"id":"CAxQH8mhCsgj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summarization(\"\"\"<s>[서울=뉴시스] 임종명 기자 = 러시아가 제안한 우크라이나 인도주의 상황에 관한 결의안이 유엔 안전보장이사회(안보리)를 통과하지 못했다.\n","\n","AP통신과 CNN 등 외신은 23일(현지시간) 미국 뉴욕 유엔본부에서 진행된 안보리에서 미국을 포함한 13개국이 표결을 기원함으로써 러시아의 제안이 부결됐다고 보도했다.\n","\n","결의안이 통과되려면 찬성 9표가 필요했는데, 이날 표결은 찬성 2표, 반대 0표, 기권 13표였다. 찬성 2표는 러시아와 중국이었던 것으로 전해졌다.\n","\n","린다 토머스-그린필드 유엔주재 미국 대사는 투표에 앞서 우크라이나를 침공한 러시아가 다시 한번 안보리를 이용해 그들의 잔인한 행동을 희석시키려 하고 있다\"고 말했다.\n","\n","그는 \"러시아가 국제사회에 우크라이나의 인도주의적 위기를 해결해 달라고 요청하는 결의를 내놓는 것은 정말 비양심적인 일'이라며 러시아는 악화하는 인도적 조건이나 수백만명의 생명과 꿈이 파괴된 전쟁에는 개의치 않는다. 만약 그들이 신경 쓰고 있다면 그들은 침공을 멈췄을 것\"이라고 강조했다.\n","\n","이 결의안에는 우크라이나에서 벌어지는 인도주의적 위기 해결을 위한 지원 등의 내용이 담겼으나 정작 그 위기를 만든 러시아의 침공에 대해서는 언급되지 않은 것으로 전해졌다.\n","\n","토머스-그린필드 대사는 \"이번 위기의 유일한 원인인 러시아의 역할에 대해서는 언급하지 않았다\"며 \"우리의 투표는 우리가 그 일에 관여하지 않을 것임을 보여줄 것\"이라고 강조했다.\n","\n","그러나 바실리 네벤자 주유엔 러시아 대사는 프랑스와 멕시코 등 다른 나라가 낸 결의안을 '러시아를 비난하기 위한 정치적 쇼'라고 주장하면서도 자국이 제출한 인도적 결의안이 다른 국가가 낸 안과 유사하다고 주장했다.\n","\n","현재 유엔 총회에는 우크라이나 관련 결의안이 2건 상정돼 있다. 유엔은 오는 24일 총회를 통해 결의안에 대한 표결을 진행할 예정이다.</s>\"\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"QNM77to_L2F_","outputId":"a7a1055b-9abb-4cdf-c38d-929e00d462e5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'러시아가 제안한 우크라이나 인도주의 상황에 관한 결의안이 유엔 안전보장이사회(안보리)를 통과하지 못했다. 유엔주재 미국 대사는 결의안에서 인도주의적 위기 해결을 위한 지원 등의 내용이 담겼으나 정작 그 위기를 만든 러시아의 침공에 대해서는 언급되지 않은 것으로 전해졌다. 유엔은 오는 24일 총회를 통해 결의안에 대한 표결을 진행할 예정이다.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["summarization(\"\"\"<s>군집분석(cluster analysis)은 패턴인식, 기계학습 분야에서 자주 활용 되는 다변량 자료분석 알고리즘 중 하나이며 마케팅 분야에서 고객 세분화, 소매업분야에서 새로운 상품 또는 서비스 개발 등 실제 세계의 문제를 푸는데 널리 활용되는 방법론이다.\n","군집분석의 목적은 그룹의 수(clusters)를 정하고 같은 그룹에 할당된 관측치들은 유사한 성질(homogenous)을 갖도록 하고, 반면에 서로 다른 그룹에 할당된 관측치들은 이질적인 성질(heterogeneous)을 갖도록 하는 것이다.\n","군집분석은 다음과 같은 두 가지 사항에 대한 명시적인 답이 존재하지 않아 대표적인 비지도학습(unsupervised learning)의 문제로 간주된다.\n","첫째로 “주어진 데이터에 최적인 군집의 수는 몇 개인가?”라는 질문이며 둘째로는 “데이터의 개별 관측치의 최적의 군집 할당이 존재하는가?”라는 질문이다.\n","위의 두 가지에 질문에 대한 명시적인 답이 없기 때문에 군집분석의 결과를 평가하는 것은 어려운 문제이며 이를 해결하기 위해 다양한 군집 타당성 지표(cluster validity index)가 제안되었다(Halkidi et al., 2002; Halkidi et al., 2002; Liu et al., 2010; Halkidi et al., 2001).\n","이들은 “적절하게 수행된 군집분석의 결과물의 경우, 같은 군집에 할당된 관측치들은 동질적이고 서로 다른 군집에 할당된 관측치들은 이질적이다.”라는 개념을 공유하지만 실제로 군집 타당성 지표들은 위의 성질들을 각기 다른 수식으로 표현하여 결과를 산출한다.\n","하지만 어떤 지표도 모든 군집분석에 대해서 최적의 결과를 판단하지는 못한다고 알려져 있다</s>\"\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"04WVOB7oPEPi","outputId":"91049a14-6a38-4756-d660-28318b3a6220"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'군집분석은 다변량 자료분석 알고리즘 중 하나이며 마케팅 분야에서 고객 세분화, 소매업분야에서 새로운 상품 또는 서비스 개발 등 실제 세계의 문제를 푸는데 널리 활용되는 방법론이다.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["summarization(\"\"\"<s>내비게이션 기술은 다양한 종류의 센서를 이용해 주변 장소에 대한 정보들을 획득하고, 이를 바탕으로 특정 목적을 수행하기 위한 이동 좌표를 결정하는 기술을 말하며, 자율주행 자동차, 탐사로봇 그리고 재난 구조 로봇 등, 많은 로봇 분야에서 응용되고 있다.\n","하지만, 기존의 내비게이션 기술은 미탐사 지역이나 복잡하고 새로운 지역에서 적용하기 어렵다는 단점이 있다.\n","이러한 한계점을 극복하기 위해, 본 연구에서는 동물의 뇌에서 일어나는 장소정보처리 원리를 이용한 새로운 내비게이션 모델을 구축하고자 한다.\n","동물의 해마체(hippocampal formation)은 장소정보를 처리하는데 특화된 영역이다.\n","특히, 환경의 경계면에 대한 정보를 처리하는 경계면 세포(boundary vector cell), 동물의 머리방향에 대한 정보를 처리하는 머리방향 세포(head direction cell), 공간의 격자 좌표 정보를 처리하는 격자 세포(grid cell), 그리고 특정 장소에 대한 정보를 처리하는 장소 세포(place cell)가 발견되었다.\n","이러한 장소정보처리 세포는 해마체 내에서 장소정보처리 신경회로를 통해 신경신호를 공유하며 장소정보를 처리하게 된다.\n","본 연구는, 뇌 장소정보처리 신경세포의 형태학적 구조와 전기생리학적 특성을 정교하게 모사할 수 있는 뉴로모픽 단일신경세포 모델을 구축하고, 해부학적으로 알려진 장소정보처리 신경회로를 기반으로 뉴로모픽 신경망 모델을 구축하는 것을 목적으로 한다.\n","또한, 구축된 뉴로모픽 신경망 모델을 가상 쥐 행동 시뮬레이터에 연결하여, 실제 쥐처럼 공간에서의 내비게이션을 수행할 수 있는지 연구하였다.\n","나아가, 쥐의 행동을 모사하는 쥐 로봇을 개발하고, 뉴로모픽 신경망 모델이 실제 환경에서 로봇을 통해 내비게이션을 수행하는지 관찰하였다.</s>\"\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"h07FmeTnPY03","outputId":"6e5f4785-cfff-4dd1-8341-6537f843d857"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'내비게이션 기술은 다양한 센서를 이용해 주변 장소에 대한 정보를 획득하고, 이를 바탕으로 특정 목적을 수행하기 위한 이동 좌표를 결정하는 기술을 말하며, 많은 로봇 분야에서 응용되고 있으나, 기존의 기술은 미탐사 지역이나 복잡하고 새로운 지역에서 적용하기 어렵다는 단점이 있다.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["summarization(\"\"\"<s>인공지능 기술의 발전과 함께 기술을 적극적으로 활용하는 대표적인 분야 중 하나는 자연어처리 분야이다.\n","특히 BERT 등의 대규모 데이터로 사전 학습한 언어 모델의 발달로 획기적인 발전을 이루었으며, 많은 자연어처리 태스크에서 괄목할만한 성과를 이루었다.\n","그 중 질의응답 태스크로 알려진 기계 독해 문제 또한 대규모 데이터로 사전 학습한 모델로 인하여 문제 해결 능력 면에서 성능이 좋아졌으며, 특히 특정 분야에 특화된 질의응답 시스템은 챗봇 등의 애플리케이션으로 상용화되고 있다.\n","이러한 질의 응답시스템은 전문적인 내용을 포함하고 있으며 보다 정밀한 설계를 필요로 한다.\n","특정 도메인을 위한 질의응답 시스템이 원활히 작동하려면 대답에 확신이 없는 경우 질문을 회피하는 능력이 중요하다.\n","이러한 회피 능력에 대한 성능은 기존 연구에서 제시된 바 없어 본 연구는 실험을 통해 이를 밝히고자 한다.\n","특정 도메인을 위한 질의응답 시스템의 학습 방법은 범용 데이터로 학습된 모델에 도메인 데이터를 추가로 학습시키는 방법과 처음부터 도메인 데이터만으로 학습시키는 방법으로 나뉜다.\n","연구 결과 전자의 방법으로 학습한 모델이 전반적으로 뛰어난 회피 능력을 보였다.\n","또한, 특정 도메인 데이터의 사용 없이 마스킹 전략만을 바꾼 사전 학습 모델의 회피 성능이 도메인 데이터를 사용한 모델과 비슷하거나 상회한다는 사실을 확인했다.</s>\"\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"AnKUgEsbRni0","outputId":"fe78aed0-ca4a-4c1d-9b06-752425eaa59f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'인공지능어처리 분야는 대규모 데이터로 사전 학습한 언어 모델의 발달로 획기적인 발전을 이루었으며, 기계 독해 문제 또한 문제 해결 능력 면에서 성능이 좋아졌으며, 특히 특정 분야에 특화된 질의응답 시스템은 챗봇 등의 애플리케이션으로 상용화되고 있다.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":[],"metadata":{"id":"iSwSVSMXSwuq"},"execution_count":null,"outputs":[]}]}