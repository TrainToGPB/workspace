{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CefqwfbCGMiY"
   },
   "source": [
    "# 한국어 Tokenizing\n",
    "\n",
    "> 작성자      \n",
    "```\n",
    "* 김성현 (bananaband657@gmail.com)  \n",
    "1기 멘토\n",
    "김바다 (qkek983@gmail.com)\n",
    "박상희 (parksanghee0103@gmail.com)  \n",
    "이정우 (jungwoo.l2.rs@gmail.com)\n",
    "2기 멘토\n",
    "박상희 (parksanghee0103@gmail.com)  \n",
    "이정우 (jungwoo.l2.rs@gmail.com)\n",
    "이녕우 (leenw2@gmail.com)\n",
    "박채훈 (qkrcogns2222@gmail.com)\n",
    "3, 4, 5기 멘토\n",
    "이녕우 (leenw2@gmail.com)\n",
    "박채훈 (qkrcogns2222@gmail.com)\n",
    "```\n",
    "[CC BY-NC-ND](https://creativecommons.org/licenses/by-nc-nd/2.0/kr/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdA1hIKn8tOd"
   },
   "source": [
    "###**콘텐츠 라이선스**\n",
    "\n",
    "<font color='red'><b>**WARNING**</b></font> : **본 교육 콘텐츠의 지식재산권은 재단법인 네이버커넥트에 귀속됩니다. 본 콘텐츠를 어떠한 경로로든 외부로 유출 및 수정하는 행위를 엄격히 금합니다.** 다만, 비영리적 교육 및 연구활동에 한정되어 사용할 수 있으나 재단의 허락을 받아야 합니다. 이를 위반하는 경우, 관련 법률에 따라 책임을 질 수 있습니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3w3Aj8wyGSDC"
   },
   "source": [
    "\n",
    "한국어에서의 다양한 tokenizing 방식을 실습해보겠습니다.   \n",
    "\n",
    "한국어는 다음의 단계로 tokenizing이 가능합니다.\n",
    "\n",
    "1. 어절 단위\n",
    "2. 형태소 단위\n",
    "3. 음절 단위\n",
    "4. 자소 단위\n",
    "5. WordPiece 단위"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQo6eH7lHSIs"
   },
   "source": [
    "## 실습용 데이터 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74i60OsjHWxv"
   },
   "source": [
    "실습을 위해 한국어 wikipedia 파일을 가져오도록 하겠습니다.   \n",
    "본 wikipedia 파일은 앞선 전처리 실습을 통해 전처리가 완료된 파일입니다.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1681398669202,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "7hVOadsRHjAg"
   },
   "outputs": [],
   "source": [
    "!mkdir my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 841,
     "status": "ok",
     "timestamp": 1681398670041,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "_3TCM_PGHKzT",
    "outputId": "744564dd-396c-4863-991f-cadee5621df2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 1323k  100 1323k    0     0   919k      0  0:00:01  0:00:01 --:--:-- 3739k\n"
     ]
    }
   ],
   "source": [
    "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" > /dev/null\n",
    "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" -o my_data/wiki_20190620_small.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03NyUBPvH0kv"
   },
   "source": [
    "데이터를 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1681398670041,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "AxajxY2LH2zv"
   },
   "outputs": [],
   "source": [
    "data = open('my_data/wiki_20190620_small.txt', 'r', encoding='utf-8')\n",
    "# 'r' 은 read를 의미합니다.\n",
    "# 본 파일은 encoding format을 UTF-8로 저장했기 때문에, UTF-8로 읽겠습니다.\n",
    "# 한국어는 특히 encoding format이 맞지 않으면, 글자가 깨지는 현상이 나타납니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1681398673671,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "ma6W4JS4IcuV"
   },
   "outputs": [],
   "source": [
    "lines = data.readlines() # 전체 문장을 list에 저장하는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 905,
     "status": "ok",
     "timestamp": 1681398674575,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "L9wGJhBcIiTk",
    "outputId": "e7003ba0-dc3a-43c1-ef80-85c694d8786b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제임스 얼 \"지미\" 카터 주니어는 민주당 출신 미국 39번째 대통령 이다.\n",
      "\n",
      "지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다.\n",
      "\n",
      "조지아 공과대학교를 졸업하였다.\n",
      "\n",
      "그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다.\n",
      "\n",
      "1953년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다.\n",
      "\n",
      "그의 별명이 \"땅콩 농부\" 로 알려졌다.\n",
      "\n",
      "1962년 조지아 주 상원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 되어 당선되고, 1966년 조지아 주 지사 선거에 낙선하지만 1970년 조지아 주 지사를 역임했다.\n",
      "\n",
      "대통령이 되기 전 조지아주 상원의원을 두번 연임했으며, 1971년부터 1975년까지 조지아 지사로 근무했다.\n",
      "\n",
      "조지아 주지사로 지내면서, 미국에 사는 흑인 등용법을 내세웠다.\n",
      "\n",
      "1976년 대통령 선거에 민주당 후보로 출마하여 도덕주의 정책으로 내세워, 포드를 누르고 당선되었다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for line in lines[0:10]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO-HFWPVJT69"
   },
   "source": [
    "## 어절 단위 tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2-BGhE1OGIj"
   },
   "source": [
    "어절 단위 tokenizing은 모든 문장을 띄어쓰기 단위로 분리하는 것을 의미합니다.\n",
    "\n",
    "\"이순신은 조선 중기의 무신이다.\" -> [\"이순신은\", \"조선\", \"중기의\", \"무신이다.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1681398678461,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "te9R8Jg0P-Gp",
    "outputId": "2e6e6629-e857-4a55-ea4d-c3a814f375fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이순신은', '조선', '중기의', '무신이다.']\n"
     ]
    }
   ],
   "source": [
    "text = \"이순신은 조선 중기의 무신이다.\"\n",
    "tokenized_text = text.split(\" \")    # split 함수는 입력 string에 대해서 특정 string을 기반으로 분리해줍니다.\n",
    "print(tokenized_text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DRj6iviWeJp"
   },
   "source": [
    "Tokenizing의 목적은 크게 두 가지입니다.  \n",
    "1. 의미를 지닌 단위로 자연어를 분절\n",
    "2. Model의 학습 시, 동일한 size로 입력\n",
    "\n",
    "따라서, tokenizer는 특정 사이즈로 token의 개수를 조절하는 함수가 필수로 포함되어야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LGjc5YkXbln"
   },
   "source": [
    "이를 위해, token의 개수가 부족할 때는 padding 처리를 해주고,    \n",
    "개수가 많을 때는 token을 잘라서 반환하는 함수를 구현하겠습니다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1681398684081,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "1OGGjfSOXRlQ",
    "outputId": "e47c83d9-13b1-43a7-9985-6d1dcc2c3c38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이순신은', '조선', '중기의', '무신이다.', 'padding', 'padding', 'padding', 'padding', 'padding', 'padding']\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 10\n",
    "# padding\n",
    "tokenized_text += [\"padding\"] * (max_seq_length - len(tokenized_text))\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1681398684081,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "t66zK6CeYD98",
    "outputId": "ed2b7c79-a3ed-40d3-9a16-731ae7d6568a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이순신은', '조선', '중기의']\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 3\n",
    "# filtering\n",
    "tokenized_text = tokenized_text[0:max_seq_length]\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqzv-evGZFax"
   },
   "source": [
    "위 코드를 이용해 tokenizer class를 만들어보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1681398684520,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "ALyO6-0AOFhx"
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer_type_list = [\"word\"]\n",
    "        self.pad_token = \"<pad>\"\n",
    "        self.max_seq_length = 10\n",
    "        self.padding = False\n",
    "    def tokenize(self, text, tokenizer_type): \n",
    "        assert tokenizer_type in self.tokenizer_type_list, \"정의되지 않은 tokenizer_type입니다.\"\n",
    "        if tokenizer_type == \"word\":\n",
    "            tokenized_text = text.split(\" \")\n",
    "        if self.padding:\n",
    "            tokenized_text += [self.pad_token] * (self.max_seq_length - len(tokenized_text))\n",
    "            return tokenized_text[:self.max_seq_length]\n",
    "        else:\n",
    "            return tokenized_text[:self.max_seq_length]\n",
    "    def batch_tokenize(self, texts, tokenizer_type):\n",
    "        for i, text in enumerate(texts):\n",
    "            texts[i] = self.tokenize(text, tokenizer_type)\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1681398686049,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "9tAlYGv-OyzX"
   },
   "outputs": [],
   "source": [
    "my_tokenizer = Tokenizer()\n",
    "my_tokenizer.pad_token = \"[PAD]\"\n",
    "my_tokenizer.max_seq_length = 10\n",
    "my_tokenizer.padding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1681398686477,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "kgkbR-L7O2Ap",
    "outputId": "d107b35e-e8d5-444f-da74-f76103123272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이순신은', '조선', '중기의', '무신이다.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[['이순신은', '조선', '중기의', '무신이다.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['그는', '임진왜란을', '승리로', '이끌었다.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']]\n"
     ]
    }
   ],
   "source": [
    "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"word\"))\n",
    "print(my_tokenizer.batch_tokenize([\"이순신은 조선 중기의 무신이다.\", \"그는 임진왜란을 승리로 이끌었다.\"], \"word\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mUQBL0g9L6u"
   },
   "source": [
    "## 형태소 단위 tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dx5kXtjS_3Ty"
   },
   "source": [
    "형태소 분석기로는 mecab을 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1681398686840,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "3UhD2MyHACDA"
   },
   "outputs": [],
   "source": [
    "# 설치 이슈로 아래 셀을 실행해주세요!\n",
    "# !pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 139777,
     "status": "ok",
     "timestamp": 1681398827250,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "fIYOh6KsoeUb",
    "outputId": "c1c17bbd-e088-4bc3-f843-078314acc6b7"
   },
   "outputs": [],
   "source": [
    "# 위 셀 말고 이 셀을 실행해주세요!\n",
    "# !curl -s https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/mecab-colab.sh | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1681398840935,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "af79UYHWATYj",
    "outputId": "ee7ada89-6c85-40be-fc43-cd415aec7a89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('아버지', 'NNG'), ('가', 'JKS'), ('방', 'NNG'), ('에', 'JKB'), ('들어가', 'VV'), ('신다', 'EP+EF'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab()\n",
    "print(mecab.pos(\"아버지가방에들어가신다.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1681398842831,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "Vzxy7U6oAtCE",
    "outputId": "a4445ff6-7b39-4bbe-b666-e11016b83080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이순신', '은', '조선', '중기', '의', '무신', '이', '다', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"이순신은 조선 중기의 무신이다.\"\n",
    "# 이순신 -> PS\n",
    "# 조선 -> DT TI\n",
    "# 중기 -> TI\n",
    "# 무신 -> OC\n",
    "# 이순신 - 직업 - 무신\n",
    "# 이순신 - 출생지 - 조선\n",
    "\n",
    "tokenized_text = [lemma[0] for lemma in mecab.pos(text)]\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RN6sNjOICGng"
   },
   "source": [
    "형태소 tokenizer도 class에 추가하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1681398845326,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "RYhBL-VpCF6O"
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer_type_list = [\"word\", \"morph\"]\n",
    "        self.pad_token = \"<pad>\"\n",
    "        self.max_seq_length = 10\n",
    "        self.padding = False\n",
    "    def tokenize(self, text, tokenizer_type): \n",
    "        assert tokenizer_type in self.tokenizer_type_list, \"정의되지 않은 tokenizer_type입니다.\"\n",
    "        if tokenizer_type == \"word\":\n",
    "            tokenized_text = text.split(\" \")\n",
    "        ###########################################################\n",
    "        # morph인 경우 형태소 단위로 토큰화\n",
    "        elif tokenizer_type == \"morph\":\n",
    "            tokenized_text = [lemma[0] for lemma in mecab.pos(text)]\n",
    "        ###########################################################\n",
    "        if self.padding:\n",
    "            tokenized_text += [self.pad_token] * (self.max_seq_length - len(tokenized_text))\n",
    "            return tokenized_text[:self.max_seq_length]\n",
    "        else:\n",
    "            return tokenized_text[:self.max_seq_length]\n",
    "    def batch_tokenize(self, texts, tokenizer_type):\n",
    "        for i, text in enumerate(texts):\n",
    "            texts[i] = self.tokenize(text, tokenizer_type)\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1681398845586,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "fMas8FOVF94u"
   },
   "outputs": [],
   "source": [
    "my_tokenizer = Tokenizer()\n",
    "my_tokenizer.pad_token = \"[PAD]\"\n",
    "my_tokenizer.max_seq_length = 10\n",
    "my_tokenizer.padding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1681398847311,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "1NJ5PgGoCSie",
    "outputId": "e91600c2-f818-4b24-ac50-02818d498d5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이순신', '은', '조선', '중기', '의', '무신', '이', '다', '.', '[PAD]']\n",
      "[['이순신', '은', '조선', '중기', '의', '무신', '이', '다', '.', '[PAD]'], ['그', '는', '임진왜란', '을', '승리', '로', '이끌', '었', '다', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"morph\"))\n",
    "print(my_tokenizer.batch_tokenize([\"이순신은 조선 중기의 무신이다.\", \"그는 임진왜란을 승리로 이끌었다.\"], \"morph\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ta-DT6gdbjwB"
   },
   "source": [
    "## 음절 단위 tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--hnlgWmCaCm"
   },
   "source": [
    "음절 단위 tokenizing은 한 자연어를 한 글자씩 분리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1681398849959,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "rV8SNdlxb_GS",
    "outputId": "d024c943-704d-40e5-813b-bd5106e78dfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '순', '신', '은', ' ', '조', '선', ' ', '중', '기', '의', ' ', '무', '신', '이', '다', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"이순신은 조선 중기의 무신이다.\"\n",
    "tokenized_text = list(text)    # split 함수는 입력 string에 대해서 특정 string을 기반으로 분리해줍니다.\n",
    "print(tokenized_text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1681398851408,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "H7fr-5HN-7Yx"
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer_type_list = [\"word\", \"morph\", \"syllable\"]\n",
    "        self.pad_token = \"<pad>\"\n",
    "        self.max_seq_length = 10\n",
    "        self.padding = False\n",
    "    def tokenize(self, text, tokenizer_type): \n",
    "        assert tokenizer_type in self.tokenizer_type_list, \"정의되지 않은 tokenizer_type입니다.\"\n",
    "        if tokenizer_type == \"word\":\n",
    "            tokenized_text = text.split(\" \")\n",
    "        elif tokenizer_type == \"morph\":\n",
    "            tokenized_text = [lemma[0] for lemma in mecab.pos(text)]\n",
    "        #########################################\n",
    "        # syllable인 경우 음절 단위 분리\n",
    "        elif tokenizer_type == \"syllable\":\n",
    "            tokenized_text = list(text)\n",
    "        #########################################\n",
    "        if self.padding:\n",
    "            tokenized_text += [self.pad_token] * (self.max_seq_length - len(tokenized_text))\n",
    "            return tokenized_text[:self.max_seq_length]\n",
    "        else:\n",
    "            return tokenized_text[:self.max_seq_length]\n",
    "    def batch_tokenize(self, texts, tokenizer_type):\n",
    "        for i, text in enumerate(texts):\n",
    "            texts[i] = self.tokenize(text, tokenizer_type)\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1681398852460,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "4udPQal-_s_M"
   },
   "outputs": [],
   "source": [
    "my_tokenizer = Tokenizer()\n",
    "my_tokenizer.pad_token = \"[PAD]\"\n",
    "my_tokenizer.max_seq_length = 20\n",
    "my_tokenizer.padding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1681398853557,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "xXcB2P1j_vdk",
    "outputId": "bcd20195-aa6a-4fb2-afbb-7fbf34c0758e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '순', '신', '은', ' ', '조', '선', ' ', '중', '기', '의', ' ', '무', '신', '이', '다', '.', '[PAD]', '[PAD]', '[PAD]']\n",
      "[['이', '순', '신', '은', ' ', '조', '선', ' ', '중', '기', '의', ' ', '무', '신', '이', '다', '.', '[PAD]', '[PAD]', '[PAD]'], ['그', '는', ' ', '임', '진', '왜', '란', '을', ' ', '승', '리', '로', ' ', '이', '끌', '었', '다', '.', '[PAD]', '[PAD]']]\n"
     ]
    }
   ],
   "source": [
    "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"syllable\"))\n",
    "print(my_tokenizer.batch_tokenize([\"이순신은 조선 중기의 무신이다.\", \"그는 임진왜란을 승리로 이끌었다.\"], \"syllable\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ruy_UuG8DA5t"
   },
   "source": [
    "## 자소 단위 tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5InCTAaDESD"
   },
   "source": [
    "한글은 하나의 문자도 최대 초성, 중성, 종성, 총 3개의 자소로 분리가 가능합니다.   \n",
    "실습에서는 자소 분리를 위해 hgtk 라이브러리를 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5953,
     "status": "ok",
     "timestamp": 1681398861690,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "WzhVFKORDQSu",
    "outputId": "19d432e3-efd8-4748-b288-f9f65740bec1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hgtk\n",
      "  Downloading hgtk-0.2.0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: hgtk\n",
      "Successfully installed hgtk-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install hgtk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1681398866520,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "j2o0uPPtDzUf"
   },
   "outputs": [],
   "source": [
    "import hgtk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1681398867683,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "fIv84Y-IDnV1",
    "outputId": "0077b7b4-82e3-4b5b-c9d3-3db3729b2f09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ㅇ', 'ㅣ', 'ᴥ', 'ㅅ', 'ㅜ', 'ㄴ', 'ᴥ', 'ㅅ', 'ㅣ', 'ㄴ', 'ᴥ', 'ㅇ', 'ㅡ', 'ㄴ', 'ᴥ', ' ', 'ㅈ', 'ㅗ', 'ᴥ', 'ㅅ', 'ㅓ', 'ㄴ', 'ᴥ', ' ', 'ㅈ', 'ㅜ', 'ㅇ', 'ᴥ', 'ㄱ', 'ㅣ', 'ᴥ', 'ㅇ', 'ㅢ', 'ᴥ', ' ', 'ㅁ', 'ㅜ', 'ᴥ', 'ㅅ', 'ㅣ', 'ㄴ', 'ᴥ', 'ㅇ', 'ㅣ', 'ᴥ', 'ㄷ', 'ㅏ', 'ᴥ', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"이순신은 조선 중기의 무신이다.\"\n",
    "tokenized_text = list(hgtk.text.decompose(text))\n",
    "print(tokenized_text)\n",
    "# ㅇ ㅣ ㅅ ㅜ ㄴ ㅅ ㅣ ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1681398868800,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "cqlMZJgXD_gz"
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer_type_list = [\"word\", \"morph\", \"syllable\", \"jaso\"]\n",
    "        self.pad_token = \"<pad>\"\n",
    "        self.max_seq_length = 10\n",
    "        self.padding = False\n",
    "    def tokenize(self, text, tokenizer_type): \n",
    "        assert tokenizer_type in self.tokenizer_type_list, \"정의되지 않은 tokenizer_type입니다.\"\n",
    "        if tokenizer_type == \"word\":\n",
    "            tokenized_text = text.split(\" \")\n",
    "        elif tokenizer_type == \"morph\":\n",
    "            tokenized_text = [lemma[0] for lemma in mecab.pos(text)]\n",
    "        elif tokenizer_type == \"syllable\":\n",
    "            tokenized_text = list(text)\n",
    "        ####################################################\n",
    "        # jaso인 경우 자소 단위로 분리\n",
    "        elif tokenizer_type == \"jaso\":\n",
    "            tokenized_text = list(hgtk.text.decompose(text))\n",
    "        ####################################################\n",
    "        if self.padding:\n",
    "            tokenized_text += [self.pad_token] * (self.max_seq_length - len(tokenized_text))\n",
    "            return tokenized_text[:self.max_seq_length]\n",
    "        else:\n",
    "            return tokenized_text[:self.max_seq_length]\n",
    "    def batch_tokenize(self, texts, tokenizer_type):\n",
    "        for i, text in enumerate(texts):\n",
    "            texts[i] = self.tokenize(text, tokenizer_type)\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1681398871467,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "StfkrgZvEIY2"
   },
   "outputs": [],
   "source": [
    "my_tokenizer = Tokenizer()\n",
    "my_tokenizer.pad_token = \"[PAD]\"\n",
    "my_tokenizer.max_seq_length = 20\n",
    "my_tokenizer.padding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1681398871986,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "8-xKOWtpEKdB",
    "outputId": "6f690c28-7c77-4e05-aaa1-a6a7566b258a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ㅇ', 'ㅣ', 'ᴥ', 'ㅅ', 'ㅜ', 'ㄴ', 'ᴥ', 'ㅅ', 'ㅣ', 'ㄴ', 'ᴥ', 'ㅇ', 'ㅡ', 'ㄴ', 'ᴥ', ' ', 'ㅈ', 'ㅗ', 'ᴥ', 'ㅅ']\n",
      "[['ㅇ', 'ㅣ', 'ᴥ', 'ㅅ', 'ㅜ', 'ㄴ', 'ᴥ', 'ㅅ', 'ㅣ', 'ㄴ', 'ᴥ', 'ㅇ', 'ㅡ', 'ㄴ', 'ᴥ', ' ', 'ㅈ', 'ㅗ', 'ᴥ', 'ㅅ'], ['ㄱ', 'ㅡ', 'ᴥ', 'ㄴ', 'ㅡ', 'ㄴ', 'ᴥ', ' ', 'ㅇ', 'ㅣ', 'ㅁ', 'ᴥ', 'ㅈ', 'ㅣ', 'ㄴ', 'ᴥ', 'ㅇ', 'ㅙ', 'ᴥ', 'ㄹ']]\n"
     ]
    }
   ],
   "source": [
    "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"jaso\"))\n",
    "print(my_tokenizer.batch_tokenize([\"이순신은 조선 중기의 무신이다.\", \"그는 임진왜란을 승리로 이끌었다.\"], \"jaso\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2ppAG0FGS8h"
   },
   "source": [
    "## WordPiece tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13438,
     "status": "ok",
     "timestamp": 1681398886333,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "SWut4yYlnFH-",
    "outputId": "705ce254-fd71-4406-aae3-e41050a11061"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1681398888445,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "szyYpZ0DRQT3"
   },
   "outputs": [],
   "source": [
    "!mkdir wordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1106,
     "status": "ok",
     "timestamp": 1681398890808,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "yZB0YedROqe0",
    "outputId": "0a023725-934c-4f56-ad0c-7425f6927e13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['wordPieceTokenizer/my_tokenizer-vocab.txt']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# Initialize an empty tokenizer\n",
    "wp_tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True, # [이순신, ##은, ' ', 조선]\n",
    "    handle_chinese_chars=True, # 본문 내에 존재하는 한자들이 모두 음절단위로 분리\n",
    "    strip_accents=False, # True: [YepHamza] -> [Yep, Hamza] (띄어쓰기 없이 대문자로 붙어있는 단어들을 분리)\n",
    "    lowercase=False, # 모든 알파벳 소문자화\n",
    ")\n",
    "\n",
    "# And then train\n",
    "wp_tokenizer.train(\n",
    "    files=\"my_data/wiki_20190620_small.txt\", # 위키 데이터를 통해서 토크나이저를 직접 만들게 됨(wordPiece 단위 = BPE)\n",
    "    vocab_size=10000, # 최대 10000개까지. 이게 값이 커지면 거의 음절 단위로 잘려짐(개수를 만족할 때까지 계속 돌기 때문)\n",
    "    min_frequency=2, # 두 번 이상 반복되는 단어에 대해서만 subword화\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"], # 내부적으로 이미 정의가 되어있음(BertWordPieceTokenizer)\n",
    "    limit_alphabet=1000, # \n",
    "    wordpieces_prefix=\"##\"\n",
    ")\n",
    "\n",
    "# Save the files\n",
    "wp_tokenizer.save_model(\"wordPieceTokenizer\", \"my_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1681398891057,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "SmRK4AFFIXBg",
    "outputId": "322470f3-97e8-4dd5-ee5d-eabc56ae2af5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(wp_tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1681398893197,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "kG7GtaevKRLH",
    "outputId": "5bd20346-eb0b-4681-80be-5c391b35ae97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=10, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "['이', '##순', '##신은', '조선', '중', '##기의', '무', '##신이', '##다', '.']\n",
      "[705, 1021, 7619, 2001, 753, 2603, 453, 8506, 1059, 16]\n"
     ]
    }
   ],
   "source": [
    "text = \"이순신은 조선 중기의 무신이다.\"\n",
    "tokenized_text = wp_tokenizer.encode(text)\n",
    "print(tokenized_text)\n",
    "print(tokenized_text.tokens)\n",
    "print(tokenized_text.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1681398893198,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "VfEKVuu6GnW7"
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer_type_list = [\"word\", \"morph\", \"syllable\", \"jaso\", \"wordPiece\"]\n",
    "        self.pad_token = \"<pad>\"\n",
    "        self.max_seq_length = 10\n",
    "        self.padding = False\n",
    "    def tokenize(self, text, tokenizer_type): \n",
    "        assert tokenizer_type in self.tokenizer_type_list, \"정의되지 않은 tokenizer_type입니다.\"\n",
    "        if tokenizer_type == \"word\":\n",
    "            tokenized_text = text.split(\" \")\n",
    "        elif tokenizer_type == \"morph\":\n",
    "            tokenized_text = [lemma[0] for lemma in mecab.pos(text)]\n",
    "        elif tokenizer_type == \"syllable\":\n",
    "            tokenized_text = list(text)\n",
    "        elif tokenizer_type == \"jaso\":\n",
    "            tokenized_text = list(hgtk.text.decompose(text))\n",
    "        #####################################################\n",
    "        # BertTokenizer의 wordPiece 단위 tokenizer\n",
    "        elif tokenizer_type == \"wordPiece\":\n",
    "            tokenized_text = wp_tokenizer.encode(text).tokens\n",
    "        #####################################################\n",
    "        if self.padding:\n",
    "            tokenized_text += [self.pad_token] * (self.max_seq_length - len(tokenized_text))\n",
    "            return tokenized_text[:self.max_seq_length]\n",
    "        else:\n",
    "            return tokenized_text[:self.max_seq_length]\n",
    "    def batch_tokenize(self, texts, tokenizer_type):\n",
    "        for i, text in enumerate(texts):\n",
    "            texts[i] = self.tokenize(text, tokenizer_type)\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 387,
     "status": "ok",
     "timestamp": 1681398895696,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "anIwM9sZR_u8"
   },
   "outputs": [],
   "source": [
    "my_tokenizer = Tokenizer()\n",
    "my_tokenizer.pad_token = \"[PAD]\"\n",
    "my_tokenizer.max_seq_length = 10\n",
    "my_tokenizer.padding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1681398895696,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "vG8zOIN1SBLj",
    "outputId": "58e17cd8-2080-49fd-9038-9f9b1f041770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '##순', '##신은', '조선', '중', '##기의', '무', '##신이', '##다', '.']\n",
      "[['이', '##순', '##신은', '조선', '중', '##기의', '무', '##신이', '##다', '.'], ['그는', '임진', '##왜', '##란을', '승리', '##로', '이끌었다', '.', '[PAD]', '[PAD]']]\n"
     ]
    }
   ],
   "source": [
    "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"wordPiece\"))\n",
    "print(my_tokenizer.batch_tokenize([\"이순신은 조선 중기의 무신이다.\", \"그는 임진왜란을 승리로 이끌었다.\"], \"wordPiece\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RumHmOm4nH5C"
   },
   "source": [
    "구현된 tokenizing 함수들을 모두 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1681398895696,
     "user": {
      "displayName": "NW Lee",
      "userId": "09379932391728727752"
     },
     "user_tz": -540
    },
    "id": "05tvdp1uSEss",
    "outputId": "d12d7a09-31fe-48fb-d3d3-7df1f403ae02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이순신은', '조선', '중기의', '무신이다.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['이순신', '은', '조선', '중기', '의', '무신', '이', '다', '.', '[PAD]']\n",
      "['이', '순', '신', '은', ' ', '조', '선', ' ', '중', '기']\n",
      "['ㅇ', 'ㅣ', 'ᴥ', 'ㅅ', 'ㅜ', 'ㄴ', 'ᴥ', 'ㅅ', 'ㅣ', 'ㄴ']\n",
      "['이', '##순', '##신은', '조선', '중', '##기의', '무', '##신이', '##다', '.']\n"
     ]
    }
   ],
   "source": [
    "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"word\"))\n",
    "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"morph\"))\n",
    "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"syllable\"))\n",
    "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"jaso\"))\n",
    "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"wordPiece\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgIH8nwOSeP3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
