{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrzmQff3nU6z"
   },
   "source": [
    "# GPT-2 학습해보기\n",
    "\n",
    "> 작성자      \n",
    "```\n",
    "* 김성현 (bananaband657@gmail.com)  \n",
    "1기 멘토\n",
    "김바다 (qkek983@gmail.com)\n",
    "박상희 (parksanghee0103@gmail.com)  \n",
    "이정우 (jungwoo.l2.rs@gmail.com)\n",
    "2기 멘토\n",
    "박상희 (parksanghee0103@gmail.com)  \n",
    "이정우 (jungwoo.l2.rs@gmail.com)\n",
    "이녕우 (leenw2@gmail.com)\n",
    "박채훈 (qkrcogns2222@gmail.com)\n",
    "3, 4, 5기 멘토\n",
    "이녕우 (leenw2@gmail.com)\n",
    "박채훈 (qkrcogns2222@gmail.com)\n",
    "```\n",
    "[CC BY-NC-ND](https://creativecommons.org/licenses/by-nc-nd/2.0/kr/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKtjztp98gJo"
   },
   "source": [
    "###**콘텐츠 라이선스**\n",
    "\n",
    "<font color='red'><b>**WARNING**</b></font> : **본 교육 콘텐츠의 지식재산권은 재단법인 네이버커넥트에 귀속됩니다. 본 콘텐츠를 어떠한 경로로든 외부로 유출 및 수정하는 행위를 엄격히 금합니다.** 다만, 비영리적 교육 및 연구활동에 한정되어 사용할 수 있으나 재단의 허락을 받아야 합니다. 이를 위반하는 경우, 관련 법률에 따라 책임을 질 수 있습니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmoeUd5qnaq9"
   },
   "source": [
    "이번 시간엔 한국어 코퍼스를 활용해, 직접 한국어 GPT-2를 학습해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7800,
     "status": "ok",
     "timestamp": 1617978776452,
     "user": {
      "displayName": "바나나인간",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhCM2sOGSCeogj8b2W7aPl7KKywidts5H45gy6vCA=s64",
      "userId": "05069217733258421588"
     },
     "user_tz": -540
    },
    "id": "id8FcYRa48Gc",
    "outputId": "c5259169-44d5-458d-cc0b-cfcf11443cf1"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9226,
     "status": "ok",
     "timestamp": 1617978778870,
     "user": {
      "displayName": "바나나인간",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhCM2sOGSCeogj8b2W7aPl7KKywidts5H45gy6vCA=s64",
      "userId": "05069217733258421588"
     },
     "user_tz": -540
    },
    "id": "lrihNAMK5EsC",
    "outputId": "d571163c-0e2d-44fa-a130-c68b9973c28f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbfDhMRUC-OL"
   },
   "source": [
    "역시 위키 데이터를 가져와볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3954,
     "status": "ok",
     "timestamp": 1617978756095,
     "user": {
      "displayName": "바나나인간",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhCM2sOGSCeogj8b2W7aPl7KKywidts5H45gy6vCA=s64",
      "userId": "05069217733258421588"
     },
     "user_tz": -540
    },
    "id": "2UYtcoe0C9QT",
    "outputId": "5fc2896f-547c-42c4-db0e-949cd7c010ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘my_data’: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 1323k  100 1323k    0     0   689k      0  0:00:01  0:00:01 --:--:-- 2792k\n"
     ]
    }
   ],
   "source": [
    "!mkdir my_data\n",
    "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" > /dev/null\n",
    "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" -o my_data/wiki_20190620_small.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kB-X8eCflkD-"
   },
   "outputs": [],
   "source": [
    "path = \"./my_data/wiki_20190620_small.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAIHClHAgz5U"
   },
   "source": [
    "지금까지는 BertWordPieceTokenizer를 사용해왔다면,   \n",
    "이번에는 SentencePiceBPETokenizer를 사용해 모델을 학습해보겠습니다.\n",
    "\n",
    "각 tokenizer의 차이는 허훈님의 블로그 [여기](https://huffon.github.io/2020/07/05/tokenizers/) 에서 확인하실 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cC7g-nhTu0LN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from tokenizers.normalizers import BertNormalizer\n",
    "\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "\n",
    "tokenizer._tokenizer.normalizer = BertNormalizer(clean_text=True,\n",
    "handle_chinese_chars=False,\n",
    "lowercase=False)\n",
    "\n",
    "tokenizer.train(\n",
    "    path,\n",
    "    vocab_size=10000,\n",
    "    special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 486,
     "status": "ok",
     "timestamp": 1617978787584,
     "user": {
      "displayName": "바나나인간",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhCM2sOGSCeogj8b2W7aPl7KKywidts5H45gy6vCA=s64",
      "userId": "05069217733258421588"
     },
     "user_tz": -540
    },
    "id": "irrc7HKq8kpi",
    "outputId": "86cb8c24-16a4-41b9-f60e-7db58c9bc8fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "[1005, 578, 6613, 1303, 1041, 2071, 1136, 595, 1033]\n",
      "['▁이', '순', '신은', '▁조선', '▁중', '기의', '▁무', '신', '이다.']\n",
      "이순신은 조선 중기의 무신이다.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\"))\n",
    "print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\").ids)\n",
    "print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\").tokens)\n",
    "print(tokenizer.decode(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").ids, skip_special_tokens=True))\n",
    "# SentencePiece를 사용하면, 나중에 decoding 과정에서 '_' 만 ' '로 replace해주면 띄어쓰기 복원이 가능해집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQyn3mpR-YiI",
    "outputId": "36686fea-be94-4f5f-b4cd-b07994899679"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./vocab.json', './merges.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_model(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHbGAlyODPaB"
   },
   "outputs": [],
   "source": [
    "tokenizer = SentencePieceBPETokenizer.from_file(vocab_filename=\"vocab.json\", merges_filename=\"merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p63PBpMZtNii",
    "outputId": "2647d910-9e44-4b0a-f7b7-34f5dc7b303d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "[1005, 578, 6613, 1303, 1041, 2071, 1136, 595, 1033]\n",
      "['▁이', '순', '신은', '▁조선', '▁중', '기의', '▁무', '신', '이다.']\n",
      "['▁<', 's', '>', '이', '순', '신은', '▁조선', '▁중', '기의', '▁무', '신', '이다.', '<', '/s', '>']\n",
      "<s>이순신은 조선 중기의 무신이다.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\"))\n",
    "print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\").ids)\n",
    "print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\").tokens)\n",
    "print(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").tokens)\n",
    "print(tokenizer.decode(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eyf_iqsnDa9-",
    "outputId": "ba27102a-ccab-4872-df89-cf54afede345"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1005, 578, 6613, 1303, 1041, 2071, 1136, 595, 1033, 2]\n",
      "['<s>', '▁이', '순', '신은', '▁조선', '▁중', '기의', '▁무', '신', '이다.', '</s>']\n",
      "이순신은 조선 중기의 무신이다.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_special_tokens([\"<s>\", \"</s>\", \"<unk>\", \"<pad>\", \"<shkim>\"])\n",
    "tokenizer.pad_token_id = tokenizer.token_to_id(\"<pad>\")\n",
    "tokenizer.unk_token_id = tokenizer.token_to_id(\"<unk>\")\n",
    "tokenizer.bos_token_id = tokenizer.token_to_id(\"<bos>\")\n",
    "tokenizer.eos_token_id = tokenizer.token_to_id(\"<eos>\")\n",
    "\n",
    "print(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").ids)\n",
    "print(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").tokens)\n",
    "print(tokenizer.decode(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHWcg4ba7E-U"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "# creating the configurations from which the model can be made\n",
    "config = GPT2Config(\n",
    "  vocab_size=tokenizer.get_vocab_size(),\n",
    "  bos_token_id=tokenizer.token_to_id(\"<s>\"),\n",
    "  eos_token_id=tokenizer.token_to_id(\"</s>\"),\n",
    ")\n",
    "# creating the model\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IhsrdEnd7Xur",
    "outputId": "a88f33c2-58c4-4b89-bdb6-b7470a5ef254"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93522432"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMcqoMGE1p3A"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "from filelock import FileLock\n",
    "\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from transformers.utils import logging\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach soon.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        file_path: str,\n",
    "        block_size: int,\n",
    "        overwrite_cache=False,\n",
    "        cache_dir: Optional[str] = None,\n",
    "    ):\n",
    "        assert os.path.isfile(file_path), f\"Input file path {file_path} not found\"\n",
    "\n",
    "        block_size = block_size - tokenizer.num_special_tokens_to_add(is_pair=False)\n",
    "\n",
    "        directory, filename = os.path.split(file_path)\n",
    "        cached_features_file = os.path.join(\n",
    "            cache_dir if cache_dir is not None else directory,\n",
    "            \"cached_lm_{}_{}_{}\".format(\n",
    "                tokenizer.__class__.__name__,\n",
    "                str(block_size),\n",
    "                filename,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Make sure only the first process in distributed training processes the dataset,\n",
    "        # and the others will use the cache.\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "        with FileLock(lock_path):\n",
    "\n",
    "            if os.path.exists(cached_features_file) and not overwrite_cache:\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"rb\") as handle:\n",
    "                    self.examples = pickle.load(handle)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                logger.info(f\"Creating features from dataset file at {directory}\")\n",
    "                # 여기서부터 본격적으로 데이터셋을 만들기 시작합니다.\n",
    "                self.examples = []\n",
    "                text = \"\"\n",
    "                with open(file_path, encoding=\"utf-8\") as f:\n",
    "                    lines = f.readlines()\n",
    "                    for line in lines:\n",
    "                        line = line.strip()\n",
    "                        line = \"<s>\"+line+\"</s>\" # 학습 데이터 앞 뒤에 문장 구분 기호를 추가해줍니다.\n",
    "                        text += line    # 'text' 객체에 모든 학습 데이터를 다 합쳐버립니다 :-)\n",
    "                tokenized_text = tokenizer.encode(text).ids\n",
    "\n",
    "                # 모델의 최대 sequence length만큼 데이터를 잘라서 저장합니다.\n",
    "                for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n",
    "                    self.examples.append(\n",
    "                        tokenized_text[i : i + block_size]\n",
    "                    )\n",
    "                # Note that we are losing the last truncated example here for the sake of simplicity (no padding)\n",
    "                # If your dataset is small, first you should look for a bigger one :-) and second you\n",
    "                # can change this behavior by adding (model specific) padding.\n",
    "\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"wb\") as handle:\n",
    "                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                logger.info(\n",
    "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3mSeU-40mmG"
   },
   "outputs": [],
   "source": [
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=path,\n",
    "    block_size=128,\n",
    ")\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(    # GPT는 생성모델이기 때문에 [MASK] 가 필요 없습니다 :-)\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ghC6K4AX6UbY",
    "outputId": "55004b91-e95c-43ef-8a4e-2869120573f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   0, 3997, 3546, 8404,  464,    4, 5480, 9527, 1798, 1890, 2297, 1262,\n",
      "        9626, 2679, 1188, 2174,    2,    0, 5708, 5480,  255, 6466,  750, 3426,\n",
      "         872, 1556,  680,  895, 1626, 9223,  587, 3621, 1010, 3303,    2,    0,\n",
      "        6466, 7418, 2305,  404, 2217, 1074,    2,    0, 1013, 1107, 3716,  646,\n",
      "        8574, 1024,  940,   92, 7323,  372,   92,  721, 9295,  705, 1651,  454,\n",
      "        3166, 1032, 1074,    2,    0, 6343, 1262, 3716, 1009, 2932, 1176,  913,\n",
      "        2036, 1171, 3227,  843,   92,  440,  974, 1486, 1017,    3, 1323, 3914,\n",
      "        2095, 1042,    2,    0, 1383, 2068, 2225, 1095,  327,  843, 1824,  507,\n",
      "           4, 1240, 7698,    2,    0, 3897, 6466, 1053, 1077,  686, 2318, 4649,\n",
      "        5204, 5671, 1013, 1759,  115, 2742, 3004,  104,  655, 2283, 9765, 1192,\n",
      "        1796, 2449, 2546, 9939, 6466, 1053, 1037,  534])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEGfg7JL7KWJ"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='model_output',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=64, # 512:32  # 128:64\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwdMy08j7u-F"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpeIS3C2ipTZ"
   },
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jxg4-f3P7zh2"
   },
   "outputs": [],
   "source": [
    "USE_GPU = 1\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if (torch.cuda.is_available() and USE_GPU) else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XKGRHjzya13d",
    "outputId": "5b881c98-3fc9-4e33-c4d2-c767c36b695a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATED SEQUENCE : 이순신의 반은 인으로 데인 반라도 가장 많이 오자가 되었다.\n",
      "GENERATED SEQUENCE : 이순신적인 것은 '사가 큰 도문에 비해 만들어 것을 하는 것이다.\n",
      "GENERATED SEQUENCE : 이순신가 지 두 원을 시작은 그의 정아야체의 같은 리의 때문에 191인 때 전로 사용하여, 그는 \"용카키레한 후키에 의해 위해 “트를 \"카라 가져, 로예트를 고통되어 중국 비디오 대전할 수 없는 「스 바린 시작되어 여러 차례 5월인 독일·자한 자전적인 소설라키의 중국계리문이다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "input_ids = torch.tensor(tokenizer.encode(\"<s>이순신\", add_special_tokens=True).ids).unsqueeze(0).to('cuda')\n",
    "\n",
    "output_sequences = model.generate(input_ids=input_ids, do_sample=True, max_length=100, num_return_sequences=3)\n",
    "for generated_sequence in output_sequences:\n",
    "    generated_sequence = generated_sequence.tolist()\n",
    "    print(\"GENERATED SEQUENCE : {0}\".format(tokenizer.decode(generated_sequence, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xUL4-4v8IONB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "ml2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
