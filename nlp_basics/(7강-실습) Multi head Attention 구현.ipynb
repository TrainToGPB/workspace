{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KsBGZpKkWki"
   },
   "source": [
    "##**7. Multi-head Attention**\n",
    "1. Multi-head attention 및 self-attention 구현.\n",
    "2. 각 과정에서 일어나는 연산과 input/output 형태 이해."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qRU5DFY2OM8"
   },
   "source": [
    "### **필요 패키지 import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lDtMioSQQ1bB"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBiZObgRep_Q"
   },
   "source": [
    "### **데이터 전처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "e9ULZIqTenSc"
   },
   "outputs": [],
   "source": [
    "pad_id = 0\n",
    "vocab_size = 100\n",
    "\n",
    "data = [\n",
    "  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n",
    "  [60, 96, 51, 32, 90],\n",
    "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n",
    "  [75, 51],\n",
    "  [66, 88, 98, 47],\n",
    "  [21, 39, 10, 64, 21],\n",
    "  [98],\n",
    "  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n",
    "  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n",
    "  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6Hx3mcivgMyH"
   },
   "outputs": [],
   "source": [
    "def padding(data):\n",
    "  max_len = len(max(data, key=len))\n",
    "  print(f\"Maximum sequence length: {max_len}\")\n",
    "\n",
    "  for i, seq in enumerate(tqdm(data)):\n",
    "    if len(seq) < max_len:\n",
    "      data[i] = seq + [pad_id] * (max_len - len(seq))\n",
    "\n",
    "  return data, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s3e8FiNvgX60",
    "outputId": "06f18a9b-37c6-46bb-d6f8-88d893555c48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 190650.18it/s]\n"
     ]
    }
   ],
   "source": [
    "data, max_len = padding(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hwPSIWYugaN0",
    "outputId": "8e7fd9c3-e892-410f-dbf3-f4c55da63ec0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0],\n",
       " [60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [77,\n",
       "  65,\n",
       "  51,\n",
       "  77,\n",
       "  19,\n",
       "  15,\n",
       "  35,\n",
       "  19,\n",
       "  23,\n",
       "  97,\n",
       "  50,\n",
       "  46,\n",
       "  53,\n",
       "  42,\n",
       "  45,\n",
       "  91,\n",
       "  66,\n",
       "  3,\n",
       "  43,\n",
       "  10],\n",
       " [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0],\n",
       " [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwqjACx8iidc"
   },
   "source": [
    "### **Hyperparameter 세팅 및 embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "p-Ngp2nWimS8"
   },
   "outputs": [],
   "source": [
    "d_model = 512  # model의 hidden size\n",
    "num_heads = 8  # head의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GJMi2Xsni5uq"
   },
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "# B: batch size, L: maximum sequence length\n",
    "batch = torch.LongTensor(data)  # (B, L)\n",
    "batch_emb = embedding(batch)  # (B, L, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3tLCUQwojcUb",
    "outputId": "9c0aa33e-e215-49dd-dba7-08b1e61d9afb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.3829,  0.2615, -0.2509,  ...,  0.7878,  0.7341, -1.5561],\n",
      "         [-1.5745,  1.2790,  0.6530,  ...,  3.2941,  0.0916, -0.4197],\n",
      "         [ 0.3946, -0.9075,  0.1039,  ..., -1.3712,  2.2372,  0.6184],\n",
      "         ...,\n",
      "         [-0.9100, -0.1834,  0.1774,  ..., -2.0337,  1.1422,  0.5385],\n",
      "         [-0.9100, -0.1834,  0.1774,  ..., -2.0337,  1.1422,  0.5385],\n",
      "         [-0.9100, -0.1834,  0.1774,  ..., -2.0337,  1.1422,  0.5385]],\n",
      "\n",
      "        [[ 0.2132, -0.2783,  0.8494,  ...,  1.3866, -0.3096,  0.1386],\n",
      "         [ 0.8789, -0.1996,  0.8290,  ..., -0.3165,  1.2627,  0.0771],\n",
      "         [-1.1631,  0.5746, -0.9279,  ...,  0.5000,  0.1728,  0.4771],\n",
      "         ...,\n",
      "         [-0.9100, -0.1834,  0.1774,  ..., -2.0337,  1.1422,  0.5385],\n",
      "         [-0.9100, -0.1834,  0.1774,  ..., -2.0337,  1.1422,  0.5385],\n",
      "         [-0.9100, -0.1834,  0.1774,  ..., -2.0337,  1.1422,  0.5385]],\n",
      "\n",
      "        [[ 2.5604,  1.4206,  0.3539,  ..., -0.1646, -0.0159,  1.1848],\n",
      "         [ 0.3831,  0.7947, -1.1260,  ...,  0.1265, -0.5203,  0.4472],\n",
      "         [-0.2835, -0.7978, -0.0833,  ...,  1.9911, -0.2928, -0.4508],\n",
      "         ...,\n",
      "         [-0.9100, -0.1834,  0.1774,  ..., -2.0337,  1.1422,  0.5385],\n",
      "         [-0.9100, -0.1834,  0.1774,  ..., -2.0337,  1.1422,  0.5385],\n",
      "         [-0.9100, -0.1834,  0.1774,  ..., -2.0337,  1.1422,  0.5385]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2858, -1.7559, -0.2673,  ..., -1.4181, -1.5043,  0.0820],\n",
      "         [ 0.2924, -0.3240, -1.1354,  ..., -1.5954,  2.0296, -0.4236],\n",
      "         [-1.1631,  0.5746, -0.9279,  ...,  0.5000,  0.1728,  0.4771],\n",
      "         ...,\n",
      "         [-0.3501, -0.4458, -1.8440,  ...,  0.5205,  1.8536, -0.6717],\n",
      "         [-1.8028, -0.9843,  0.2339,  ...,  0.4292, -0.8402,  0.1860],\n",
      "         [-0.1723, -1.1977,  0.0681,  ...,  0.3844, -0.3633,  1.8238]],\n",
      "\n",
      "        [[-0.4168, -0.2649,  0.0208,  ...,  0.6222,  0.5593, -1.3068],\n",
      "         [ 0.3162, -1.3911, -0.9913,  ..., -0.1196,  1.4046, -0.2444],\n",
      "         [ 0.7890,  0.4770,  0.2184,  ...,  0.5703,  1.1294,  0.2943],\n",
      "         ...,\n",
      "         [-0.9100, -0.1834,  0.1774,  ..., -2.0337,  1.1422,  0.5385],\n",
      "         [-0.9100, -0.1834,  0.1774,  ..., -2.0337,  1.1422,  0.5385],\n",
      "         [-0.9100, -0.1834,  0.1774,  ..., -2.0337,  1.1422,  0.5385]],\n",
      "\n",
      "        [[ 1.6668, -0.1689, -0.9711,  ..., -0.9945, -0.3110,  1.0887],\n",
      "         [ 0.3162, -1.3911, -0.9913,  ..., -0.1196,  1.4046, -0.2444],\n",
      "         [-0.0353, -1.2790,  0.2302,  ..., -1.1026,  0.3405,  0.4365],\n",
      "         ...,\n",
      "         [-0.9100, -0.1834,  0.1774,  ..., -2.0337,  1.1422,  0.5385],\n",
      "         [-0.9100, -0.1834,  0.1774,  ..., -2.0337,  1.1422,  0.5385],\n",
      "         [-0.9100, -0.1834,  0.1774,  ..., -2.0337,  1.1422,  0.5385]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(batch_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3tLCUQwojcUb",
    "outputId": "9c0aa33e-e215-49dd-dba7-08b1e61d9afb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "print(batch_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0Lhx892gmi3"
   },
   "source": [
    "### **Linear transformation & 여러 head로 나누기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urXMBRnRgqvw"
   },
   "source": [
    "Multi-head attention 내에서 쓰이는 linear transformation matrix들을 정의합니다.\n",
    "\n",
    "- query, key, value를 서로 **다른** linear transformation matrix를 사용하여 linear projection 합니다.\n",
    "- 동일한 representations (`batch_emb`)으로부터 서로 다른 query, key, value를 생성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9DWKDqgCgfMk"
   },
   "outputs": [],
   "source": [
    "w_q = nn.Linear(d_model, d_model)\n",
    "w_k = nn.Linear(d_model, d_model)\n",
    "w_v = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tcLuhda7m-Lm"
   },
   "outputs": [],
   "source": [
    "w_0 = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H-vSL7PwnV6k",
    "outputId": "cad5c4ab-7839-44fe-d41f-d57d37b385fb"
   },
   "outputs": [],
   "source": [
    "q = w_q(batch_emb)  # (B, L, d_model)\n",
    "k = w_k(batch_emb)  # (B, L, d_model)\n",
    "v = w_v(batch_emb)  # (B, L, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3762, -0.3919,  0.2359,  ...,  0.2422, -0.1041, -0.2670],\n",
      "         [ 0.4900,  0.5566, -0.6025,  ...,  0.1934,  0.1268, -1.5310],\n",
      "         [-0.9232, -0.2311,  0.6882,  ..., -0.0455, -0.0301, -0.3920],\n",
      "         ...,\n",
      "         [-1.3817, -0.2090, -0.6947,  ..., -0.0486,  0.0123,  0.1691],\n",
      "         [-1.3817, -0.2090, -0.6947,  ..., -0.0486,  0.0123,  0.1691],\n",
      "         [-1.3817, -0.2090, -0.6947,  ..., -0.0486,  0.0123,  0.1691]],\n",
      "\n",
      "        [[-0.1338, -0.5917, -0.3523,  ...,  0.1090, -1.1387,  0.1942],\n",
      "         [ 1.1372, -0.5879,  0.5035,  ..., -0.9368, -0.9443, -0.1616],\n",
      "         [ 0.0893, -0.4137,  0.5938,  ...,  0.1984,  0.2541,  0.0110],\n",
      "         ...,\n",
      "         [-1.3817, -0.2090, -0.6947,  ..., -0.0486,  0.0123,  0.1691],\n",
      "         [-1.3817, -0.2090, -0.6947,  ..., -0.0486,  0.0123,  0.1691],\n",
      "         [-1.3817, -0.2090, -0.6947,  ..., -0.0486,  0.0123,  0.1691]],\n",
      "\n",
      "        [[ 0.0539, -0.8496,  0.1570,  ..., -0.4510, -0.0048,  1.3414],\n",
      "         [ 0.1464, -0.3278, -0.0076,  ..., -0.5705, -0.8856, -0.5596],\n",
      "         [-0.1739,  0.8323, -0.3212,  ..., -0.1056,  0.4098,  0.1000],\n",
      "         ...,\n",
      "         [-1.3817, -0.2090, -0.6947,  ..., -0.0486,  0.0123,  0.1691],\n",
      "         [-1.3817, -0.2090, -0.6947,  ..., -0.0486,  0.0123,  0.1691],\n",
      "         [-1.3817, -0.2090, -0.6947,  ..., -0.0486,  0.0123,  0.1691]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.6463,  1.0896, -0.0640,  ...,  0.0231,  0.6554,  0.0857],\n",
      "         [ 0.9622,  0.1936, -0.9559,  ...,  0.1967,  0.8381, -0.3388],\n",
      "         [ 0.0893, -0.4137,  0.5938,  ...,  0.1984,  0.2541,  0.0110],\n",
      "         ...,\n",
      "         [-0.1382, -0.3167,  0.4938,  ...,  0.7715, -0.4991,  0.8316],\n",
      "         [-0.0875,  0.2299,  0.1527,  ...,  0.3765, -0.4521,  0.5690],\n",
      "         [-0.5651,  0.6226,  0.2989,  ...,  0.3185,  0.2609,  0.9972]],\n",
      "\n",
      "        [[ 0.1869,  0.8112, -0.1421,  ...,  0.2625,  0.0817, -0.2625],\n",
      "         [-0.2100, -0.3473, -0.1996,  ...,  0.1677, -0.1974,  0.2485],\n",
      "         [ 0.1458, -0.4365,  0.1705,  ...,  0.8086, -0.2117,  0.1620],\n",
      "         ...,\n",
      "         [-1.3817, -0.2090, -0.6947,  ..., -0.0486,  0.0123,  0.1691],\n",
      "         [-1.3817, -0.2090, -0.6947,  ..., -0.0486,  0.0123,  0.1691],\n",
      "         [-1.3817, -0.2090, -0.6947,  ..., -0.0486,  0.0123,  0.1691]],\n",
      "\n",
      "        [[ 0.1036,  0.0331,  0.0385,  ..., -0.9854,  0.6696,  0.1282],\n",
      "         [-0.2100, -0.3473, -0.1996,  ...,  0.1677, -0.1974,  0.2485],\n",
      "         [ 0.8754,  0.4515,  0.2102,  ..., -0.7269, -0.1251,  0.0630],\n",
      "         ...,\n",
      "         [-1.3817, -0.2090, -0.6947,  ..., -0.0486,  0.0123,  0.1691],\n",
      "         [-1.3817, -0.2090, -0.6947,  ..., -0.0486,  0.0123,  0.1691],\n",
      "         [-1.3817, -0.2090, -0.6947,  ..., -0.0486,  0.0123,  0.1691]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H-vSL7PwnV6k",
    "outputId": "cad5c4ab-7839-44fe-d41f-d57d37b385fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 512])\n",
      "torch.Size([10, 20, 512])\n",
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wnvlum-LnF1T"
   },
   "source": [
    "Q, k, v를 `num_head`개의 차원 분할된 여러 vector로 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_tiOKAv9nEli",
    "outputId": "fcbf6d2e-6d88-4837-8eb5-86445ccacdb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 8, 64])\n",
      "torch.Size([10, 20, 8, 64])\n",
      "torch.Size([10, 20, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "batch_size = q.shape[0]\n",
    "d_k = d_model // num_heads\n",
    "\n",
    "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5tNb2isfn5Cx",
    "outputId": "8a10671e-e3f1-46ee-b68c-f50db610504d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 8, 20, 64])\n",
      "torch.Size([10, 8, 20, 64])\n",
      "torch.Size([10, 8, 20, 64])\n"
     ]
    }
   ],
   "source": [
    "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWrDA5_Sofad"
   },
   "source": [
    "### **Scaled dot-product self-attention 구현**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w52C4k3Wfl8m"
   },
   "source": [
    "각 head에서 실행되는 self-attetion 과정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A5waKr0Hfi2K",
    "outputId": "916e22ca-8193-4680-d4d3-7f7c14054ca9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0408, 0.0443, 0.0295,  ..., 0.0756, 0.0756, 0.0756],\n",
      "          [0.0521, 0.0429, 0.0406,  ..., 0.0640, 0.0640, 0.0640],\n",
      "          [0.0542, 0.0406, 0.0442,  ..., 0.0406, 0.0406, 0.0406],\n",
      "          ...,\n",
      "          [0.0392, 0.0435, 0.0454,  ..., 0.0542, 0.0542, 0.0542],\n",
      "          [0.0392, 0.0435, 0.0454,  ..., 0.0542, 0.0542, 0.0542],\n",
      "          [0.0392, 0.0435, 0.0454,  ..., 0.0542, 0.0542, 0.0542]],\n",
      "\n",
      "         [[0.0486, 0.0364, 0.0543,  ..., 0.0457, 0.0457, 0.0457],\n",
      "          [0.0638, 0.0641, 0.0541,  ..., 0.0252, 0.0252, 0.0252],\n",
      "          [0.0757, 0.0344, 0.0448,  ..., 0.0413, 0.0413, 0.0413],\n",
      "          ...,\n",
      "          [0.0705, 0.0539, 0.0397,  ..., 0.0650, 0.0650, 0.0650],\n",
      "          [0.0705, 0.0539, 0.0397,  ..., 0.0650, 0.0650, 0.0650],\n",
      "          [0.0705, 0.0539, 0.0397,  ..., 0.0650, 0.0650, 0.0650]],\n",
      "\n",
      "         [[0.0601, 0.0447, 0.0686,  ..., 0.0362, 0.0362, 0.0362],\n",
      "          [0.0345, 0.0353, 0.0340,  ..., 0.0851, 0.0851, 0.0851],\n",
      "          [0.0418, 0.0613, 0.0547,  ..., 0.0555, 0.0555, 0.0555],\n",
      "          ...,\n",
      "          [0.0584, 0.0927, 0.0269,  ..., 0.0465, 0.0465, 0.0465],\n",
      "          [0.0584, 0.0927, 0.0269,  ..., 0.0465, 0.0465, 0.0465],\n",
      "          [0.0584, 0.0927, 0.0269,  ..., 0.0465, 0.0465, 0.0465]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0492, 0.0488, 0.0496,  ..., 0.0601, 0.0601, 0.0601],\n",
      "          [0.0456, 0.0482, 0.0926,  ..., 0.0632, 0.0632, 0.0632],\n",
      "          [0.0680, 0.0451, 0.0607,  ..., 0.0400, 0.0400, 0.0400],\n",
      "          ...,\n",
      "          [0.0383, 0.0503, 0.0571,  ..., 0.0344, 0.0344, 0.0344],\n",
      "          [0.0383, 0.0503, 0.0571,  ..., 0.0344, 0.0344, 0.0344],\n",
      "          [0.0383, 0.0503, 0.0571,  ..., 0.0344, 0.0344, 0.0344]],\n",
      "\n",
      "         [[0.0333, 0.0394, 0.0941,  ..., 0.0591, 0.0591, 0.0591],\n",
      "          [0.0595, 0.0355, 0.0441,  ..., 0.0539, 0.0539, 0.0539],\n",
      "          [0.0425, 0.0384, 0.0577,  ..., 0.0418, 0.0418, 0.0418],\n",
      "          ...,\n",
      "          [0.0588, 0.0356, 0.0427,  ..., 0.0427, 0.0427, 0.0427],\n",
      "          [0.0588, 0.0356, 0.0427,  ..., 0.0427, 0.0427, 0.0427],\n",
      "          [0.0588, 0.0356, 0.0427,  ..., 0.0427, 0.0427, 0.0427]],\n",
      "\n",
      "         [[0.0266, 0.0476, 0.0970,  ..., 0.0389, 0.0389, 0.0389],\n",
      "          [0.0347, 0.0504, 0.0231,  ..., 0.0732, 0.0732, 0.0732],\n",
      "          [0.0496, 0.0628, 0.0701,  ..., 0.0527, 0.0527, 0.0527],\n",
      "          ...,\n",
      "          [0.0742, 0.0566, 0.0298,  ..., 0.0501, 0.0501, 0.0501],\n",
      "          [0.0742, 0.0566, 0.0298,  ..., 0.0501, 0.0501, 0.0501],\n",
      "          [0.0742, 0.0566, 0.0298,  ..., 0.0501, 0.0501, 0.0501]]],\n",
      "\n",
      "\n",
      "        [[[0.0438, 0.0540, 0.0346,  ..., 0.0519, 0.0519, 0.0519],\n",
      "          [0.0333, 0.0281, 0.0517,  ..., 0.0531, 0.0531, 0.0531],\n",
      "          [0.0792, 0.0849, 0.0497,  ..., 0.0413, 0.0413, 0.0413],\n",
      "          ...,\n",
      "          [0.0616, 0.0580, 0.0416,  ..., 0.0484, 0.0484, 0.0484],\n",
      "          [0.0616, 0.0580, 0.0416,  ..., 0.0484, 0.0484, 0.0484],\n",
      "          [0.0616, 0.0580, 0.0416,  ..., 0.0484, 0.0484, 0.0484]],\n",
      "\n",
      "         [[0.0293, 0.0456, 0.0262,  ..., 0.0528, 0.0528, 0.0528],\n",
      "          [0.0700, 0.1056, 0.0418,  ..., 0.0448, 0.0448, 0.0448],\n",
      "          [0.0492, 0.0356, 0.0412,  ..., 0.0540, 0.0540, 0.0540],\n",
      "          ...,\n",
      "          [0.0476, 0.0572, 0.0487,  ..., 0.0522, 0.0522, 0.0522],\n",
      "          [0.0476, 0.0572, 0.0487,  ..., 0.0522, 0.0522, 0.0522],\n",
      "          [0.0476, 0.0572, 0.0487,  ..., 0.0522, 0.0522, 0.0522]],\n",
      "\n",
      "         [[0.0571, 0.0523, 0.0329,  ..., 0.0499, 0.0499, 0.0499],\n",
      "          [0.0588, 0.0551, 0.0432,  ..., 0.0508, 0.0508, 0.0508],\n",
      "          [0.1070, 0.0635, 0.0826,  ..., 0.0399, 0.0399, 0.0399],\n",
      "          ...,\n",
      "          [0.0435, 0.0613, 0.0459,  ..., 0.0486, 0.0486, 0.0486],\n",
      "          [0.0435, 0.0613, 0.0459,  ..., 0.0486, 0.0486, 0.0486],\n",
      "          [0.0435, 0.0613, 0.0459,  ..., 0.0486, 0.0486, 0.0486]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0522, 0.0435, 0.0250,  ..., 0.0541, 0.0541, 0.0541],\n",
      "          [0.0496, 0.0656, 0.0454,  ..., 0.0505, 0.0505, 0.0505],\n",
      "          [0.0781, 0.0513, 0.0645,  ..., 0.0439, 0.0439, 0.0439],\n",
      "          ...,\n",
      "          [0.0660, 0.0687, 0.0685,  ..., 0.0420, 0.0420, 0.0420],\n",
      "          [0.0660, 0.0687, 0.0685,  ..., 0.0420, 0.0420, 0.0420],\n",
      "          [0.0660, 0.0687, 0.0685,  ..., 0.0420, 0.0420, 0.0420]],\n",
      "\n",
      "         [[0.0329, 0.0193, 0.0224,  ..., 0.0580, 0.0580, 0.0580],\n",
      "          [0.0641, 0.0485, 0.0603,  ..., 0.0470, 0.0470, 0.0470],\n",
      "          [0.0460, 0.0829, 0.0531,  ..., 0.0490, 0.0490, 0.0490],\n",
      "          ...,\n",
      "          [0.0490, 0.0574, 0.0631,  ..., 0.0462, 0.0462, 0.0462],\n",
      "          [0.0490, 0.0574, 0.0631,  ..., 0.0462, 0.0462, 0.0462],\n",
      "          [0.0490, 0.0574, 0.0631,  ..., 0.0462, 0.0462, 0.0462]],\n",
      "\n",
      "         [[0.0353, 0.0457, 0.0572,  ..., 0.0494, 0.0494, 0.0494],\n",
      "          [0.0324, 0.0406, 0.0533,  ..., 0.0511, 0.0511, 0.0511],\n",
      "          [0.0470, 0.0335, 0.0485,  ..., 0.0519, 0.0519, 0.0519],\n",
      "          ...,\n",
      "          [0.0592, 0.0363, 0.0579,  ..., 0.0495, 0.0495, 0.0495],\n",
      "          [0.0592, 0.0363, 0.0579,  ..., 0.0495, 0.0495, 0.0495],\n",
      "          [0.0592, 0.0363, 0.0579,  ..., 0.0495, 0.0495, 0.0495]]],\n",
      "\n",
      "\n",
      "        [[[0.0256, 0.0396, 0.0440,  ..., 0.0585, 0.0585, 0.0585],\n",
      "          [0.0305, 0.0219, 0.0459,  ..., 0.0618, 0.0618, 0.0618],\n",
      "          [0.0305, 0.0320, 0.0378,  ..., 0.0544, 0.0544, 0.0544],\n",
      "          ...,\n",
      "          [0.0424, 0.0500, 0.0566,  ..., 0.0493, 0.0493, 0.0493],\n",
      "          [0.0424, 0.0500, 0.0566,  ..., 0.0493, 0.0493, 0.0493],\n",
      "          [0.0424, 0.0500, 0.0566,  ..., 0.0493, 0.0493, 0.0493]],\n",
      "\n",
      "         [[0.0467, 0.0276, 0.0524,  ..., 0.0603, 0.0603, 0.0603],\n",
      "          [0.0539, 0.0421, 0.0725,  ..., 0.0392, 0.0392, 0.0392],\n",
      "          [0.0427, 0.0672, 0.0527,  ..., 0.0485, 0.0485, 0.0485],\n",
      "          ...,\n",
      "          [0.0597, 0.0326, 0.0335,  ..., 0.0550, 0.0550, 0.0550],\n",
      "          [0.0597, 0.0326, 0.0335,  ..., 0.0550, 0.0550, 0.0550],\n",
      "          [0.0597, 0.0326, 0.0335,  ..., 0.0550, 0.0550, 0.0550]],\n",
      "\n",
      "         [[0.0363, 0.0480, 0.0363,  ..., 0.0476, 0.0476, 0.0476],\n",
      "          [0.0485, 0.0635, 0.0752,  ..., 0.0340, 0.0340, 0.0340],\n",
      "          [0.0228, 0.0145, 0.0327,  ..., 0.0689, 0.0689, 0.0689],\n",
      "          ...,\n",
      "          [0.0495, 0.0539, 0.0504,  ..., 0.0468, 0.0468, 0.0468],\n",
      "          [0.0495, 0.0539, 0.0504,  ..., 0.0468, 0.0468, 0.0468],\n",
      "          [0.0495, 0.0539, 0.0504,  ..., 0.0468, 0.0468, 0.0468]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0396, 0.0292, 0.0361,  ..., 0.0591, 0.0591, 0.0591],\n",
      "          [0.0327, 0.0472, 0.0555,  ..., 0.0614, 0.0614, 0.0614],\n",
      "          [0.0300, 0.0421, 0.0511,  ..., 0.0598, 0.0598, 0.0598],\n",
      "          ...,\n",
      "          [0.0380, 0.0451, 0.0415,  ..., 0.0426, 0.0426, 0.0426],\n",
      "          [0.0380, 0.0451, 0.0415,  ..., 0.0426, 0.0426, 0.0426],\n",
      "          [0.0380, 0.0451, 0.0415,  ..., 0.0426, 0.0426, 0.0426]],\n",
      "\n",
      "         [[0.0207, 0.0277, 0.0370,  ..., 0.0624, 0.0624, 0.0624],\n",
      "          [0.0343, 0.0569, 0.0400,  ..., 0.0527, 0.0527, 0.0527],\n",
      "          [0.0591, 0.0273, 0.0293,  ..., 0.0568, 0.0568, 0.0568],\n",
      "          ...,\n",
      "          [0.0623, 0.0552, 0.0867,  ..., 0.0396, 0.0396, 0.0396],\n",
      "          [0.0623, 0.0552, 0.0867,  ..., 0.0396, 0.0396, 0.0396],\n",
      "          [0.0623, 0.0552, 0.0867,  ..., 0.0396, 0.0396, 0.0396]],\n",
      "\n",
      "         [[0.0760, 0.0481, 0.0830,  ..., 0.0533, 0.0533, 0.0533],\n",
      "          [0.0559, 0.0483, 0.0535,  ..., 0.0427, 0.0427, 0.0427],\n",
      "          [0.0458, 0.0437, 0.0762,  ..., 0.0450, 0.0450, 0.0450],\n",
      "          ...,\n",
      "          [0.0402, 0.0537, 0.0640,  ..., 0.0442, 0.0442, 0.0442],\n",
      "          [0.0402, 0.0537, 0.0640,  ..., 0.0442, 0.0442, 0.0442],\n",
      "          [0.0402, 0.0537, 0.0640,  ..., 0.0442, 0.0442, 0.0442]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.0451, 0.0269, 0.0618,  ..., 0.0581, 0.0386, 0.0377],\n",
      "          [0.0344, 0.0520, 0.0281,  ..., 0.0580, 0.0728, 0.1172],\n",
      "          [0.0512, 0.0514, 0.0377,  ..., 0.0335, 0.0647, 0.1078],\n",
      "          ...,\n",
      "          [0.0557, 0.0564, 0.0597,  ..., 0.0558, 0.0887, 0.0444],\n",
      "          [0.0370, 0.0563, 0.0862,  ..., 0.0473, 0.1112, 0.0549],\n",
      "          [0.0532, 0.0375, 0.0535,  ..., 0.0396, 0.0405, 0.0613]],\n",
      "\n",
      "         [[0.0335, 0.0301, 0.0430,  ..., 0.0508, 0.0542, 0.0332],\n",
      "          [0.0421, 0.0585, 0.0502,  ..., 0.0306, 0.0631, 0.0368],\n",
      "          [0.0397, 0.0549, 0.0575,  ..., 0.0428, 0.0680, 0.0350],\n",
      "          ...,\n",
      "          [0.0537, 0.0449, 0.0418,  ..., 0.0627, 0.0522, 0.0553],\n",
      "          [0.0983, 0.0407, 0.0418,  ..., 0.0363, 0.0313, 0.0388],\n",
      "          [0.0790, 0.0509, 0.0352,  ..., 0.0509, 0.0243, 0.0556]],\n",
      "\n",
      "         [[0.0443, 0.0880, 0.0369,  ..., 0.0653, 0.0612, 0.0427],\n",
      "          [0.0329, 0.0656, 0.0464,  ..., 0.0450, 0.0743, 0.0805],\n",
      "          [0.0910, 0.0496, 0.0551,  ..., 0.0460, 0.0583, 0.0509],\n",
      "          ...,\n",
      "          [0.0555, 0.0480, 0.0447,  ..., 0.0460, 0.0404, 0.0741],\n",
      "          [0.0406, 0.0549, 0.0352,  ..., 0.0468, 0.0355, 0.0442],\n",
      "          [0.0787, 0.0327, 0.0562,  ..., 0.0569, 0.0454, 0.0404]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0537, 0.0298, 0.0592,  ..., 0.0206, 0.0714, 0.0614],\n",
      "          [0.0935, 0.0334, 0.0860,  ..., 0.0357, 0.0457, 0.0444],\n",
      "          [0.0315, 0.0664, 0.0380,  ..., 0.0547, 0.0470, 0.0559],\n",
      "          ...,\n",
      "          [0.0321, 0.0603, 0.0529,  ..., 0.0641, 0.0560, 0.0364],\n",
      "          [0.0425, 0.0291, 0.0790,  ..., 0.0649, 0.0424, 0.0428],\n",
      "          [0.0603, 0.0361, 0.0735,  ..., 0.0634, 0.0558, 0.0472]],\n",
      "\n",
      "         [[0.0393, 0.0739, 0.0699,  ..., 0.0568, 0.0331, 0.0302],\n",
      "          [0.0547, 0.0405, 0.0462,  ..., 0.0314, 0.0590, 0.0327],\n",
      "          [0.0404, 0.0517, 0.0535,  ..., 0.0390, 0.0575, 0.0382],\n",
      "          ...,\n",
      "          [0.0409, 0.0615, 0.0539,  ..., 0.0509, 0.0311, 0.0276],\n",
      "          [0.0457, 0.0604, 0.0495,  ..., 0.0504, 0.0844, 0.0774],\n",
      "          [0.0535, 0.0338, 0.0400,  ..., 0.0635, 0.0544, 0.0683]],\n",
      "\n",
      "         [[0.0496, 0.0537, 0.0323,  ..., 0.0587, 0.0903, 0.0617],\n",
      "          [0.0570, 0.0469, 0.0413,  ..., 0.0967, 0.0480, 0.0336],\n",
      "          [0.0737, 0.0282, 0.0439,  ..., 0.0418, 0.0393, 0.0573],\n",
      "          ...,\n",
      "          [0.0395, 0.0375, 0.0591,  ..., 0.0540, 0.0628, 0.0311],\n",
      "          [0.0301, 0.0603, 0.0315,  ..., 0.0362, 0.0340, 0.1298],\n",
      "          [0.0704, 0.0539, 0.0721,  ..., 0.0423, 0.0422, 0.0484]]],\n",
      "\n",
      "\n",
      "        [[[0.0635, 0.0270, 0.0500,  ..., 0.0508, 0.0508, 0.0508],\n",
      "          [0.0316, 0.0518, 0.0410,  ..., 0.0491, 0.0491, 0.0491],\n",
      "          [0.0691, 0.0412, 0.0231,  ..., 0.0772, 0.0772, 0.0772],\n",
      "          ...,\n",
      "          [0.0545, 0.0456, 0.0620,  ..., 0.0460, 0.0460, 0.0460],\n",
      "          [0.0545, 0.0456, 0.0620,  ..., 0.0460, 0.0460, 0.0460],\n",
      "          [0.0545, 0.0456, 0.0620,  ..., 0.0460, 0.0460, 0.0460]],\n",
      "\n",
      "         [[0.0650, 0.0638, 0.0601,  ..., 0.0212, 0.0212, 0.0212],\n",
      "          [0.0753, 0.0376, 0.0758,  ..., 0.0373, 0.0373, 0.0373],\n",
      "          [0.0618, 0.0997, 0.0404,  ..., 0.0578, 0.0578, 0.0578],\n",
      "          ...,\n",
      "          [0.0419, 0.0766, 0.0324,  ..., 0.0558, 0.0558, 0.0558],\n",
      "          [0.0419, 0.0766, 0.0324,  ..., 0.0558, 0.0558, 0.0558],\n",
      "          [0.0419, 0.0766, 0.0324,  ..., 0.0558, 0.0558, 0.0558]],\n",
      "\n",
      "         [[0.0726, 0.0579, 0.0541,  ..., 0.0181, 0.0181, 0.0181],\n",
      "          [0.0317, 0.0593, 0.0591,  ..., 0.0321, 0.0321, 0.0321],\n",
      "          [0.0731, 0.0340, 0.0466,  ..., 0.0467, 0.0467, 0.0467],\n",
      "          ...,\n",
      "          [0.0695, 0.0365, 0.0433,  ..., 0.0473, 0.0473, 0.0473],\n",
      "          [0.0695, 0.0365, 0.0433,  ..., 0.0473, 0.0473, 0.0473],\n",
      "          [0.0695, 0.0365, 0.0433,  ..., 0.0473, 0.0473, 0.0473]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0407, 0.0260, 0.0366,  ..., 0.0509, 0.0509, 0.0509],\n",
      "          [0.0325, 0.0549, 0.0410,  ..., 0.0636, 0.0636, 0.0636],\n",
      "          [0.0462, 0.0615, 0.0219,  ..., 0.0358, 0.0358, 0.0358],\n",
      "          ...,\n",
      "          [0.0629, 0.0681, 0.0446,  ..., 0.0362, 0.0362, 0.0362],\n",
      "          [0.0629, 0.0681, 0.0446,  ..., 0.0362, 0.0362, 0.0362],\n",
      "          [0.0629, 0.0681, 0.0446,  ..., 0.0362, 0.0362, 0.0362]],\n",
      "\n",
      "         [[0.0637, 0.0546, 0.0972,  ..., 0.0296, 0.0296, 0.0296],\n",
      "          [0.0648, 0.0613, 0.1143,  ..., 0.0306, 0.0306, 0.0306],\n",
      "          [0.0757, 0.0336, 0.0578,  ..., 0.0443, 0.0443, 0.0443],\n",
      "          ...,\n",
      "          [0.0779, 0.0275, 0.0563,  ..., 0.0424, 0.0424, 0.0424],\n",
      "          [0.0779, 0.0275, 0.0563,  ..., 0.0424, 0.0424, 0.0424],\n",
      "          [0.0779, 0.0275, 0.0563,  ..., 0.0424, 0.0424, 0.0424]],\n",
      "\n",
      "         [[0.0278, 0.0331, 0.0645,  ..., 0.0358, 0.0358, 0.0358],\n",
      "          [0.0337, 0.0663, 0.0500,  ..., 0.0336, 0.0336, 0.0336],\n",
      "          [0.0393, 0.0683, 0.0685,  ..., 0.0636, 0.0636, 0.0636],\n",
      "          ...,\n",
      "          [0.0373, 0.0319, 0.0448,  ..., 0.0464, 0.0464, 0.0464],\n",
      "          [0.0373, 0.0319, 0.0448,  ..., 0.0464, 0.0464, 0.0464],\n",
      "          [0.0373, 0.0319, 0.0448,  ..., 0.0464, 0.0464, 0.0464]]],\n",
      "\n",
      "\n",
      "        [[[0.0396, 0.0525, 0.0415,  ..., 0.0536, 0.0536, 0.0536],\n",
      "          [0.0698, 0.0498, 0.0663,  ..., 0.0472, 0.0472, 0.0472],\n",
      "          [0.0460, 0.0400, 0.0399,  ..., 0.0407, 0.0407, 0.0407],\n",
      "          ...,\n",
      "          [0.0197, 0.0504, 0.0420,  ..., 0.0508, 0.0508, 0.0508],\n",
      "          [0.0197, 0.0504, 0.0420,  ..., 0.0508, 0.0508, 0.0508],\n",
      "          [0.0197, 0.0504, 0.0420,  ..., 0.0508, 0.0508, 0.0508]],\n",
      "\n",
      "         [[0.0481, 0.0475, 0.0481,  ..., 0.0626, 0.0626, 0.0626],\n",
      "          [0.0534, 0.0396, 0.0398,  ..., 0.0392, 0.0392, 0.0392],\n",
      "          [0.0589, 0.0220, 0.0302,  ..., 0.0616, 0.0616, 0.0616],\n",
      "          ...,\n",
      "          [0.0409, 0.0823, 0.0298,  ..., 0.0600, 0.0600, 0.0600],\n",
      "          [0.0409, 0.0823, 0.0298,  ..., 0.0600, 0.0600, 0.0600],\n",
      "          [0.0409, 0.0823, 0.0298,  ..., 0.0600, 0.0600, 0.0600]],\n",
      "\n",
      "         [[0.0237, 0.0258, 0.0701,  ..., 0.0464, 0.0464, 0.0464],\n",
      "          [0.0571, 0.0544, 0.0236,  ..., 0.0294, 0.0294, 0.0294],\n",
      "          [0.0434, 0.0395, 0.0263,  ..., 0.0611, 0.0611, 0.0611],\n",
      "          ...,\n",
      "          [0.0630, 0.0374, 0.0346,  ..., 0.0486, 0.0486, 0.0486],\n",
      "          [0.0630, 0.0374, 0.0346,  ..., 0.0486, 0.0486, 0.0486],\n",
      "          [0.0630, 0.0374, 0.0346,  ..., 0.0486, 0.0486, 0.0486]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0223, 0.0479, 0.0549,  ..., 0.0608, 0.0608, 0.0608],\n",
      "          [0.0249, 0.0508, 0.0383,  ..., 0.0588, 0.0588, 0.0588],\n",
      "          [0.0361, 0.0416, 0.0737,  ..., 0.0364, 0.0364, 0.0364],\n",
      "          ...,\n",
      "          [0.0499, 0.0759, 0.0364,  ..., 0.0403, 0.0403, 0.0403],\n",
      "          [0.0499, 0.0759, 0.0364,  ..., 0.0403, 0.0403, 0.0403],\n",
      "          [0.0499, 0.0759, 0.0364,  ..., 0.0403, 0.0403, 0.0403]],\n",
      "\n",
      "         [[0.0403, 0.0287, 0.0290,  ..., 0.0710, 0.0710, 0.0710],\n",
      "          [0.0285, 0.0628, 0.0585,  ..., 0.0314, 0.0314, 0.0314],\n",
      "          [0.0245, 0.0345, 0.0806,  ..., 0.0468, 0.0468, 0.0468],\n",
      "          ...,\n",
      "          [0.0835, 0.0282, 0.0559,  ..., 0.0436, 0.0436, 0.0436],\n",
      "          [0.0835, 0.0282, 0.0559,  ..., 0.0436, 0.0436, 0.0436],\n",
      "          [0.0835, 0.0282, 0.0559,  ..., 0.0436, 0.0436, 0.0436]],\n",
      "\n",
      "         [[0.0404, 0.0691, 0.0371,  ..., 0.0526, 0.0526, 0.0526],\n",
      "          [0.0792, 0.0679, 0.0770,  ..., 0.0343, 0.0343, 0.0343],\n",
      "          [0.0205, 0.0341, 0.0427,  ..., 0.0676, 0.0676, 0.0676],\n",
      "          ...,\n",
      "          [0.0609, 0.0292, 0.0508,  ..., 0.0425, 0.0425, 0.0425],\n",
      "          [0.0609, 0.0292, 0.0508,  ..., 0.0425, 0.0425, 0.0425],\n",
      "          [0.0609, 0.0292, 0.0508,  ..., 0.0425, 0.0425, 0.0425]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([10, 8, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
    "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
    "\n",
    "print(attn_dists)\n",
    "print(attn_dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7megouWpgCck",
    "outputId": "112fcc79-f7e5-437e-efb7-7c8934ff48d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 8, 20, 64])\n"
     ]
    }
   ],
   "source": [
    "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
    "\n",
    "print(attn_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmSTaymdg-P_"
   },
   "source": [
    "### **각 head의 결과물 병합**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSdQZCk0hCNd"
   },
   "source": [
    "각 head의 결과물을 concat하고 동일 차원(`d_model`)으로 linear transformation합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eaK0bpMGhQZ2",
    "outputId": "fbfb6249-aa40-4118-ed03-20676eb191de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n",
    "attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\n",
    "\n",
    "print(attn_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LTng_2SXhdH1",
    "outputId": "8ac24f02-2e8e-4887-edb3-d8282af6996f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 3.7536e-02, -1.1127e-01, -1.0192e-01,  ...,  3.4096e-02,\n",
      "           1.9917e-01, -4.7854e-02],\n",
      "         [ 9.2709e-02, -1.1835e-01, -3.0714e-02,  ...,  6.1924e-02,\n",
      "           1.9109e-01, -4.9547e-02],\n",
      "         [ 2.2903e-02, -3.2129e-02, -4.0533e-02,  ...,  8.2119e-03,\n",
      "           5.1191e-02, -8.5249e-02],\n",
      "         ...,\n",
      "         [-3.7873e-02,  1.1777e-01, -8.4572e-02,  ...,  3.0946e-02,\n",
      "          -8.4204e-02,  8.9798e-02],\n",
      "         [-4.7355e-02,  8.8719e-02, -1.2927e-01,  ...,  1.0065e-01,\n",
      "           1.2332e-01, -2.4758e-01],\n",
      "         [ 5.6735e-03,  5.5543e-03, -1.1913e-01,  ...,  7.0541e-02,\n",
      "           8.4119e-02, -1.9495e-01]],\n",
      "\n",
      "        [[-2.0735e-01,  5.9715e-02, -5.6107e-01,  ...,  2.6348e-01,\n",
      "           2.9746e-01, -1.3711e-01],\n",
      "         [-2.1326e-01,  7.9526e-02, -5.6328e-01,  ...,  2.5622e-01,\n",
      "           2.9996e-01, -1.3119e-01],\n",
      "         [-8.9059e-02,  1.8120e-01, -2.4666e-02,  ...,  4.1081e-01,\n",
      "           1.2801e-01, -1.1793e-01],\n",
      "         ...,\n",
      "         [-3.5409e-02, -6.3312e-02, -4.2198e-02,  ...,  6.7065e-02,\n",
      "          -1.2095e-01,  1.0889e-01],\n",
      "         [-9.1584e-02,  4.8354e-02, -1.1350e-01,  ...,  2.2173e-01,\n",
      "           1.4046e-01, -2.8342e-01],\n",
      "         [-9.4643e-02,  3.6570e-02, -1.1211e-01,  ...,  2.2128e-01,\n",
      "           1.4713e-01, -2.7965e-01]],\n",
      "\n",
      "        [[-1.8096e-01,  3.9330e-02, -3.1184e-01,  ...,  2.1282e-01,\n",
      "           2.9782e-01, -1.6633e-01],\n",
      "         [-1.4231e-01,  6.6343e-02, -3.4063e-01,  ...,  2.5930e-01,\n",
      "           2.8598e-01, -1.4176e-01],\n",
      "         [-7.4941e-02,  1.6712e-01, -2.3828e-03,  ...,  2.6035e-01,\n",
      "           1.4078e-01, -2.0676e-01],\n",
      "         ...,\n",
      "         [ 4.3634e-02,  4.4490e-02, -7.8591e-02,  ...,  2.0417e-02,\n",
      "          -2.8706e-02,  1.2629e-01],\n",
      "         [-2.5080e-02,  8.5112e-02, -1.0506e-01,  ...,  1.1202e-01,\n",
      "           1.6267e-02, -1.2910e-01],\n",
      "         [-3.5320e-02,  4.5479e-02, -1.1727e-01,  ...,  1.1441e-01,\n",
      "           4.2265e-02, -9.5218e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.5729e-01,  1.1465e-01, -5.0632e-02,  ...,  8.5887e-02,\n",
      "           5.9277e-02, -3.2446e-02],\n",
      "         [-1.5100e-01,  1.5999e-01, -9.5314e-02,  ...,  6.4303e-02,\n",
      "           5.9252e-02, -2.7797e-02],\n",
      "         [ 6.9411e-02,  1.9502e-01, -4.9360e-02,  ...,  5.0011e-02,\n",
      "          -8.3114e-02, -5.8978e-02],\n",
      "         ...,\n",
      "         [-7.8162e-02,  3.0315e-02,  4.6567e-02,  ..., -4.1490e-02,\n",
      "          -3.4094e-02, -3.9978e-02],\n",
      "         [-4.1036e-02, -1.0470e-02,  3.6058e-02,  ...,  1.0393e-01,\n",
      "          -8.8011e-02,  2.3050e-02],\n",
      "         [-5.8788e-02, -1.4088e-02,  2.4841e-02,  ...,  1.2470e-01,\n",
      "          -1.3678e-01,  4.4696e-02]],\n",
      "\n",
      "        [[-4.7312e-02,  5.2697e-02,  1.0953e-02,  ..., -1.9048e-02,\n",
      "           2.0815e-01, -3.1900e-02],\n",
      "         [-7.8714e-02,  4.9438e-02, -1.5350e-02,  ...,  5.8811e-02,\n",
      "           1.6507e-01, -5.2986e-02],\n",
      "         [-4.2139e-02,  1.7814e-01, -4.3208e-02,  ...,  8.0416e-02,\n",
      "           1.6605e-01, -7.2169e-02],\n",
      "         ...,\n",
      "         [ 9.3916e-03, -4.4223e-02, -2.1050e-02,  ...,  6.6327e-02,\n",
      "          -1.8763e-01,  1.6251e-02],\n",
      "         [-8.8001e-02, -7.8788e-04, -7.0988e-02,  ...,  8.4456e-02,\n",
      "           8.7612e-03, -1.5987e-01],\n",
      "         [-1.4616e-01,  2.2326e-02,  2.4024e-02,  ...,  1.4851e-01,\n",
      "          -4.8358e-02, -1.3105e-01]],\n",
      "\n",
      "        [[-1.4518e-01,  5.0434e-02, -2.0175e-01,  ...,  1.1807e-01,\n",
      "           2.2798e-01, -3.8163e-02],\n",
      "         [-8.5004e-02, -6.8139e-05, -2.0738e-01,  ...,  1.7648e-01,\n",
      "           1.6635e-01, -3.8847e-02],\n",
      "         [-2.5988e-03,  2.0905e-01,  6.1100e-02,  ...,  1.5796e-01,\n",
      "           5.6844e-02, -1.2466e-01],\n",
      "         ...,\n",
      "         [ 1.2077e-02, -8.8530e-02,  6.9656e-02,  ...,  5.7485e-02,\n",
      "          -5.8874e-02,  1.1751e-01],\n",
      "         [ 1.2404e-01, -1.4442e-02,  4.4764e-02,  ...,  1.3223e-01,\n",
      "           2.0866e-02,  1.1941e-02],\n",
      "         [ 7.8743e-02, -6.2382e-02,  3.9170e-02,  ...,  1.0054e-01,\n",
      "           2.8670e-02,  2.1896e-02]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "outputs = w_0(attn_values)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LTng_2SXhdH1",
    "outputId": "8ac24f02-2e8e-4887-edb3-d8282af6996f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goX70VKqhxQH"
   },
   "source": [
    "### **전체 코드**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtNyV7mMj7V_"
   },
   "source": [
    "위의 과정을 모두 합쳐 하나의 Multi-head attention 모듈을 구현하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "U_kNhOTrkBHm"
   },
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MultiheadAttention, self).__init__()\n",
    "\n",
    "    # Q, K, V learnable matrices\n",
    "    self.w_q = nn.Linear(d_model, d_model)\n",
    "    self.w_k = nn.Linear(d_model, d_model)\n",
    "    self.w_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "    # Linear transformation for concatenated outputs\n",
    "    self.w_0 = nn.Linear(d_model, d_model)\n",
    "\n",
    "  def forward(self, q, k, v):\n",
    "    batch_size = q.shape[0]\n",
    "\n",
    "    q = self.w_q(q)  # (B, L, d_model)\n",
    "    k = self.w_k(k)  # (B, L, d_model)\n",
    "    v = self.w_v(v)  # (B, L, d_model)\n",
    "\n",
    "    q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "    k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "    v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "\n",
    "    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "\n",
    "    attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\n",
    "    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\n",
    "\n",
    "    return self.w_0(attn_values)\n",
    "\n",
    "  def self_attention(self, q, k, v):\n",
    "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
    "    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
    "\n",
    "    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
    "\n",
    "    return attn_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "jYLuu_9alQxT"
   },
   "outputs": [],
   "source": [
    "multihead_attn = MultiheadAttention()\n",
    "\n",
    "outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KMiXlYjSlTfB",
    "outputId": "aeadbb6c-1b93-409a-fb23-cc0aee86bd2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0870,  0.0750,  0.0305,  ...,  0.1137,  0.0775, -0.0140],\n",
      "         [-0.0297,  0.0265,  0.0692,  ...,  0.1198,  0.1127,  0.0661],\n",
      "         [-0.0761,  0.0685,  0.0723,  ...,  0.0506,  0.1020, -0.0112],\n",
      "         ...,\n",
      "         [-0.1376,  0.0968, -0.0120,  ...,  0.0367,  0.0514,  0.0476],\n",
      "         [-0.1376,  0.0968, -0.0120,  ...,  0.0367,  0.0514,  0.0476],\n",
      "         [-0.1376,  0.0968, -0.0120,  ...,  0.0367,  0.0514,  0.0476]],\n",
      "\n",
      "        [[-0.0709, -0.0145,  0.0304,  ...,  0.0891,  0.1155,  0.1585],\n",
      "         [-0.0871, -0.0206,  0.0171,  ...,  0.0145,  0.1359,  0.1032],\n",
      "         [-0.0992,  0.0107,  0.0189,  ..., -0.0168,  0.0872,  0.1363],\n",
      "         ...,\n",
      "         [-0.0776, -0.0085,  0.0244,  ...,  0.0144,  0.0907,  0.0911],\n",
      "         [-0.0776, -0.0085,  0.0244,  ...,  0.0144,  0.0907,  0.0911],\n",
      "         [-0.0776, -0.0085,  0.0244,  ...,  0.0144,  0.0907,  0.0911]],\n",
      "\n",
      "        [[-0.0836, -0.0048,  0.0421,  ...,  0.0171,  0.0731,  0.1269],\n",
      "         [-0.0532,  0.0138,  0.0299,  ...,  0.0908,  0.0153,  0.0798],\n",
      "         [-0.0544,  0.0382,  0.1105,  ...,  0.1254,  0.0164,  0.1290],\n",
      "         ...,\n",
      "         [-0.0540, -0.0002,  0.0515,  ...,  0.0544,  0.0309,  0.0611],\n",
      "         [-0.0540, -0.0002,  0.0515,  ...,  0.0544,  0.0309,  0.0611],\n",
      "         [-0.0540, -0.0002,  0.0515,  ...,  0.0544,  0.0309,  0.0611]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0550,  0.0663, -0.0719,  ..., -0.1562, -0.0922, -0.1271],\n",
      "         [-0.0709,  0.0434, -0.0487,  ..., -0.1390, -0.0938, -0.1346],\n",
      "         [-0.0468,  0.0270, -0.0889,  ..., -0.1265, -0.1070, -0.1305],\n",
      "         ...,\n",
      "         [-0.0895,  0.0811, -0.0413,  ..., -0.1455, -0.1660, -0.0681],\n",
      "         [-0.0176,  0.0375, -0.0110,  ..., -0.1340, -0.1035, -0.1207],\n",
      "         [-0.0562,  0.0261, -0.0510,  ..., -0.1565, -0.1604, -0.1277]],\n",
      "\n",
      "        [[-0.0488,  0.1060, -0.0227,  ..., -0.0131, -0.0110, -0.0843],\n",
      "         [-0.0176,  0.1131, -0.0141,  ...,  0.0417,  0.0234, -0.0534],\n",
      "         [-0.0186,  0.0518, -0.0371,  ...,  0.0808,  0.0184, -0.1174],\n",
      "         ...,\n",
      "         [-0.0289,  0.0688, -0.0795,  ..., -0.0360,  0.0277, -0.0917],\n",
      "         [-0.0289,  0.0688, -0.0795,  ..., -0.0360,  0.0277, -0.0917],\n",
      "         [-0.0289,  0.0688, -0.0795,  ..., -0.0360,  0.0277, -0.0917]],\n",
      "\n",
      "        [[-0.1022,  0.0773, -0.0727,  ...,  0.0809,  0.0292,  0.0109],\n",
      "         [-0.0748,  0.0464, -0.0554,  ...,  0.1405,  0.0559,  0.0557],\n",
      "         [-0.0818,  0.0284, -0.1872,  ..., -0.0036,  0.0387,  0.0353],\n",
      "         ...,\n",
      "         [-0.0319,  0.0696, -0.1356,  ...,  0.0132,  0.0470,  0.0334],\n",
      "         [-0.0319,  0.0696, -0.1356,  ...,  0.0132,  0.0470,  0.0334],\n",
      "         [-0.0319,  0.0696, -0.1356,  ...,  0.0132,  0.0470,  0.0334]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KMiXlYjSlTfB",
    "outputId": "aeadbb6c-1b93-409a-fb23-cc0aee86bd2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N63dWMoaMEv8"
   },
   "source": [
    "###**콘텐츠 라이선스**\n",
    "\n",
    "<font color='red'><b>**WARNING**</b></font> : **본 교육 콘텐츠의 지식재산권은 재단법인 네이버커넥트에 귀속됩니다. 본 콘텐츠를 어떠한 경로로든 외부로 유출 및 수정하는 행위를 엄격히 금합니다.** 다만, 비영리적 교육 및 연구활동에 한정되어 사용할 수 있으나 재단의 허락을 받아야 합니다. 이를 위반하는 경우, 관련 법률에 따라 책임을 질 수 있습니다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "ml2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "aff83ea1928dcc0287770e0ea68916ae9727a33d95a26ca69018331bcc2781f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
