{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KsBGZpKkWki"
   },
   "source": [
    "##**8. Masked Multi-head Attention**\n",
    "1. Masked Multi-head Attention 구현.\n",
    "2. Encoder-Decoder Attention 구현."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qRU5DFY2OM8"
   },
   "source": [
    "### **필요 패키지 import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lDtMioSQQ1bB"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBiZObgRep_Q"
   },
   "source": [
    "### **데이터 전처리**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfTSaGYteuze"
   },
   "source": [
    "데이터의 값과 형태를 좀 더 명확하게 보기 위해 sample을 줄이겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "e9ULZIqTenSc"
   },
   "outputs": [],
   "source": [
    "pad_id = 0\n",
    "vocab_size = 100\n",
    "\n",
    "data = [\n",
    "  [62, 13, 47, 39, 78, 33, 56, 13],\n",
    "  [60, 96, 51, 32, 90],\n",
    "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21],\n",
    "  [66, 88, 98, 47],\n",
    "  [77, 65, 51, 77, 19, 15, 35, 19, 23]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6Hx3mcivgMyH"
   },
   "outputs": [],
   "source": [
    "def padding(data):\n",
    "  max_len = len(max(data, key=len))\n",
    "  print(f\"Maximum sequence length: {max_len}\")\n",
    "\n",
    "  for i, seq in enumerate(tqdm(data)):\n",
    "    if len(seq) < max_len:\n",
    "      data[i] = seq + [pad_id] * (max_len - len(seq))\n",
    "\n",
    "  return data, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s3e8FiNvgX60",
    "outputId": "624bb622-8e7a-4353-9d76-ed70dc2259e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 106997.55it/s]\n"
     ]
    }
   ],
   "source": [
    "data, max_len = padding(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hwPSIWYugaN0",
    "outputId": "a31c386e-bd4c-4296-8944-a25820662bef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[62, 13, 47, 39, 78, 33, 56, 13, 0, 0],\n",
       " [60, 96, 51, 32, 90, 0, 0, 0, 0, 0],\n",
       " [35, 45, 48, 65, 91, 99, 92, 10, 3, 21],\n",
       " [66, 88, 98, 47, 0, 0, 0, 0, 0, 0],\n",
       " [77, 65, 51, 77, 19, 15, 35, 19, 23, 0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwqjACx8iidc"
   },
   "source": [
    "### **Hyperparameter 세팅 및 embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "p-Ngp2nWimS8"
   },
   "outputs": [],
   "source": [
    "d_model = 8  # model의 hidden size\n",
    "num_heads = 2  # head의 개수\n",
    "inf = 1e12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GJMi2Xsni5uq"
   },
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "# B: batch size, L: maximum sequence length\n",
    "batch = torch.LongTensor(data)  # (B, L)\n",
    "batch_emb = embedding(batch)  # (B, L, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3tLCUQwojcUb",
    "outputId": "ca253d99-a727-440c-ebb9-468e39bc846b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-8.5668e-01, -5.6692e-01,  1.4254e+00,  1.0340e+00, -5.6951e-01,\n",
      "          -3.2086e-01, -1.6549e+00,  2.1722e-01],\n",
      "         [ 1.2143e+00,  6.8545e-01, -7.5511e-02, -1.8285e+00, -5.8766e-01,\n",
      "          -3.0575e-01, -5.1290e-01, -8.3287e-01],\n",
      "         [ 4.3816e-01, -1.1702e+00,  5.9156e-01, -1.1647e+00,  4.4995e-01,\n",
      "           1.1627e+00, -4.9441e-02, -2.8493e-01],\n",
      "         [-4.9367e-01,  2.1836e-01,  2.3368e+00, -3.6854e-01,  6.8153e-01,\n",
      "           1.1698e-01,  8.5581e-02, -1.4454e+00],\n",
      "         [ 5.8628e-01,  1.2795e+00, -1.1619e+00,  1.0901e+00, -9.1943e-01,\n",
      "           3.2441e-01,  8.9015e-01,  1.1154e-01],\n",
      "         [ 1.0066e+00, -1.8870e+00, -2.2291e+00,  1.7119e+00, -7.0112e-01,\n",
      "          -6.8351e-01,  1.7704e+00, -1.7746e+00],\n",
      "         [-1.3992e+00,  1.0362e+00, -7.9058e-01, -2.3070e-01,  4.4054e-01,\n",
      "           8.3687e-01,  1.0816e+00, -9.4442e-01],\n",
      "         [ 1.2143e+00,  6.8545e-01, -7.5511e-02, -1.8285e+00, -5.8766e-01,\n",
      "          -3.0575e-01, -5.1290e-01, -8.3287e-01],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03]],\n",
      "\n",
      "        [[-6.4000e-01, -8.1289e-01,  5.8705e-01, -8.0971e-02, -7.6337e-02,\n",
      "          -1.5370e+00, -1.3368e-01,  2.4909e-01],\n",
      "         [-1.2542e-01,  2.1067e+00,  2.9581e-01,  2.0146e-01,  1.7841e+00,\n",
      "           4.6702e-01,  1.7564e+00,  1.3776e+00],\n",
      "         [ 8.3020e-01,  2.9746e-01,  1.4095e+00, -2.3850e+00, -1.2913e-01,\n",
      "          -4.5624e-01, -5.8587e-01,  9.7529e-01],\n",
      "         [ 4.2012e-02,  8.6972e-01,  5.8644e-01, -1.1267e+00,  1.8267e+00,\n",
      "          -9.1979e-01,  9.7950e-01,  1.0155e+00],\n",
      "         [ 7.0022e-01, -2.0605e-01, -3.8227e-01,  1.3874e+00, -6.3143e-01,\n",
      "           8.0153e-02, -1.1874e+00,  9.3165e-01],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03]],\n",
      "\n",
      "        [[-3.0368e-01, -4.9860e-01,  1.1507e+00, -1.4831e-01,  3.1657e-01,\n",
      "           1.6311e+00,  5.6068e-01, -2.9067e-01],\n",
      "         [ 1.3559e-01,  1.1251e+00, -1.0321e-01,  2.1018e+00,  2.9730e-01,\n",
      "          -4.2602e-01,  1.6979e+00,  3.1769e-01],\n",
      "         [ 1.4092e+00,  5.7430e-01, -5.1597e-01,  6.1245e-01,  4.9186e-01,\n",
      "          -1.0590e+00,  4.5348e-01, -1.3222e+00],\n",
      "         [ 3.6999e-01,  1.5763e+00,  1.2954e+00,  3.1074e-01, -1.4411e+00,\n",
      "           2.0412e-01, -1.0128e+00,  1.8116e-02],\n",
      "         [-1.0464e+00, -9.1203e-01,  2.1724e+00, -8.7238e-01, -3.5748e-01,\n",
      "           1.4582e+00, -8.6076e-02,  2.0172e+00],\n",
      "         [-5.3566e-01, -1.0242e+00, -2.8131e-01,  6.1909e-03, -2.1635e+00,\n",
      "           3.5258e-01, -1.1305e+00,  8.6449e-01],\n",
      "         [ 8.6939e-01,  2.3633e+00,  1.6549e+00, -1.6864e+00, -1.2287e+00,\n",
      "           9.3694e-01, -1.9049e-01, -2.4219e-01],\n",
      "         [ 2.7114e-01,  1.9239e+00, -1.0204e+00,  5.0334e-01,  4.2448e-01,\n",
      "          -4.2336e-01, -5.8318e-01, -2.0390e+00],\n",
      "         [-1.1946e-01,  1.9310e+00, -3.9795e-01,  3.6377e-01,  3.0593e-01,\n",
      "          -1.0249e+00,  1.0163e+00,  5.4177e-01],\n",
      "         [-1.9270e-01, -9.3582e-01,  1.5246e-01, -2.1858e+00, -7.3653e-01,\n",
      "          -1.3844e+00,  1.2624e+00, -5.7535e-01]],\n",
      "\n",
      "        [[-4.3769e-02,  1.3886e+00, -8.7915e-01,  1.0358e+00, -2.1764e-01,\n",
      "          -2.8116e-01,  2.7359e-01,  5.6646e-01],\n",
      "         [ 1.5190e+00,  2.3673e-01,  8.5125e-01, -2.6435e-02, -1.0885e+00,\n",
      "          -6.6441e-01,  1.3077e+00,  1.1305e+00],\n",
      "         [ 4.5991e-01, -2.8315e+00, -1.0522e+00, -1.1914e+00,  9.5653e-01,\n",
      "           1.2577e+00, -1.1288e+00,  2.5119e+00],\n",
      "         [ 4.3816e-01, -1.1702e+00,  5.9156e-01, -1.1647e+00,  4.4995e-01,\n",
      "           1.1627e+00, -4.9441e-02, -2.8493e-01],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03]],\n",
      "\n",
      "        [[ 1.4622e+00,  1.0418e+00, -7.2836e-01, -8.4209e-01, -2.2144e-01,\n",
      "           5.8917e-01, -8.2640e-01, -1.9216e-01],\n",
      "         [ 3.6999e-01,  1.5763e+00,  1.2954e+00,  3.1074e-01, -1.4411e+00,\n",
      "           2.0412e-01, -1.0128e+00,  1.8116e-02],\n",
      "         [ 8.3020e-01,  2.9746e-01,  1.4095e+00, -2.3850e+00, -1.2913e-01,\n",
      "          -4.5624e-01, -5.8587e-01,  9.7529e-01],\n",
      "         [ 1.4622e+00,  1.0418e+00, -7.2836e-01, -8.4209e-01, -2.2144e-01,\n",
      "           5.8917e-01, -8.2640e-01, -1.9216e-01],\n",
      "         [-4.7657e-01, -7.2773e-01,  5.6690e-01,  8.9533e-01,  3.2724e-01,\n",
      "          -5.7196e-01,  7.7989e-01, -2.3528e+00],\n",
      "         [ 7.1560e-01,  7.7900e-01,  2.0400e-03, -5.6552e-01,  1.5183e+00,\n",
      "          -3.4172e-01, -2.5706e-01,  2.1520e-01],\n",
      "         [-3.0368e-01, -4.9860e-01,  1.1507e+00, -1.4831e-01,  3.1657e-01,\n",
      "           1.6311e+00,  5.6068e-01, -2.9067e-01],\n",
      "         [-4.7657e-01, -7.2773e-01,  5.6690e-01,  8.9533e-01,  3.2724e-01,\n",
      "          -5.7196e-01,  7.7989e-01, -2.3528e+00],\n",
      "         [ 3.3738e-01, -6.0215e-01, -5.2962e-01, -1.3398e+00, -1.7869e+00,\n",
      "           5.3335e-02,  7.9015e-01, -7.3433e-01],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([5, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "print(batch_emb)\n",
    "print(batch_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dO3gxeyhpyF2"
   },
   "source": [
    "### **Mask 구축**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NDEQF64p5pN"
   },
   "source": [
    "`True`는 attention이 적용될 부분, `False`는 masking될 자리입니다.\n",
    "\n",
    "- Masked Multi-head Attention의 경우, decoding timestep에서 현재 타임스텝 이후의 token들을 참조하지 못하도록 하는 역할을 합니다. 이에 따른 mask는 `nopeak_mask`에서 결정됩니다.\n",
    "- `padding_mask`: batch를 만들기 위해 채워 넣은 `pad_id`는 특별한 의미를 가지는 토큰이 아니므로 참조할 대상이 아님. 따라서 padding 부분을 masking 하는 `padding_mask`를 생성함. 미래의 토큰을 볼 수 없도록 하는 `nopeak_mask`와 결합하여 최종 `mask`를 생성하게 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aB0A4elupM2g",
    "outputId": "69325f99-106f-444f-fd6c-a95412a55ebb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True,  True,  True,  True,  True,  True,  True,  True, False, False]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True, False, False, False, False, False]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True, False, False, False, False, False, False]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True, False]]])\n",
      "torch.Size([5, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "padding_mask = (batch != pad_id).unsqueeze(1)  # (B, 1, L)\n",
    "\n",
    "print(padding_mask)\n",
    "print(padding_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88cD54evrEo6",
    "outputId": "add7f80f-bc1f-4cff-b95d-22972d5a4b55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True, False, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])\n",
      "torch.Size([1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "nopeak_mask = torch.ones([1, max_len, max_len], dtype=torch.bool)  # (1, L, L)\n",
    "nopeak_mask = torch.tril(nopeak_mask)  # (1, L, L)\n",
    "\n",
    "print(nopeak_mask)\n",
    "print(nopeak_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FMzB8_jarycy",
    "outputId": "03c905e2-1dc8-4fa3-b1b6-cc3a9061e0c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True, False, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False]],\n",
      "\n",
      "        [[ True, False, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True, False, False, False, False, False]],\n",
      "\n",
      "        [[ True, False, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True, False, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False]],\n",
      "\n",
      "        [[ True, False, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False]]])\n",
      "torch.Size([5, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "mask = padding_mask & nopeak_mask  # (B, L, L)\n",
    "\n",
    "print(mask)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urXMBRnRgqvw"
   },
   "source": [
    "### **Linear transformation & 여러 head로 나누기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DWKDqgCgfMk"
   },
   "outputs": [],
   "source": [
    "w_q = nn.Linear(d_model, d_model)\n",
    "w_k = nn.Linear(d_model, d_model)\n",
    "w_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "w_0 = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H-vSL7PwnV6k",
    "outputId": "c0539e6c-2f07-49ec-cb59-227298453d77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 10, 4])\n",
      "torch.Size([5, 2, 10, 4])\n",
      "torch.Size([5, 2, 10, 4])\n"
     ]
    }
   ],
   "source": [
    "q = w_q(batch_emb)  # (B, L, d_model)\n",
    "k = w_k(batch_emb)  # (B, L, d_model)\n",
    "v = w_v(batch_emb)  # (B, L, d_model)\n",
    "\n",
    "batch_size = q.shape[0]\n",
    "d_k = d_model // num_heads\n",
    "\n",
    "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "\n",
    "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWrDA5_Sofad"
   },
   "source": [
    "### **Masking이 적용된 self-attention 구현**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GqaQmVQdvMZB"
   },
   "outputs": [],
   "source": [
    "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adlRCt6mvMy5"
   },
   "outputs": [],
   "source": [
    "masks = mask.unsqueeze(1)  # (B, 1, L, L)\n",
    "masked_attn_scores = attn_scores.masked_fill_(masks == False, -1 * inf)  # (B, num_heads, L, L)\n",
    "\n",
    "print(masked_attn_scores)\n",
    "print(masked_attn_scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EqMuVJFwHhI"
   },
   "source": [
    "`-1* inf`로 masking된 부분은 softmax 후 0이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVNze4elv4Uf"
   },
   "outputs": [],
   "source": [
    "attn_dists = F.softmax(masked_attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
    "\n",
    "print(attn_dists)\n",
    "print(attn_dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBwm34bswV7e"
   },
   "outputs": [],
   "source": [
    "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
    "\n",
    "print(attn_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2Xab7WKzTEU"
   },
   "source": [
    "### **전체 코드**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LlF7R6DIzVWc"
   },
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MultiheadAttention, self).__init__()\n",
    "\n",
    "    # Q, K, V learnable matrices\n",
    "    self.w_q = nn.Linear(d_model, d_model)\n",
    "    self.w_k = nn.Linear(d_model, d_model)\n",
    "    self.w_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "    # Linear transformation for concatenated outputs\n",
    "    self.w_0 = nn.Linear(d_model, d_model)\n",
    "\n",
    "  def forward(self, q, k, v, mask=None):\n",
    "    batch_size = q.shape[0]\n",
    "\n",
    "    q = self.w_q(q)  # (B, L, d_model)\n",
    "    k = self.w_k(k)  # (B, L, d_model)\n",
    "    v = self.w_v(v)  # (B, L, d_model)\n",
    "\n",
    "    q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "    k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "    v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "\n",
    "    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "\n",
    "    attn_values = self.self_attention(q, k, v, mask=mask)  # (B, num_heads, L, d_k)\n",
    "    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\n",
    "\n",
    "    return self.w_0(attn_values)\n",
    "\n",
    "  def self_attention(self, q, k, v, mask=None):\n",
    "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
    "\n",
    "    if mask is not None:\n",
    "      mask = mask.unsqueeze(1)  # (B, 1, L, L) or  (B, 1, 1, L)\n",
    "      attn_scores = attn_scores.masked_fill_(mask == False, -1*inf)\n",
    "\n",
    "    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
    "\n",
    "    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
    "\n",
    "    return attn_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jYLuu_9alQxT"
   },
   "outputs": [],
   "source": [
    "multihead_attn = MultiheadAttention()\n",
    "\n",
    "outputs = multihead_attn(batch_emb, batch_emb, batch_emb, mask=mask)  # (B, L, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMiXlYjSlTfB"
   },
   "outputs": [],
   "source": [
    "print(outputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1g99JEEFwFv"
   },
   "source": [
    "### **Encoder-Decoder attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2PRoF4fF4ah"
   },
   "source": [
    "Query, key, value만 달라질 뿐 구현은 동일합니다.  \n",
    "Decoder에 들어갈 batch만 별도 구현하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p26ra2BsGEdb"
   },
   "outputs": [],
   "source": [
    "trg_data = [\n",
    "  [33, 11, 49, 10],\n",
    "  [88, 34, 5, 29, 99, 45, 11, 25],\n",
    "  [67, 25, 15, 90, 54, 4, 92, 10, 46, 20, 88 ,19],\n",
    "  [16, 58, 91, 47, 12, 5, 8],\n",
    "  [71, 63, 62, 7, 9, 11, 55, 91, 32, 48]\n",
    "]\n",
    "\n",
    "trg_data, trg_max_len = padding(trg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYysB4EKHKGI"
   },
   "outputs": [],
   "source": [
    "# S_L: source maximum sequence length, T_L: target maximum sequence length\n",
    "src_batch = batch  # (B, S_L)\n",
    "trg_batch = torch.LongTensor(trg_data)  # (B, T_L)\n",
    "\n",
    "print(src_batch.shape)\n",
    "print(trg_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AieDxWYIHXKc"
   },
   "outputs": [],
   "source": [
    "src_emb = embedding(src_batch)  # (B, S_L, d_w)\n",
    "trg_emb = embedding(trg_batch)  # (B, T_L, d_w)\n",
    "\n",
    "print(src_emb.shape)\n",
    "print(trg_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxCjmPurH2b7"
   },
   "source": [
    "`src_emb`를 encoder에서 나온 결과, 그리고 `trg_emb`를 masked multi-head attention 후 결과로 가정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AUhY-z8JHeUE"
   },
   "outputs": [],
   "source": [
    "q = w_q(trg_emb)  # (B, T_L, d_model)\n",
    "k = w_k(src_emb)  # (B, S_L, d_model)\n",
    "v = w_v(src_emb)  # (B, S_L, d_model)\n",
    "\n",
    "batch_size = q.shape[0]\n",
    "d_k = d_model // num_heads\n",
    "\n",
    "q = q.view(batch_size, -1, num_heads, d_k)  # (B, T_L, num_heads, d_k)\n",
    "k = k.view(batch_size, -1, num_heads, d_k)  # (B, S_L, num_heads, d_k)\n",
    "v = v.view(batch_size, -1, num_heads, d_k)  # (B, S_L, num_heads, d_k)\n",
    "\n",
    "q = q.transpose(1, 2)  # (B, num_heads, T_L, d_k)\n",
    "k = k.transpose(1, 2)  # (B, num_heads, S_L, d_k)\n",
    "v = v.transpose(1, 2)  # (B, num_heads, S_L, d_k)\n",
    "\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeqjkVqkIdxO"
   },
   "outputs": [],
   "source": [
    "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, T_L, S_L)\n",
    "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, T_L, S_L)\n",
    "\n",
    "print(attn_dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQv4IINbItS0"
   },
   "outputs": [],
   "source": [
    "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, T_L, d_k)\n",
    "\n",
    "print(attn_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLCHeCbtJDy9"
   },
   "source": [
    "Masked multi-head attention 후 나온 결과와 동일한 shape를 가지며 이후 layer에서 전체 연산도 동일하게 진행됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAHCJ2xASvlB"
   },
   "source": [
    "###**콘텐츠 라이선스**\n",
    "\n",
    "<font color='red'><b>**WARNING**</b></font> : **본 교육 콘텐츠의 지식재산권은 재단법인 네이버커넥트에 귀속됩니다. 본 콘텐츠를 어떠한 경로로든 외부로 유출 및 수정하는 행위를 엄격히 금합니다.** 다만, 비영리적 교육 및 연구활동에 한정되어 사용할 수 있으나 재단의 허락을 받아야 합니다. 이를 위반하는 경우, 관련 법률에 따라 책임을 질 수 있습니다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "ml2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "aff83ea1928dcc0287770e0ea68916ae9727a33d95a26ca69018331bcc2781f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
