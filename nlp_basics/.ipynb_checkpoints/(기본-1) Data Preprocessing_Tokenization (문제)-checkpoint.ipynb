{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUTA4qLbnv9g"
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## ê¸°ë³¸ê³¼ì œ 1: Data Preprocessing & Tokenization\n",
    "\n",
    "> Reference ì½”ë“œëŠ” Solution ê³¼ í•¨ê»˜ ê³µê°œë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0sJ05vTnv9k"
   },
   "source": [
    "### Introduction\n",
    "\n",
    "* ë³¸ ê³¼ì œì˜ ëª©ì ì€ ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ì— í™œìš©í•˜ëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì „ì¹˜ë¦¬ ë° í† í°í™” ê³¼ì •ì˜ ê°œë…ì„ ìµíˆëŠ” ê²ƒ ì…ë‹ˆë‹¤.\n",
    "* ì˜ì–´ í…ìŠ¤íŠ¸ì—ì„  í† í°í™” ë° Vocabulary ì‘ì„±ì„ í†µí•´ í† í°í™”ì˜ ê¸°ë³¸ì„ ë°°ìš°ê³  [Spacy](https://spacy.io/)ìœ¼ë¡œ ë¶ˆìš©ì–´ë¥¼ ì œì™¸ ë° ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "* í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„  [Konlpy](https://konlpy.org/ko/latest/)ë¥¼ í™œìš©í•˜ì—¬ í˜•íƒœì†Œ ê¸°ë°˜ í† í°í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "* **ANSWER HERE** ì´ë¼ê³  ì‘ì„±ëœ ë¶€ë¶„ì„ ì±„ì›Œ ì™„ì„±í•˜ì‹œë©´ ë©ë‹ˆë‹¤. ë‹¤ë¥¸ ë¶€ë¶„ì˜ ì½”ë“œë¥¼ ë³€ê²½í•˜ë©´ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "> ê³¼ì œ ì™„ì„± í›„ ipynb íŒŒì¼ì„ ì œì¶œí•´ ì£¼ì„¸ìš”.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8lFsD7Gnv9k"
   },
   "source": [
    "### 0. ë°ì´í„° ì—…ë¡œë“œ\n",
    "\n",
    "1. Boostcourse [ê¸°ë³¸ ê³¼ì œ] Data Preprocessing & Tokenization ì—ì„œ `corpus.txt` íŒŒì¼ì„ ë‹¤ìš´ë°›ìŠµë‹ˆë‹¤.\n",
    "2. ë³¸ Colab í™˜ê²½ì— `corpus.txt`, íŒŒì¼ì„ ì—…ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "3. `!ls` command ë¥¼ ì‹¤í–‰í–ˆì„ ë•Œ, `corpus.txt sample_data` ê°€ ë‚˜ì˜¤ë©´ ì„±ê³µì ìœ¼ë¡œ ë°ì´í„° ì¤€ë¹„ê°€ ì™„ë£Œëœ ê²ƒ ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "f87-7zDFnv9l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'(10ê°•-ì‹¤ìŠµ) HuggingFace_s Transformers 2.ipynb'\r\n",
      "'(1ê°•-ì‹¤ìŠµ-1) Naive Bayes classifier êµ¬í˜„.ipynb'\r\n",
      "'(1ê°•-ì‹¤ìŠµ-2) Corpus Cleaning.ipynb'\r\n",
      "'(2ê°•-ì‹¤ìŠµ-1) Word2Vec êµ¬í˜„ ë° Embedding ì‹œê°í™”.ipynb'\r\n",
      "'(2ê°•-ì‹¤ìŠµ-2) ë‹¤êµ­ì–´ ì„ë² ë”©.ipynb'\r\n",
      "'(3ê°•-ì‹¤ìŠµ) Basic RNN ì‹¤ìŠµ.ipynb'\r\n",
      "'(4ê°•-ì‹¤ìŠµ) LSTM, GRU ì‹¤ìŠµ.ipynb'\r\n",
      "'(5ê°•-ì‹¤ìŠµ) Seq2Seq êµ¬í˜„.ipynb'\r\n",
      "'(6ê°•-ì‹¤ìŠµ) Seq2Seq with Attention êµ¬í˜„.ipynb'\r\n",
      "'(7ê°•-ì‹¤ìŠµ) Multi head Attention êµ¬í˜„.ipynb'\r\n",
      "'(8ê°•-ì‹¤ìŠµ) Masked Multi-head Attention êµ¬í˜„.ipynb'\r\n",
      "'(9ê°•-ì‹¤ìŠµ) HuggingFace_s Transformers 1.ipynb'\r\n",
      "'(ê¸°ë³¸-1) Data Preprocessing_Tokenization (ë¬¸ì œ).ipynb'\r\n",
      "'(ê¸°ë³¸-2) RNN-based Language Model (ë¬¸ì œ).ipynb'\r\n",
      "'(ê¸°ë³¸-3) Subword-level Language Model (ë¬¸ì œ).ipynb'\r\n",
      "'(ê¸°ë³¸-4) Preprocessing for NMT Model (ë¬¸ì œ).ipynb'\r\n",
      "'(ì‹¬í™”-1) BERT Fine-tuning with Transformers (ë¬¸ì œ).ipynb'\r\n",
      "'(ì‹¬í™”-2) NMT training with Fairseq (ë¬¸ì œ).ipynb'\r\n",
      " corpus.txt\r\n",
      " wikitext-2\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-FRJo54onv9m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1071\n",
      "A young man participates in a career while the subject who records it smiles.\n",
      "\n",
      "The man is scratching the back of his neck while looking for a book in a book store.\n",
      "\n",
      "A person wearing goggles and a hat is sled riding.\n",
      "\n",
      "A girl in a pink coat and flowered goloshes sledding down a hill.\n",
      "\n",
      "Three girls are standing in front of a window of a building.\n",
      "\n",
      "two dog is playing with a same chump on their mouth\n",
      "\n",
      "Low angle view of people suspended from the swings of a carnival ride.\n",
      "\n",
      "Black and white photo of two people watching a beautiful painting\n",
      "\n",
      "Man dressed in black crosses the street screaming\n",
      "\n",
      "A child sits on street on a busy street.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('./corpus.txt', 'r', encoding='utf-8') as fd:\n",
    "    corpus = fd.readlines()\n",
    "\n",
    "# ë§ë­‰ì¹˜ í¬ê¸° í™•ì¸\n",
    "print(len(corpus))\n",
    "\n",
    "# ì²« ì—´ ë¬¸ì¥ì„ print í•´ ë´…ì‹œë‹¤.\n",
    "for sentence in corpus[:10]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3AnsPdUnv9m"
   },
   "source": [
    "### 1. íŒŒì´ì¬ ê¸°ë³¸ ì½”ë“œë¥¼ ì´ìš©í•œ ì˜ì–´ í…ìŠ¤íŠ¸ í† í°í™” ë° ì „ì²˜ë¦¬\n",
    "\n",
    "\n",
    "ğŸ’¡ í† í°í™”(tokenization)ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\n",
    "\n",
    "í† í°í™”ëŠ” ì£¼ì–´ì§„ ì…ë ¥ ë°ì´í„°ë¥¼ ìì—°ì–´ì²˜ë¦¬ ëª¨ë¸ì´ ì¸ì‹í•  ìˆ˜ ìˆëŠ” ë‹¨ìœ„ë¡œ ë³€í™˜í•´ì£¼ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. \n",
    "\n",
    "ğŸ’¡ ë‹¨ì–´ ë‹¨ìœ„ í† í°í™”(word tokenization)ëŠ”ìš”?\n",
    "\n",
    "ë‹¨ì–´ë‹¨ìœ„ í† í°í™”ì˜ ê²½ìš° \"ë‹¨ì–´\"ê°€ ìì—°ì–´ì²˜ë¦¬ ëª¨ë¸ì´ ì¸ì‹í•˜ëŠ” ë‹¨ìœ„ê°€ ë©ë‹ˆë‹¤.\n",
    "\"I have a meal\"ì´ë¼ê³  í•˜ëŠ” ë¬¸ì¥ì„ ê°€ì§€ê³  ë‹¨ì–´ ë‹¨ìœ„ í† í°í™”ë¥¼ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \n",
    "\n",
    "- ['I', 'have', 'a', 'meal']\n",
    "\n",
    "ì˜ì–´ì˜ ê²½ìš° ëŒ€ë¶€ë¶„ ê³µë°±(space)ì„ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ê°€ ì •ì˜ë˜ê¸° ë•Œë¬¸ì— `.split()`ì„ ì´ìš©í•´ ì‰½ê²Œ ë‹¨ì–´ ë‹¨ìœ„ í† í°í™”ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "íŠ¹íˆ, ì˜ì–´ì—ì„œ ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ë¥¼ êµ¬ë¶„í•œ ë‹¨ì–´ ë‹¨ìœ„ í† í°í™”ëŠ” ê³µë°± ë‹¨ìœ„ í† í°í™” (space tokenization)ì´ë¼ê³ ë„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "> ì´ ì„¹ì…˜ì—ì„œëŠ” íŒŒì´ì¬ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ (Python Standard Libary)ë§Œì„ ì‚¬ìš©í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgaCeX5Pnv9n"
   },
   "source": [
    "#### 1-A) í† í°í™”ê¸° (tokenizer) êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_PiVs9ganv9n"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re\n",
    "\n",
    "def tokenize(sentence: str) -> List[str]:\n",
    "    \"\"\" í† í°í™”ê¸° êµ¬í˜„\n",
    "    ê³µë°±ìœ¼ë¡œ í† í°ì„ êµ¬ë¶„í•˜ë˜ . , ! ? ë¬¸ì¥ ë¶€í˜¸ëŠ” ë³„ê°œì˜ í† í°ìœ¼ë¡œ ì²˜ë¦¬ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "    ì˜ë¬¸ì—ì„œ Apostropheì— í•´ë‹¹í•˜ëŠ” ' ëŠ” ë‘ê°€ì§€ ê²½ìš°ì— ëŒ€í•´ ì²˜ë¦¬í•´ì•¼í•©ë‹ˆë‹¤.\n",
    "    1. notì˜ ì¤€ë§ì¸ n'tì€ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ì²˜ë¦¬ë˜ì–´ì•¼ í•©ë‹ˆë‹¤: don't ==> do n't\n",
    "    2. ë‹¤ë¥¸ Apostrophe ìš©ë²•ì€ ë’¤ì˜ ê¸€ìë“¤ì„ ë¶™ì—¬ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤: 's 'm 're ë“±ë“± \n",
    "    ê·¸ ì™¸ ë‹¤ë¥¸ ë¬¸ì¥ ë¶€í˜¸ëŠ” ê³ ë ¤í•˜ì§€ ì•Šìœ¼ë©°, ì‘ì€ ë”°ì˜´í‘œëŠ” ëª¨ë‘ Apostropheë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "    ëª¨ë“  í† í°ì€ ì†Œë¬¸ìë¡œ ë³€í™˜ë˜ì–´ì•¼ í•©ë‚˜ë‹¤.\n",
    "\n",
    "    íŒíŠ¸: ì •ê·œí‘œí˜„ì‹ì„ ì•ˆë‹¤ë©´ re ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ ë³´ì„¸ìš”!\n",
    "\n",
    "    ì˜ˆì‹œ: 'I don't like Jenifer's work.'\n",
    "    ==> ['i', 'do', 'n\\'t', 'like', 'jenifer', '\\'s', 'work', '.']\n",
    "\n",
    "    Arguments:\n",
    "    sentence -- í† í°í™”í•  ì˜ë¬¸ ë¬¸ì¥\n",
    "    \n",
    "    Return:\n",
    "    tokens -- í† í°í™”ëœ í† í° ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE \n",
    "    ### ANSWER HERE ###\n",
    "    \n",
    "    sentence = re.sub(r\"n't\", r\" n't\", sentence)\n",
    "\n",
    "    tokens = re.findall(r\"n't|\\w+|[.,!?]+|'\\w+\", sentence)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'do', \"n't\", 'like', 'you', \"'re\", 'working', '.']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"I don't like You're working.\"\n",
    "tokenize(test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGtJjlgdnv9o"
   },
   "source": [
    "**ë¬¸ì œ 1-Aì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CFLfJmrznv9p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Tokenizer Test Cases======\n",
      "ì²«ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\n",
      "ë‘ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\n",
      "ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!\n"
     ]
    }
   ],
   "source": [
    "print (\"======Tokenizer Test Cases======\")\n",
    "\n",
    "# First test\n",
    "sentence = \"This sentence should be tokenized properly.\"\n",
    "tokens = tokenize(sentence)\n",
    "assert tokens == ['this', 'sentence', 'should', 'be', 'tokenized', 'properly', '.'], \\\n",
    "    \"í† í°í™”ëœ ë¦¬ìŠ¤íŠ¸ê°€ ê¸°ëŒ€ ê²°ê³¼ì™€ ë‹¤ë¦…ë‹ˆë‹¤.\"\n",
    "print(\"ì²«ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "\n",
    "# Second test\n",
    "sentence = \"Jhon's book isn't popular, but he loves his book.\"\n",
    "tokens = tokenize(sentence)\n",
    "assert tokens == [\"jhon\", \"'s\", \"book\", \"is\", \"n't\", \"popular\", \",\", \"but\", \"he\", \"loves\", \"his\", \"book\", \".\"], \\\n",
    "    \"í† í°í™”ëœ ë¦¬ìŠ¤íŠ¸ê°€ ê¸°ëŒ€ ê²°ê³¼ì™€ ë‹¤ë¦…ë‹ˆë‹¤.\"\n",
    "print(\"ë‘ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "\n",
    "print(\"ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktSGEaoQnv9p"
   },
   "source": [
    "### 1-B) Vocabulary ë§Œë“¤ê¸°\n",
    "ì»´í“¨í„°ëŠ” ê¸€ìë¥¼ ì•Œì•„ë³¼ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— ê° í† í°ì„ ìˆ«ì í˜•ì‹ì˜ ìœ ì¼í•œ idì— ë§¤í•‘í•´ì•¼í•©ë‹ˆë‹¤.\n",
    "\n",
    "- ['I', 'have', 'a', 'meal'] ==> [194, 123, 2, 54]\n",
    "\n",
    "ì´ëŸ¬í•œ ë§¤í•‘ì€ ëª¨ë¸ í•™ìŠµ ì „ì— ì‚¬ì „ ì •ì˜ë˜ì–´ì•¼í•©ë‹ˆë‹¤.\n",
    "ì´ë•Œ, ëª¨ë¸ì´ ë‹¤ë¥¼ ìˆ˜ ìˆëŠ” í† í°ë“¤ì˜ ì§‘í•©ê³¼ ì´ ë§¤í•‘ì„ Vocabë¼ê³  í”íˆ ë¶€ë¦…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ë§¤í•‘ì„ ë§Œë“¤ì–´ ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "C1ztvrLinv9p"
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "# [UNK] í† í°\n",
    "unk_token = \"[UNK]\"\n",
    "unk_token_id = 0 # [UNK] í† í°ì˜ idëŠ” 0ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "def build_vocab(\n",
    "    sentences: List[List[str]],\n",
    "    min_freq: int\n",
    ") -> Tuple[List[str], Dict[str, int]]:\n",
    "    \"\"\" Vocabulary ë§Œë“¤ê¸°\n",
    "    í† í°í™”ëœ ë¬¸ì¥ë“¤ì„ ë°›ì•„ ê° í† í°ì„ ìˆ«ìë¡œ ë§¤í•‘í•˜ëŠ” token2idì™€ ê·¸ ì—­ë§¤í•‘ì¸ id2tokenë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "    ìì£¼ ì•ˆë‚˜ì˜¤ëŠ” ë‹¨ì–´ëŠ” ê³¼ì í•©ì„ ì¼ìœ¼í‚¬ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë¹ˆë„ê°€ ì ì€ ë‹¨ì–´ëŠ” [UNK] í† í°ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "    ì´ëŠ” Unknownì˜ ì¤€ë§ì…ë‹ˆë‹¤.\n",
    "    í† í°ì˜ id ë²ˆí˜¸ ìˆœì„œëŠ” [UNK] í† í°ì„ ì œì™¸í•˜ê³ ëŠ” ììœ ì…ë‹ˆë‹¤.\n",
    "\n",
    "    íŒíŠ¸: collection ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ Counter ê°ì²´ë¥¼ í™œìš©í•´ë³´ì„¸ìš”.\n",
    "\n",
    "    Arguments:\n",
    "    sentences -- Vocabularyë¥¼ ë§Œë“¤ê¸° ìœ„í•œ í† í°í™”ëœ ë¬¸ì¥ë“¤\n",
    "    min_freq -- ë‹¨ì¼ í† í°ìœ¼ë¡œ ì²˜ë¦¬ë˜ê¸° ìœ„í•œ ìµœì†Œ ë¹ˆë„\n",
    "                ë°ì´í„°ì…‹ì—ì„œ ìµœì†Œ ë¹ˆë„ë³´ë‹¤ ë” ì ê²Œ ë“±ì¥í•˜ëŠ” í† í°ì€ [UNK] í† í°ìœ¼ë¡œ ì²˜ë¦¬ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Return:\n",
    "    id2token -- idë¥¼ ë°›ìœ¼ë©´ í•´ë‹¹í•˜ëŠ” í† í°ì„ ë°˜í™˜í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ \n",
    "    token2id -- í† í°ì„ ë°›ìœ¼ë©´ í•´ë‹¹í•˜ëŠ” idë¥¼ ë°˜í™˜í•˜ëŠ” ë”•ì…”ë„ˆë¦¬\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    ### ANSWER HERE ###\n",
    "    \n",
    "    counter = Counter()\n",
    "    for sentence in sentences:\n",
    "        counter += Counter(sentence)\n",
    "    \n",
    "    id2token = [unk_token]\n",
    "    token2id = {unk_token: unk_token_id}\n",
    "    token_id = 1\n",
    "    for token, freq in zip(counter.keys(), counter.values()):\n",
    "        if freq >= min_freq:\n",
    "            id2token.append(token)\n",
    "            token2id[token] = token_id\n",
    "            token_id += 1\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    assert id2token[unk_token_id] == unk_token and token2id[unk_token] == unk_token_id, \\\n",
    "        \"[UNK] í† í°ì„ ì ì ˆíˆ ì‚½ì…í•˜ì„¸ìš”\"\n",
    "    assert len(id2token) == len(token2id), \\\n",
    "        \"id2wordê³¼ word2idì˜ í¬ê¸°ëŠ” ê°™ì•„ì•¼ í•©ë‹ˆë‹¤\"\n",
    "    return id2token, token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YqFU9McS5hen"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'a': 1, 'b': 2, 'c': 2, 'd': 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "Counter(chain(*[['a', 'b', 'c'], ['b', 'c', 'd']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HhPEbVtnv9q"
   },
   "source": [
    "**ë¬¸ì œ 1-Bì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wo74EyoHnv9q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Vocabulary Builder Test Cases======\n",
      "ì²«ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\n",
      "ë‘ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\n",
      "ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!\n"
     ]
    }
   ],
   "source": [
    "print (\"======Vocabulary Builder Test Cases======\")\n",
    "\n",
    "# First test\n",
    "sentences = [[\"this\", \"sentence\", \"be\", \"tokenized\", \"propery\", \".\"],\n",
    "                [\"jhon\", \"'s\", \"book\", \"is\", \"n't\", \"popular\", \",\", \"but\", \"he\", \"loves\", \"his\", \"book\", \".\"]]\n",
    "\n",
    "id2token, token2id = build_vocab(sentences, min_freq=1)\n",
    "assert sentences == [[id2token[token2id[token]] for token in sentence] for sentence in sentences], \\\n",
    "    \"token2idì™€ id2tokenì´ ì„œë¡œ ì—­ë§¤í•‘ì´ ì•„ë‹™ë‹ˆë‹¤.\"\n",
    "print(\"ì²«ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "\n",
    "# Second test\n",
    "sentences = [[\"a\", \"b\", \"c\", \"d\", \"e\"],\n",
    "                [\"c\", \"d\", \"f\", \"g\"],\n",
    "                [\"d\", \"e\", \"g\", \"h\"]]\n",
    "\n",
    "id2token, token2id = build_vocab(sentences, min_freq=2)\n",
    "assert set(token2id.keys()) == {unk_token, 'c', 'd', 'e', 'g'} == set(id2token) and len(id2token) == 5, \\\n",
    "    \"min_freq ì¸ìê°€ ì œëŒ€ë¡œ ì‘ë™ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\"\n",
    "print(\"ë‘ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "\n",
    "print(\"ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngupPsEJnv9r"
   },
   "source": [
    "### 1-C) ì¸ì½”ë”© ë° ë””ì½”ë”©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9E6YlDOInv9r"
   },
   "source": [
    "ì´ì œ ë¬¸ì¥ì„ ë°›ì•„ í† í°í™”í•˜ê³  ì´ë“¤ì„ ì ì ˆí•œ idë“¤ë¡œ ë°”ê¾¸ëŠ” ì¸ì½”ë”© í•¨ìˆ˜ë¥¼ ì‘ì„±í•´ ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "O6y5-YUAnv9r"
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def encode(\n",
    "    tokenize: Callable[[str], List[str]],\n",
    "    sentence: str,\n",
    "    token2id: Dict[str, int]\n",
    ") -> List[str]:\n",
    "    \"\"\" ì¸ì½”ë”©\n",
    "    ë¬¸ì¥ì„ ë°›ì•„ í† í°í™”í•˜ê³  ì´ë“¤ì„ ì ì ˆí•œ idë“¤ë¡œ ë°”ê¿‰ë‹ˆë‹¤.\n",
    "    í† í°í™” ë° ì¸ë±ì‹±ì€ ì¸ìë¡œ ë“¤ì–´ì˜¨ tokenize í•¨ìˆ˜ì™€ ì¸ìë¡œ ì£¼ì–´ì§„ token2idë¥¼ í™œìš©í•©ë‹ˆë‹¤.\n",
    "    Vocabì— ì—†ëŠ” ë‹¨ì–´ëŠ” [UNK] í† í°ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Arguments:\n",
    "    tokenize -- í† í°í™” í•¨ìˆ˜: ë¬¸ì¥ì„ ë°›ìœ¼ë©´ í† í°ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "    sentence -- í† í°í™”í•  ì˜ë¬¸ ë¬¸ì¥\n",
    "    token2id -- í† í°ì„ ë°›ìœ¼ë©´ í•´ë‹¹í•˜ëŠ” idë¥¼ ë°˜í™˜í•˜ëŠ” ë”•ì…”ë„ˆë¦¬\n",
    "    \n",
    "    Return:\n",
    "    token_ids -- ë¬¸ì¥ì„ ì¸ì½”ë”©í•˜ì—¬ ìˆ«ìë¡œ ë³€í™˜í•œ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE \n",
    "    ### ANSWER HERE ###\n",
    "    \n",
    "    unk_token_id = 0\n",
    "    tokens = tokenize(sentence)\n",
    "    token_ids = [token2id[token] if token in token2id else unk_token_id for token in tokens]\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return token_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10ibEviEnv9r"
   },
   "source": [
    "ê±°ê¾¸ë¡œ idë“¤ì´ ìˆì„ ë•Œ ì›ë¬¸ì¥ì„ ë³µì›í•˜ëŠ” ë””ì½”ë”© í•¨ìˆ˜ë„ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "ê·¸ëŸ¬ë‚˜ í† í°í™” ê³¼ì •ì—ì„œ ê³µë°± ë° ëŒ€ì†Œë¬¸ì ì •ë³´ë¥¼ ìƒì–´ë²„ë¦¬ê³ , [UNK] í† í°ìœ¼ë¡œ ì¸í•´ ì›ë¬¸ì¥ì„ ë³µì›í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
    "ë•Œë¬¸ì—, ë‹¨ìˆœíˆ ê³µë°±ìœ¼ë¡œ ì—°ê²°ëœ ë¬¸ì¥ìœ¼ë¡œ ë””ì½”ë”©í•©ì‹œë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7psbI7DTnv9r"
   },
   "outputs": [],
   "source": [
    "def decode(\n",
    "    token_ids: List[int],\n",
    "    id2token: List[str]\n",
    ") -> str:\n",
    "    \"\"\" ë””ì½”ë”©\n",
    "    ê° idë¥¼ ì ì ˆí•œ í† í°ìœ¼ë¡œ ë°”ê¾¸ê³  ê³µë°±ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ ë¬¸ì¥ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    return ' '.join(id2token[token_id] for token_id in token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsHhMzZHnv9s"
   },
   "source": [
    "**ì•ì„œ ë§Œë“  í•¨ìˆ˜ë¡œ ë§ë­‰ì¹˜ë¥¼ ì¸ì½”ë”©í•´ë´…ì‹œë‹¤.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QPa37yU8nv9s"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "id2token, token2id = build_vocab(list(map(tokenize, corpus)), min_freq=2)\n",
    "input_ids = list(map(partial(encode, tokenize, token2id=token2id), corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OeK5I0wwnv9s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======1=====\n",
      "ì›ë¬¸: A young man participates in a career while the subject who records it smiles.\n",
      "\n",
      "ì¸ì½”ë”© ê²°ê³¼: [1, 2, 3, 0, 4, 1, 0, 5, 6, 0, 7, 0, 8, 0, 9]\n",
      "ë””ì½”ë”© ê²°ê³¼: a young man [UNK] in a [UNK] while the [UNK] who [UNK] it [UNK] .\n",
      "\n",
      "======2=====\n",
      "ì›ë¬¸: The man is scratching the back of his neck while looking for a book in a book store.\n",
      "\n",
      "ì¸ì½”ë”© ê²°ê³¼: [6, 3, 10, 11, 6, 12, 13, 14, 0, 5, 15, 16, 1, 17, 4, 1, 17, 18, 9]\n",
      "ë””ì½”ë”© ê²°ê³¼: the man is scratching the back of his [UNK] while looking for a book in a book store .\n",
      "\n",
      "======3=====\n",
      "ì›ë¬¸: A person wearing goggles and a hat is sled riding.\n",
      "\n",
      "ì¸ì½”ë”© ê²°ê³¼: [1, 19, 20, 21, 22, 1, 23, 10, 0, 24, 9]\n",
      "ë””ì½”ë”© ê²°ê³¼: a person wearing goggles and a hat is [UNK] riding .\n",
      "\n",
      "======4=====\n",
      "ì›ë¬¸: A girl in a pink coat and flowered goloshes sledding down a hill.\n",
      "\n",
      "ì¸ì½”ë”© ê²°ê³¼: [1, 25, 4, 1, 26, 27, 22, 28, 0, 0, 29, 1, 30, 9]\n",
      "ë””ì½”ë”© ê²°ê³¼: a girl in a pink coat and flowered [UNK] [UNK] down a hill .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sid, sentence, token_ids in zip(range(1, 5), corpus, input_ids):\n",
    "    print(f\"======{sid}=====\")\n",
    "    print(f\"ì›ë¬¸: {sentence}\")\n",
    "    print(f\"ì¸ì½”ë”© ê²°ê³¼: {token_ids}\"),\n",
    "    print(f\"ë””ì½”ë”© ê²°ê³¼: {decode(token_ids, id2token)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzcdjwuanv9s"
   },
   "source": [
    "### 2. [Spacy](https://spacy.io/)ë¥¼ ì´ìš©í•œ ì˜ì–´ í…ìŠ¤íŠ¸ í† í°í™” ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "id": "56iMuXyinv9t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (910 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m911.0/911.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m492.2/492.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.7/124.7 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy) (67.6.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy) (1.23.5)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /home/kingstar/.local/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/kingstar/.local/lib/python3.10/site-packages (from spacy) (23.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy) (4.65.0)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kingstar/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/kingstar/.local/lib/python3.10/site-packages (from jinja2->spacy) (2.1.2)\n",
      "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, smart-open, pydantic, murmurhash, langcodes, catalogue, blis, srsly, preshed, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.7 smart-open-6.3.0 spacy-3.5.1 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.9 typer-0.7.0 wasabi-1.1.1\n",
      "2023-03-29 01:18:30.769369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-29 01:18:30.888934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-29 01:18:30.888994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from en-core-web-sm==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: setuptools in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.6.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/kingstar/.local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: jinja2 in /home/kingstar/.local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kingstar/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/kingstar/.local/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.5.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! pip install spacy\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ah64AjZnv9t"
   },
   "source": [
    "### 2-A) Spacyë¥¼ í™œìš©ë²•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "it8P7F2Knv9t"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 01:45:37.697753: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy_tokenizer = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsN9KjUVnv9t"
   },
   "source": [
    "Spacyë¥¼ í™œìš©í•œ í† í°í™”ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ì§€ë§Œ, í† í°í™” ì™¸ì—ë„ í’ˆì‚¬ ë° ë‹¨ì–´ì˜ ê¸°ë³¸í˜• ì •ë³´ ë“± í•´ë‹¹ ë¬¸ì¥ì— ëŒ€í•´ ë§ì€ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZheC-FJ9nv9t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Jhon', 'Jhon', 'PROPN'), (\"'s\", \"'s\", 'PART'), ('book', 'book', 'NOUN'), ('is', 'be', 'AUX'), (\"n't\", 'not', 'PART'), ('popular', 'popular', 'ADJ'), (',', ',', 'PUNCT'), ('but', 'but', 'CCONJ'), ('he', 'he', 'PRON'), ('loves', 'love', 'VERB'), ('his', 'his', 'PRON'), ('book', 'book', 'NOUN'), ('.', '.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "tokens = spacy_tokenizer(\"Jhon's book isn't popular, but he loves his book.\")\n",
    "print ([(token.text, token.lemma_, token.pos_) for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5z8zPlInv9t"
   },
   "source": [
    "**ë¶ˆìš©ì–´(Stopword)**\n",
    "\n",
    "ë¶ˆìš©ì–´ë€ í•œ ì–¸ì–´ì—ì„œ ìì£¼ ë“±ì¥í•˜ì§€ë§Œ í° ì˜ë¯¸ê°€ ì—†ëŠ” ë‹¨ì–´ë¥¼ ëœ¯í•©ë‹ˆë‹¤.\n",
    "ê³ ì „ì ì¸ ìì—°ì–´ ì²˜ë¦¬ì—ì„œëŠ” ì´ëŸ¬í•œ ë‹¨ì–´ë“¤ì€ ë¶„ì„ì— ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤ê³  ìƒê°í•˜ì˜€ê¸° ë•Œë¬¸ì— ì´ë¥¼ ì œê±°í•©ë‹ˆë‹¤.\n",
    "`Spacy`ì—ì„œëŠ” ë¶ˆìš©ì–´ ë‹¨ì–´ì˜ ëª©ë¡ì„ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y3r2nOGInv9t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'though', 'after', 'against', 'â€˜ll', 'few', 'here', 'wherein', 'and', 'doing', 'onto', 'around', 'they', 'meanwhile', 'move', 'therefore', 'somehow', 'she', 'your', \"'d\", 'beside', \"'re\", 'many', 'about', 'somewhere', 'could', 'who', 'done', 'take', 'keep', 'her', 'front', 'also', 'â€˜ve', 'one', 'nowhere', 'perhaps', 'seemed', 'serious', 'yet', 'four', 'to', 'please', 'my', 'thereupon', 'using', 'amount', 'must', 'off', 'that', 'used', 'made', 'not', 'may', 'whereupon', 'anything', 'six', 'amongst', 'beforehand', 'whoever', 'ca', 'very', 'give', 'does', 'thru', 'whenever', 'own', 'name', 'all', 'often', 'together', \"'s\", 'nâ€˜t', 'us', 'bottom', 'afterwards', 'been', 'have', 'thereby', 'an', 'latter', 'show', 'per', 'i', 'others', 'a', 'most', 'would', 'otherwise', 'did', 'becoming', 'toward', 'eight', 'twelve', 'however', 'our', 'why', 'up', 'part', 'yourself', 'hereafter', 'fifty', 'go', 'hundred', 'into', 'whom', 'just', 'anyway', 'latterly', 'something', 'even', 'much', 'under', 'either', 'for', 'during', 'hence', 'due', 'â€™s', 'where', 'anywhere', 'least', 'â€˜s', 'this', 'sometimes', 'but', 'by', 'third', \"'ve\", 'before', 'whatever', 'them', 'both', 'their', 'â€™m', 'can', 'thereafter', 'how', 'such', 'same', 'if', 'ours', 'sometime', 'side', 'almost', 'from', 'mostly', 'no', 'next', 'nevertheless', 'other', 'thus', \"'m\", 'then', 'since', 'seems', 'always', 'whole', 'yourselves', 'whereby', 'everything', 'various', 'say', 'whether', 'â€˜m', 'than', 'â€™ve', 'quite', 'are', 'now', 'were', 'get', 'him', 'else', 'upon', 'across', 'elsewhere', 'less', 'at', 'between', 'still', 'with', 'several', 'last', 'above', 'regarding', 'unless', 'am', 'further', 'herself', 'â€™d', 'his', 'while', 'yours', 'herein', 'neither', 'themselves', 'of', 'see', 'none', 'hereby', 'over', 'nâ€™t', 'the', 'there', 'namely', 'anyhow', 'hers', 'which', 'will', 'moreover', 'seem', 'or', 'first', 'really', 'whereas', 'along', 'in', 'until', 'throughout', 'three', 'top', 'already', 'full', 'whence', 'sixty', 'be', 'when', 'within', 'without', 'you', 'again', 'beyond', 'rather', 'besides', 'had', 'cannot', 'too', 'become', 'might', 'once', 'nine', 'â€™ll', 'twenty', 'was', 'more', 'wherever', 'nor', 'formerly', 'whereafter', 'do', 'towards', 'every', 'whose', 'each', 'among', 'ourselves', 'down', 'we', 'on', 'itself', 'those', 'another', 'mine', 'myself', 'himself', \"n't\", 'what', 'â€˜re', 'make', 'someone', 'noone', 'empty', 'except', 'whither', 'back', 'thence', 'enough', 'eleven', 'indeed', 'former', 'becomes', 'hereupon', 'became', 'never', 'is', 'ten', 'although', 'put', 'via', 'anyone', 'being', 'â€˜d', 'forty', 'some', 'everywhere', 'its', \"'ll\", 'out', 'alone', 'behind', 'as', 'seeming', 'nobody', 'therein', 'so', 'fifteen', 'only', 'he', 'me', 'below', 'call', 're', 'nothing', 'two', 'well', 'it', 'these', 'should', 'ever', 'â€™re', 'everyone', 'any', 'through', 'five', 'has', 'because'}\n"
     ]
    }
   ],
   "source": [
    "print(spacy.lang.en.stop_words.STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8RNGaXanv9u"
   },
   "source": [
    "### 2-B) Spacyë¥¼ í™œìš©í•œ ì „ì²˜ë¦¬ ë° í† í°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4QPSLb2knv9u"
   },
   "outputs": [],
   "source": [
    "def spacy_tokenize(\n",
    "    tokenizer: spacy.language.Language,\n",
    "    sentence: str\n",
    ") -> List[str]:\n",
    "    \"\"\" Spacyë¥¼ í™œìš©í•œ í† í¬ë‚˜ì´ì € êµ¬í˜„\n",
    "    Spacyë¥¼ í™œìš©í•´ì„œ í† í°í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤. ì´ë•Œ ë¶ˆìš©ì–´ëŠ” ì œì™¸í•˜ê³  ì–´ê°„ì„ í† í°ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    ì˜ˆì‹œ: 'I don't like Jenifer's work.'\n",
    "    ==> ['I', 'like', 'Jenifer', 'work', '.']\n",
    "\n",
    "    Arguments:\n",
    "    tokenizer -- Spacy í† í°í™”ê¸°\n",
    "    sentence -- í† í°í™”í•  ì˜ë¬¸ ë¬¸ì¥\n",
    "    \n",
    "    Return:\n",
    "    tokens -- ë¶ˆìš©ì–´ ì œê±° ë° í† í°í™”ëœ í† í° ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE \n",
    "    ### ANSWER HERE ###\n",
    "    \n",
    "    stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    tokenized_sentence = spacy_tokenizer(sentence)\n",
    "    \n",
    "    tokens = []\n",
    "    for token in tokenized_sentence:\n",
    "        if token.lemma_ not in stop_words:\n",
    "            tokens.append(token.lemma_)\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMNzsMm8nv9u"
   },
   "source": [
    "**ë¬¸ì œ 2-Bì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XFytcYYenv9u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Spacy Tokenizer Test Cases======\n",
      "ì²«ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\n",
      "ë‘ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\n",
      "ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!\n"
     ]
    }
   ],
   "source": [
    "print (\"======Spacy Tokenizer Test Cases======\")\n",
    "\n",
    "# First test\n",
    "sentence = \"This sentence should be tokenized properly.\"\n",
    "tokens = spacy_tokenize(spacy_tokenizer, sentence)\n",
    "assert tokens == ['sentence', 'tokenize', 'properly', '.'], \\\n",
    "    \"í† í°í™”ëœ ë¦¬ìŠ¤íŠ¸ê°€ ê¸°ëŒ€ ê²°ê³¼ì™€ ë‹¤ë¦…ë‹ˆë‹¤.\"\n",
    "print(\"ì²«ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "\n",
    "# Second test\n",
    "sentence = \"Jhon's book isn't popular, but he loves his book.\"\n",
    "tokens = spacy_tokenize(spacy_tokenizer, sentence)\n",
    "assert tokens == ['Jhon', 'book', 'popular', ',', 'love', 'book', '.'], \\\n",
    "    \"í† í°í™”ëœ ë¦¬ìŠ¤íŠ¸ê°€ ê¸°ëŒ€ ê²°ê³¼ì™€ ë‹¤ë¦…ë‹ˆë‹¤.\"\n",
    "print(\"ë‘ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "\n",
    "print(\"ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMOsG6vLnv9u"
   },
   "source": [
    "**ì•ì„œ ë§Œë“  í•¨ìˆ˜ë¡œ ë§ë­‰ì¹˜ë¥¼ ì¸ì½”ë”©í•´ë´…ì‹œë‹¤.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "VbhwbjZ1nv9u"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a626b0c1274f3495105b9fa2878f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building:   0%|          | 0/1071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c087ef07d84918838e9630834f8a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/1071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functools import partial\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "my_tokenize = partial(spacy_tokenize, spacy_tokenizer)\n",
    "id2token, token2id = build_vocab(list(map(my_tokenize, tqdm(corpus, desc=\"Building\"))), min_freq=3)\n",
    "input_ids = list(map(partial(encode, my_tokenize, token2id=token2id), tqdm(corpus, desc=\"Tokenizing\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0na1J950nv9v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======1=====\n",
      "ì›ë¬¸: A young man participates in a career while the subject who records it smiles.\n",
      "\n",
      "ì¸ì½”ë”© ê²°ê³¼: [1, 2, 3, 0, 0, 0, 4, 5, 6]\n",
      "ë””ì½”ë”© ê²°ê³¼: young man participate [UNK] [UNK] [UNK] smile . \n",
      "\n",
      "\n",
      "======2=====\n",
      "ì›ë¬¸: The man is scratching the back of his neck while looking for a book in a book store.\n",
      "\n",
      "ì¸ì½”ë”© ê²°ê³¼: [2, 7, 0, 8, 9, 9, 10, 5, 6]\n",
      "ë””ì½”ë”© ê²°ê³¼: man scratch [UNK] look book book store . \n",
      "\n",
      "\n",
      "======3=====\n",
      "ì›ë¬¸: A person wearing goggles and a hat is sled riding.\n",
      "\n",
      "ì¸ì½”ë”© ê²°ê³¼: [11, 12, 0, 13, 0, 0, 5, 6]\n",
      "ë””ì½”ë”© ê²°ê³¼: person wear [UNK] hat [UNK] [UNK] . \n",
      "\n",
      "\n",
      "======4=====\n",
      "ì›ë¬¸: A girl in a pink coat and flowered goloshes sledding down a hill.\n",
      "\n",
      "ì¸ì½”ë”© ê²°ê³¼: [14, 15, 16, 17, 0, 0, 18, 5, 6]\n",
      "ë””ì½”ë”© ê²°ê³¼: girl pink coat flower [UNK] [UNK] hill . \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sid, sentence, token_ids in zip(range(1, 5), corpus, input_ids):\n",
    "    print(f\"======{sid}=====\")\n",
    "    print(f\"ì›ë¬¸: {sentence}\")\n",
    "    print(f\"ì¸ì½”ë”© ê²°ê³¼: {token_ids}\"),\n",
    "    print(f\"ë””ì½”ë”© ê²°ê³¼: {decode(token_ids, id2token)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WQPklchnv9v"
   },
   "source": [
    "### 3. [Konlpy](https://konlpy.org/ko/latest/)ë¥¼ í™œìš©í•œ í•œêµ­ì–´ í† í°í™”\n",
    "í•œêµ­ì–´ì—ì„œ \"ë‚˜ëŠ” ë°¥ì„ ë¨¹ëŠ”ë‹¤\"ë¼ëŠ” ë¬¸ì¥ì„ ë‹¨ì–´ ë‹¨ìœ„ í† í°í™”í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
    "- ['ë‚˜', 'ëŠ”', 'ë°¥', 'ì„', 'ë¨¹ëŠ”ë‹¤']\n",
    "\n",
    "í•œêµ­ì–´ì—ì„œ \"ë‹¨ì–´\"ëŠ” ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ì˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŠ” í•œêµ­ì–´ê°€ ê°–ê³  ìˆëŠ” \"êµì°©ì–´\"ë¡œì„œì˜ íŠ¹ì§• ë•Œë¬¸ì…ë‹ˆë‹¤. \n",
    "ì²´ì–¸ ë’¤ì— ì¡°ì‚¬ê°€ ë¶™ëŠ” ê²ƒì´ ëŒ€í‘œì ì¸ íŠ¹ì§•ì´ë©° ì˜ë¯¸ ë‹¨ìœ„ê°€ êµ¬ë¶„ë˜ê³  ìë¦½ì„±ì´ ìˆê¸° ë•Œë¬¸ì— ì¡°ì‚¬ëŠ” \"ë‹¨ì–´\"ì…ë‹ˆë‹¤.\n",
    "\n",
    "í•œêµ­ì–´ì—ì„œëŠ” ë‹¨ì–´ ë‹¨ìœ„ í† í°í™” ë°©ë²•ëŠ” ê³µë°±ì— ê¸°ë°˜í•˜ì§€ ì•Šê³  ì‚¬ìš©í•˜ì§€ ì•Šê³  í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ í™œìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "(ì°¸ê³  1: [êµ­ë¦½ êµ­ì–´ì›: \"ì¡°ì‚¬ëŠ” ë‹¨ì–´ì´ë‹¤\"](https://korean.go.kr/front/onlineQna/onlineQnaView.do?mn_id=216&qna_seq=261271&pageIndex=1#:~:text=%EC%95%88%EB%85%95%ED%95%98%EC%8B%AD%EB%8B%88%EA%B9%8C%3F,%EC%A1%B0%EC%82%AC'%EB%8A%94%20%EB%8B%A8%EC%96%B4%EC%97%90%20%EC%86%8D%ED%95%A9%EB%8B%88%EB%8B%A4.) )\n",
    "\n",
    "(ì°¸ê³  2: [Konlpy: í˜•íƒœì†Œ ë¶„ì„ê¸°](https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBEqh_51nv9v"
   },
   "outputs": [],
   "source": [
    "! apt-get install -y build-essential openjdk-8-jdk python3-dev curl git automake\n",
    "! pip install konlpy \"tweepy<4.0.0\"\n",
    "! /bin/bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "x3C5EvQjnv9v"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "UDw0p5O6nv9v"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\\\n",
    "ìœ êµ¬í•œ ì—­ì‚¬ì™€ ì „í†µì— ë¹›ë‚˜ëŠ” ìš°ë¦¬ ëŒ€í•œêµ­ë¯¼ì€ \\\n",
    "3Â·1ìš´ë™ìœ¼ë¡œ ê±´ë¦½ëœ ëŒ€í•œë¯¼êµ­ì„ì‹œì •ë¶€ì˜ ë²•í†µê³¼ ë¶ˆì˜ì— í•­ê±°í•œ 4Â·19ë¯¼ì£¼ì´ë…ì„ ê³„ìŠ¹í•˜ê³ , \\\n",
    "ì¡°êµ­ì˜ ë¯¼ì£¼ê°œí˜ê³¼ í‰í™”ì  í†µì¼ì˜ ì‚¬ëª…ì— ì…ê°í•˜ì—¬ ì •ì˜Â·ì¸ë„ì™€ ë™í¬ì• ë¡œì¨ ë¯¼ì¡±ì˜ ë‹¨ê²°ì„ ê³µê³ íˆ í•˜ê³ , \\\n",
    "ëª¨ë“  ì‚¬íšŒì  íìŠµê³¼ ë¶ˆì˜ë¥¼ íƒ€íŒŒí•˜ë©°, \\\n",
    "ììœ¨ê³¼ ì¡°í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ììœ ë¯¼ì£¼ì  ê¸°ë³¸ì§ˆì„œë¥¼ ë”ìš± í™•ê³ íˆ í•˜ì—¬ \\\n",
    "ì •ì¹˜Â·ê²½ì œÂ·ì‚¬íšŒÂ·ë¬¸í™”ì˜ ëª¨ë“  ì˜ì—­ì— ìˆì–´ì„œ ê°ì¸ì˜ ê¸°íšŒë¥¼ ê· ë“±íˆ í•˜ê³ , \\\n",
    "ëŠ¥ë ¥ì„ ìµœê³ ë„ë¡œ ë°œíœ˜í•˜ê²Œ í•˜ë©°, ììœ ì™€ ê¶Œë¦¬ì— ë”°ë¥´ëŠ” ì±…ì„ê³¼ ì˜ë¬´ë¥¼ ì™„ìˆ˜í•˜ê²Œ í•˜ì—¬, \\\n",
    "ì•ˆìœ¼ë¡œëŠ” êµ­ë¯¼ìƒí™œì˜ ê· ë“±í•œ í–¥ìƒì„ ê¸°í•˜ê³  ë°–ìœ¼ë¡œëŠ” í•­êµ¬ì ì¸ ì„¸ê³„í‰í™”ì™€ ì¸ë¥˜ê³µì˜ì— ì´ë°”ì§€í•¨ìœ¼ë¡œì¨ \\\n",
    "ìš°ë¦¬ë“¤ê³¼ ìš°ë¦¬ë“¤ì˜ ìì†ì˜ ì•ˆì „ê³¼ ììœ ì™€ í–‰ë³µì„ ì˜ì›íˆ í™•ë³´í•  ê²ƒì„ ë‹¤ì§í•˜ë©´ì„œ \\\n",
    "1948ë…„ 7ì›” 12ì¼ì— ì œì •ë˜ê³  8ì°¨ì— ê±¸ì³ ê°œì •ëœ í—Œë²•ì„ ì´ì œ êµ­íšŒì˜ ì˜ê²°ì„ ê±°ì³ êµ­ë¯¼íˆ¬í‘œì— ì˜í•˜ì—¬ ê°œì •í•œë‹¤.\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "LvLEQFWVnv9v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ìœ êµ¬', 'XR'), ('í•œ', 'XSA+ETM'), ('ì—­ì‚¬', 'NNG'), ('ì™€', 'JC'), ('ì „í†µ', 'NNG'), ('ì—', 'JKB'), ('ë¹›ë‚˜', 'VV'), ('ëŠ”', 'ETM'), ('ìš°ë¦¬', 'NP'), ('ëŒ€í•œ', 'VV+ETM'), ('êµ­ë¯¼', 'NNG'), ('ì€', 'JX'), ('3', 'SN'), ('Â·', 'SC'), ('1', 'SN'), ('ìš´ë™', 'NNG'), ('ìœ¼ë¡œ', 'JKB'), ('ê±´ë¦½', 'NNG'), ('ëœ', 'XSV+ETM'), ('ëŒ€í•œë¯¼êµ­', 'NNP'), ('ì„ì‹œ', 'NNG'), ('ì •ë¶€', 'NNG'), ('ì˜', 'JKG'), ('ë²•í†µ', 'NNG'), ('ê³¼', 'JC'), ('ë¶ˆì˜', 'NNG'), ('ì—', 'JKB'), ('í•­ê±°', 'NNG'), ('í•œ', 'XSV+ETM'), ('4', 'SN'), ('Â·', 'SC'), ('19', 'SN'), ('ë¯¼ì£¼', 'NNG'), ('ì´ë…', 'NNG'), ('ì„', 'JKO'), ('ê³„ìŠ¹', 'NNG'), ('í•˜', 'XSV'), ('ê³ ', 'EC'), (',', 'SC'), ('ì¡°êµ­', 'NNG'), ('ì˜', 'JKG'), ('ë¯¼ì£¼', 'NNG'), ('ê°œí˜', 'NNG'), ('ê³¼', 'JC'), ('í‰í™”', 'NNG'), ('ì ', 'XSN'), ('í†µì¼', 'NNG'), ('ì˜', 'JKG'), ('ì‚¬ëª…', 'NNG'), ('ì—', 'JKB'), ('ì…ê°', 'NNG'), ('í•˜', 'XSV'), ('ì—¬', 'EC'), ('ì •ì˜', 'NNG'), ('Â·', 'SC'), ('ì¸ë„', 'NNP'), ('ì™€', 'JC'), ('ë™í¬', 'NNG'), ('ì• ', 'NNG'), ('ë¡œì¨', 'JKB'), ('ë¯¼ì¡±', 'NNG'), ('ì˜', 'JKG'), ('ë‹¨ê²°', 'NNG'), ('ì„', 'JKO'), ('ê³µê³ íˆ', 'MAG'), ('í•˜', 'VV'), ('ê³ ', 'EC'), (',', 'SC'), ('ëª¨ë“ ', 'MM'), ('ì‚¬íšŒ', 'NNG'), ('ì ', 'XSN'), ('íìŠµ', 'NNG'), ('ê³¼', 'JC'), ('ë¶ˆì˜', 'NNG'), ('ë¥¼', 'JKO'), ('íƒ€íŒŒ', 'NNG'), ('í•˜', 'XSV'), ('ë©°', 'EC'), (',', 'SC'), ('ììœ¨', 'NNG'), ('ê³¼', 'JC'), ('ì¡°í™”', 'NNG'), ('ë¥¼', 'JKO'), ('ë°”íƒ•', 'NNG'), ('ìœ¼ë¡œ', 'JKB'), ('ììœ ', 'NNG'), ('ë¯¼ì£¼', 'NNG'), ('ì ', 'XSN'), ('ê¸°ë³¸', 'NNG'), ('ì§ˆì„œ', 'NNG'), ('ë¥¼', 'JKO'), ('ë”ìš±', 'MAG'), ('í™•ê³ íˆ', 'MAG'), ('í•˜', 'VV'), ('ì—¬', 'EC'), ('ì •ì¹˜', 'NNG'), ('Â·', 'SC'), ('ê²½ì œ', 'NNG'), ('Â·', 'SC'), ('ì‚¬íšŒ', 'NNG'), ('Â·', 'SC'), ('ë¬¸í™”', 'NNG'), ('ì˜', 'JKG'), ('ëª¨ë“ ', 'MM'), ('ì˜ì—­', 'NNG'), ('ì—', 'JKB'), ('ìˆ', 'VV'), ('ì–´ì„œ', 'EC'), ('ê°ì¸', 'NNG'), ('ì˜', 'JKG'), ('ê¸°íšŒ', 'NNG'), ('ë¥¼', 'JKO'), ('ê· ë“±íˆ', 'MAG'), ('í•˜', 'VV'), ('ê³ ', 'EC'), (',', 'SC'), ('ëŠ¥ë ¥', 'NNG'), ('ì„', 'JKO'), ('ìµœê³ ', 'NNG'), ('ë„ë¡œ', 'NNG'), ('ë°œíœ˜', 'NNG'), ('í•˜', 'XSV'), ('ê²Œ', 'EC'), ('í•˜', 'VV'), ('ë©°', 'EC'), (',', 'SC'), ('ììœ ', 'NNG'), ('ì™€', 'JC'), ('ê¶Œë¦¬', 'NNG'), ('ì—', 'JKB'), ('ë”°ë¥´', 'VV'), ('ëŠ”', 'ETM'), ('ì±…ì„', 'NNG'), ('ê³¼', 'JC'), ('ì˜ë¬´', 'NNG'), ('ë¥¼', 'JKO'), ('ì™„ìˆ˜', 'NNG'), ('í•˜', 'XSV'), ('ê²Œ', 'EC'), ('í•˜', 'VV'), ('ì—¬', 'EC'), (',', 'SC'), ('ì•ˆ', 'NNG'), ('ìœ¼ë¡œ', 'JKB'), ('ëŠ”', 'JX'), ('êµ­ë¯¼', 'NNG'), ('ìƒí™œ', 'NNG'), ('ì˜', 'JKG'), ('ê· ë“±', 'NNG'), ('í•œ', 'XSA+ETM'), ('í–¥ìƒ', 'NNG'), ('ì„', 'JKO'), ('ê¸°í•˜', 'VV'), ('ê³ ', 'EC'), ('ë°–', 'NNG'), ('ìœ¼ë¡œ', 'JKB'), ('ëŠ”', 'JX'), ('í•­êµ¬', 'NNG'), ('ì ', 'XSN'), ('ì¸', 'VCP+ETM'), ('ì„¸ê³„', 'NNG'), ('í‰í™”', 'NNG'), ('ì™€', 'JC'), ('ì¸ë¥˜', 'NNG'), ('ê³µì˜', 'NNG'), ('ì—', 'JKB'), ('ì´ë°”ì§€', 'NNG'), ('í•¨', 'XSV+ETN'), ('ìœ¼ë¡œì¨', 'JKB'), ('ìš°ë¦¬', 'NP'), ('ë“¤', 'XSN'), ('ê³¼', 'JC'), ('ìš°ë¦¬', 'NP'), ('ë“¤', 'XSN'), ('ì˜', 'JKG'), ('ìì†', 'NNG'), ('ì˜', 'JKG'), ('ì•ˆì „', 'NNG'), ('ê³¼', 'JC'), ('ììœ ', 'NNG'), ('ì™€', 'JC'), ('í–‰ë³µ', 'NNG'), ('ì„', 'JKO'), ('ì˜ì›íˆ', 'MAG'), ('í™•ë³´', 'NNG'), ('í• ', 'XSV+ETM'), ('ê²ƒ', 'NNB'), ('ì„', 'JKO'), ('ë‹¤', 'MAG'), ('ì§', 'NNG'), ('í•˜', 'XSV'), ('ë©´ì„œ', 'EC'), ('1948', 'SN'), ('ë…„', 'NNBC'), ('7', 'SN'), ('ì›”', 'NNBC'), ('12', 'SN'), ('ì¼', 'NNBC'), ('ì—', 'JKB'), ('ì œì •', 'NNG'), ('ë˜', 'XSV'), ('ê³ ', 'EC'), ('8', 'SN'), ('ì°¨', 'NNBC'), ('ì—', 'JKB'), ('ê±¸ì³', 'VV+EC'), ('ê°œì •', 'NNG'), ('ëœ', 'XSV+ETM'), ('í—Œë²•', 'NNG'), ('ì„', 'JKO'), ('ì´ì œ', 'MAG'), ('êµ­íšŒ', 'NNG'), ('ì˜', 'JKG'), ('ì˜ê²°', 'NNG'), ('ì„', 'JKO'), ('ê±°ì³', 'VV+EC'), ('êµ­ë¯¼', 'NNG'), ('íˆ¬í‘œ', 'NNG'), ('ì—', 'JKB'), ('ì˜í•˜', 'VV'), ('ì—¬', 'EC'), ('ê°œì •', 'NNG'), ('í•œë‹¤', 'XSV+EF'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pos(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vcV9UHYRnv9w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ìœ êµ¬', 'í•œ', 'ì—­ì‚¬', 'ì™€', 'ì „í†µ', 'ì—', 'ë¹›ë‚˜', 'ëŠ”', 'ìš°ë¦¬', 'ëŒ€í•œ', 'êµ­ë¯¼', 'ì€', '3', 'Â·', '1', 'ìš´ë™', 'ìœ¼ë¡œ', 'ê±´ë¦½', 'ëœ', 'ëŒ€í•œë¯¼êµ­', 'ì„ì‹œ', 'ì •ë¶€', 'ì˜', 'ë²•í†µ', 'ê³¼', 'ë¶ˆì˜', 'ì—', 'í•­ê±°', 'í•œ', '4', 'Â·', '19', 'ë¯¼ì£¼', 'ì´ë…', 'ì„', 'ê³„ìŠ¹', 'í•˜', 'ê³ ', ',', 'ì¡°êµ­', 'ì˜', 'ë¯¼ì£¼', 'ê°œí˜', 'ê³¼', 'í‰í™”', 'ì ', 'í†µì¼', 'ì˜', 'ì‚¬ëª…', 'ì—', 'ì…ê°', 'í•˜', 'ì—¬', 'ì •ì˜', 'Â·', 'ì¸ë„', 'ì™€', 'ë™í¬', 'ì• ', 'ë¡œì¨', 'ë¯¼ì¡±', 'ì˜', 'ë‹¨ê²°', 'ì„', 'ê³µê³ íˆ', 'í•˜', 'ê³ ', ',', 'ëª¨ë“ ', 'ì‚¬íšŒ', 'ì ', 'íìŠµ', 'ê³¼', 'ë¶ˆì˜', 'ë¥¼', 'íƒ€íŒŒ', 'í•˜', 'ë©°', ',', 'ììœ¨', 'ê³¼', 'ì¡°í™”', 'ë¥¼', 'ë°”íƒ•', 'ìœ¼ë¡œ', 'ììœ ', 'ë¯¼ì£¼', 'ì ', 'ê¸°ë³¸', 'ì§ˆì„œ', 'ë¥¼', 'ë”ìš±', 'í™•ê³ íˆ', 'í•˜', 'ì—¬', 'ì •ì¹˜', 'Â·', 'ê²½ì œ', 'Â·', 'ì‚¬íšŒ', 'Â·', 'ë¬¸í™”', 'ì˜', 'ëª¨ë“ ', 'ì˜ì—­', 'ì—', 'ìˆ', 'ì–´ì„œ', 'ê°ì¸', 'ì˜', 'ê¸°íšŒ', 'ë¥¼', 'ê· ë“±íˆ', 'í•˜', 'ê³ ', ',', 'ëŠ¥ë ¥', 'ì„', 'ìµœê³ ', 'ë„ë¡œ', 'ë°œíœ˜', 'í•˜', 'ê²Œ', 'í•˜', 'ë©°', ',', 'ììœ ', 'ì™€', 'ê¶Œë¦¬', 'ì—', 'ë”°ë¥´', 'ëŠ”', 'ì±…ì„', 'ê³¼', 'ì˜ë¬´', 'ë¥¼', 'ì™„ìˆ˜', 'í•˜', 'ê²Œ', 'í•˜', 'ì—¬', ',', 'ì•ˆ', 'ìœ¼ë¡œ', 'ëŠ”', 'êµ­ë¯¼', 'ìƒí™œ', 'ì˜', 'ê· ë“±', 'í•œ', 'í–¥ìƒ', 'ì„', 'ê¸°í•˜', 'ê³ ', 'ë°–', 'ìœ¼ë¡œ', 'ëŠ”', 'í•­êµ¬', 'ì ', 'ì¸', 'ì„¸ê³„', 'í‰í™”', 'ì™€', 'ì¸ë¥˜', 'ê³µì˜', 'ì—', 'ì´ë°”ì§€', 'í•¨', 'ìœ¼ë¡œì¨', 'ìš°ë¦¬', 'ë“¤', 'ê³¼', 'ìš°ë¦¬', 'ë“¤', 'ì˜', 'ìì†', 'ì˜', 'ì•ˆì „', 'ê³¼', 'ììœ ', 'ì™€', 'í–‰ë³µ', 'ì„', 'ì˜ì›íˆ', 'í™•ë³´', 'í• ', 'ê²ƒ', 'ì„', 'ë‹¤', 'ì§', 'í•˜', 'ë©´ì„œ', '1948', 'ë…„', '7', 'ì›”', '12', 'ì¼', 'ì—', 'ì œì •', 'ë˜', 'ê³ ', '8', 'ì°¨', 'ì—', 'ê±¸ì³', 'ê°œì •', 'ëœ', 'í—Œë²•', 'ì„', 'ì´ì œ', 'êµ­íšŒ', 'ì˜', 'ì˜ê²°', 'ì„', 'ê±°ì³', 'êµ­ë¯¼', 'íˆ¬í‘œ', 'ì—', 'ì˜í•˜', 'ì—¬', 'ê°œì •', 'í•œë‹¤', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.morphs(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lL-YuMDeyJ0a"
   },
   "source": [
    "###**ì½˜í…ì¸  ë¼ì´ì„ ìŠ¤**\n",
    "\n",
    "<font color='red'><b>**WARNING**</b></font> : **ë³¸ êµìœ¡ ì½˜í…ì¸ ì˜ ì§€ì‹ì¬ì‚°ê¶Œì€ ì¬ë‹¨ë²•ì¸ ë„¤ì´ë²„ì»¤ë„¥íŠ¸ì— ê·€ì†ë©ë‹ˆë‹¤. ë³¸ ì½˜í…ì¸ ë¥¼ ì–´ë– í•œ ê²½ë¡œë¡œë“  ì™¸ë¶€ë¡œ ìœ ì¶œ ë° ìˆ˜ì •í•˜ëŠ” í–‰ìœ„ë¥¼ ì—„ê²©íˆ ê¸ˆí•©ë‹ˆë‹¤.** ë‹¤ë§Œ, ë¹„ì˜ë¦¬ì  êµìœ¡ ë° ì—°êµ¬í™œë™ì— í•œì •ë˜ì–´ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë‚˜ ì¬ë‹¨ì˜ í—ˆë½ì„ ë°›ì•„ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„ë°˜í•˜ëŠ” ê²½ìš°, ê´€ë ¨ ë²•ë¥ ì— ë”°ë¼ ì±…ì„ì„ ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "ml2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "aff83ea1928dcc0287770e0ea68916ae9727a33d95a26ca69018331bcc2781f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
