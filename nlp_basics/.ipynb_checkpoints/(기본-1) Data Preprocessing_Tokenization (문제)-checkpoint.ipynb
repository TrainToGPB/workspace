{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUTA4qLbnv9g"
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## 기본과제 1: Data Preprocessing & Tokenization\n",
    "\n",
    "> Reference 코드는 Solution 과 함께 공개됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0sJ05vTnv9k"
   },
   "source": [
    "### Introduction\n",
    "\n",
    "* 본 과제의 목적은 자연어 처리 모델에 활용하는 텍스트 데이터를 전치리 및 토큰화 과정의 개념을 익히는 것 입니다.\n",
    "* 영어 텍스트에선 토큰화 및 Vocabulary 작성을 통해 토큰화의 기본을 배우고 [Spacy](https://spacy.io/)으로 불용어를 제외 및 전처리합니다.\n",
    "* 한국어 텍스트에선 [Konlpy](https://konlpy.org/ko/latest/)를 활용하여 형태소 기반 토큰화를 진행합니다.\n",
    "* **ANSWER HERE** 이라고 작성된 부분을 채워 완성하시면 됩니다. 다른 부분의 코드를 변경하면 오류가 발생할 수 있습니다.\n",
    "\n",
    "> 과제 완성 후 ipynb 파일을 제출해 주세요.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8lFsD7Gnv9k"
   },
   "source": [
    "### 0. 데이터 업로드\n",
    "\n",
    "1. Boostcourse [기본 과제] Data Preprocessing & Tokenization 에서 `corpus.txt` 파일을 다운받습니다.\n",
    "2. 본 Colab 환경에 `corpus.txt`, 파일을 업로드합니다.\n",
    "3. `!ls` command 를 실행했을 때, `corpus.txt sample_data` 가 나오면 성공적으로 데이터 준비가 완료된 것 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "f87-7zDFnv9l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'(10강-실습) HuggingFace_s Transformers 2.ipynb'\r\n",
      "'(1강-실습-1) Naive Bayes classifier 구현.ipynb'\r\n",
      "'(1강-실습-2) Corpus Cleaning.ipynb'\r\n",
      "'(2강-실습-1) Word2Vec 구현 및 Embedding 시각화.ipynb'\r\n",
      "'(2강-실습-2) 다국어 임베딩.ipynb'\r\n",
      "'(3강-실습) Basic RNN 실습.ipynb'\r\n",
      "'(4강-실습) LSTM, GRU 실습.ipynb'\r\n",
      "'(5강-실습) Seq2Seq 구현.ipynb'\r\n",
      "'(6강-실습) Seq2Seq with Attention 구현.ipynb'\r\n",
      "'(7강-실습) Multi head Attention 구현.ipynb'\r\n",
      "'(8강-실습) Masked Multi-head Attention 구현.ipynb'\r\n",
      "'(9강-실습) HuggingFace_s Transformers 1.ipynb'\r\n",
      "'(기본-1) Data Preprocessing_Tokenization (문제).ipynb'\r\n",
      "'(기본-2) RNN-based Language Model (문제).ipynb'\r\n",
      "'(기본-3) Subword-level Language Model (문제).ipynb'\r\n",
      "'(기본-4) Preprocessing for NMT Model (문제).ipynb'\r\n",
      "'(심화-1) BERT Fine-tuning with Transformers (문제).ipynb'\r\n",
      "'(심화-2) NMT training with Fairseq (문제).ipynb'\r\n",
      " corpus.txt\r\n",
      " wikitext-2\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-FRJo54onv9m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1071\n",
      "A young man participates in a career while the subject who records it smiles.\n",
      "\n",
      "The man is scratching the back of his neck while looking for a book in a book store.\n",
      "\n",
      "A person wearing goggles and a hat is sled riding.\n",
      "\n",
      "A girl in a pink coat and flowered goloshes sledding down a hill.\n",
      "\n",
      "Three girls are standing in front of a window of a building.\n",
      "\n",
      "two dog is playing with a same chump on their mouth\n",
      "\n",
      "Low angle view of people suspended from the swings of a carnival ride.\n",
      "\n",
      "Black and white photo of two people watching a beautiful painting\n",
      "\n",
      "Man dressed in black crosses the street screaming\n",
      "\n",
      "A child sits on street on a busy street.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('./corpus.txt', 'r', encoding='utf-8') as fd:\n",
    "    corpus = fd.readlines()\n",
    "\n",
    "# 말뭉치 크기 확인\n",
    "print(len(corpus))\n",
    "\n",
    "# 첫 열 문장을 print 해 봅시다.\n",
    "for sentence in corpus[:10]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3AnsPdUnv9m"
   },
   "source": [
    "### 1. 파이썬 기본 코드를 이용한 영어 텍스트 토큰화 및 전처리\n",
    "\n",
    "\n",
    "💡 토큰화(tokenization)는 무엇인가요?\n",
    "\n",
    "토큰화는 주어진 입력 데이터를 자연어처리 모델이 인식할 수 있는 단위로 변환해주는 방법입니다. \n",
    "\n",
    "💡 단어 단위 토큰화(word tokenization)는요?\n",
    "\n",
    "단어단위 토큰화의 경우 \"단어\"가 자연어처리 모델이 인식하는 단위가 됩니다.\n",
    "\"I have a meal\"이라고 하는 문장을 가지고 단어 단위 토큰화를 하면 다음과 같습니다. \n",
    "\n",
    "- ['I', 'have', 'a', 'meal']\n",
    "\n",
    "영어의 경우 대부분 공백(space)을 기준으로 단어가 정의되기 때문에 `.split()`을 이용해 쉽게 단어 단위 토큰화를 구현할 수 있습니다.\n",
    "특히, 영어에서 공백을 기준으로 단어를 구분한 단어 단위 토큰화는 공백 단위 토큰화 (space tokenization)이라고도 할 수 있습니다.\n",
    "\n",
    "> 이 섹션에서는 파이썬 표준 라이브러리 (Python Standard Libary)만을 사용하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgaCeX5Pnv9n"
   },
   "source": [
    "#### 1-A) 토큰화기 (tokenizer) 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_PiVs9ganv9n"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re\n",
    "\n",
    "def tokenize(sentence: str) -> List[str]:\n",
    "    \"\"\" 토큰화기 구현\n",
    "    공백으로 토큰을 구분하되 . , ! ? 문장 부호는 별개의 토큰으로 처리되어야 합니다.\n",
    "    영문에서 Apostrophe에 해당하는 ' 는 두가지 경우에 대해 처리해야합니다.\n",
    "    1. not의 준말인 n't은 하나의 토큰으로 처리되어야 합니다: don't ==> do n't\n",
    "    2. 다른 Apostrophe 용법은 뒤의 글자들을 붙여서 처리합니다: 's 'm 're 등등 \n",
    "    그 외 다른 문장 부호는 고려하지 않으며, 작은 따옴표는 모두 Apostrophe로 처리합니다.\n",
    "    모든 토큰은 소문자로 변환되어야 합나다.\n",
    "\n",
    "    힌트: 정규표현식을 안다면 re 라이브러리를 사용해 보세요!\n",
    "\n",
    "    예시: 'I don't like Jenifer's work.'\n",
    "    ==> ['i', 'do', 'n\\'t', 'like', 'jenifer', '\\'s', 'work', '.']\n",
    "\n",
    "    Arguments:\n",
    "    sentence -- 토큰화할 영문 문장\n",
    "    \n",
    "    Return:\n",
    "    tokens -- 토큰화된 토큰 리스트\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE \n",
    "    ### ANSWER HERE ###\n",
    "    \n",
    "    sentence = re.sub(r\"n't\", r\" n't\", sentence)\n",
    "\n",
    "    tokens = re.findall(r\"n't|\\w+|[.,!?]+|'\\w+\", sentence)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'do', \"n't\", 'like', 'you', \"'re\", 'working', '.']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"I don't like You're working.\"\n",
    "tokenize(test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGtJjlgdnv9o"
   },
   "source": [
    "**문제 1-A에 대한 테스트 코드**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CFLfJmrznv9p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Tokenizer Test Cases======\n",
      "첫번째 테스트 통과!\n",
      "두번째 테스트 통과!\n",
      "모든 테스트 통과!\n"
     ]
    }
   ],
   "source": [
    "print (\"======Tokenizer Test Cases======\")\n",
    "\n",
    "# First test\n",
    "sentence = \"This sentence should be tokenized properly.\"\n",
    "tokens = tokenize(sentence)\n",
    "assert tokens == ['this', 'sentence', 'should', 'be', 'tokenized', 'properly', '.'], \\\n",
    "    \"토큰화된 리스트가 기대 결과와 다릅니다.\"\n",
    "print(\"첫번째 테스트 통과!\")\n",
    "\n",
    "# Second test\n",
    "sentence = \"Jhon's book isn't popular, but he loves his book.\"\n",
    "tokens = tokenize(sentence)\n",
    "assert tokens == [\"jhon\", \"'s\", \"book\", \"is\", \"n't\", \"popular\", \",\", \"but\", \"he\", \"loves\", \"his\", \"book\", \".\"], \\\n",
    "    \"토큰화된 리스트가 기대 결과와 다릅니다.\"\n",
    "print(\"두번째 테스트 통과!\")\n",
    "\n",
    "print(\"모든 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktSGEaoQnv9p"
   },
   "source": [
    "### 1-B) Vocabulary 만들기\n",
    "컴퓨터는 글자를 알아볼 수 없기 때문에 각 토큰을 숫자 형식의 유일한 id에 매핑해야합니다.\n",
    "\n",
    "- ['I', 'have', 'a', 'meal'] ==> [194, 123, 2, 54]\n",
    "\n",
    "이러한 매핑은 모델 학습 전에 사전 정의되어야합니다.\n",
    "이때, 모델이 다를 수 있는 토큰들의 집합과 이 매핑을 Vocab라고 흔히 부릅니다.\n",
    "\n",
    "이 매핑을 만들어 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "C1ztvrLinv9p"
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "# [UNK] 토큰\n",
    "unk_token = \"[UNK]\"\n",
    "unk_token_id = 0 # [UNK] 토큰의 id는 0으로 처리합니다.\n",
    "\n",
    "def build_vocab(\n",
    "    sentences: List[List[str]],\n",
    "    min_freq: int\n",
    ") -> Tuple[List[str], Dict[str, int]]:\n",
    "    \"\"\" Vocabulary 만들기\n",
    "    토큰화된 문장들을 받아 각 토큰을 숫자로 매핑하는 token2id와 그 역매핑인 id2token를 만듭니다.\n",
    "    자주 안나오는 단어는 과적합을 일으킬 수 있기 때문에 빈도가 적은 단어는 [UNK] 토큰으로 처리합니다.\n",
    "    이는 Unknown의 준말입니다.\n",
    "    토큰의 id 번호 순서는 [UNK] 토큰을 제외하고는 자유입니다.\n",
    "\n",
    "    힌트: collection 라이브러리의 Counter 객체를 활용해보세요.\n",
    "\n",
    "    Arguments:\n",
    "    sentences -- Vocabulary를 만들기 위한 토큰화된 문장들\n",
    "    min_freq -- 단일 토큰으로 처리되기 위한 최소 빈도\n",
    "                데이터셋에서 최소 빈도보다 더 적게 등장하는 토큰은 [UNK] 토큰으로 처리되어야 합니다.\n",
    "\n",
    "    Return:\n",
    "    id2token -- id를 받으면 해당하는 토큰을 반환하는 리스트 \n",
    "    token2id -- 토큰을 받으면 해당하는 id를 반환하는 딕셔너리\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    ### ANSWER HERE ###\n",
    "    \n",
    "    counter = Counter()\n",
    "    for sentence in sentences:\n",
    "        counter += Counter(sentence)\n",
    "    \n",
    "    id2token = [unk_token]\n",
    "    token2id = {unk_token: unk_token_id}\n",
    "    token_id = 1\n",
    "    for token, freq in zip(counter.keys(), counter.values()):\n",
    "        if freq >= min_freq:\n",
    "            id2token.append(token)\n",
    "            token2id[token] = token_id\n",
    "            token_id += 1\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    assert id2token[unk_token_id] == unk_token and token2id[unk_token] == unk_token_id, \\\n",
    "        \"[UNK] 토큰을 적절히 삽입하세요\"\n",
    "    assert len(id2token) == len(token2id), \\\n",
    "        \"id2word과 word2id의 크기는 같아야 합니다\"\n",
    "    return id2token, token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YqFU9McS5hen"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'a': 1, 'b': 2, 'c': 2, 'd': 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "Counter(chain(*[['a', 'b', 'c'], ['b', 'c', 'd']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HhPEbVtnv9q"
   },
   "source": [
    "**문제 1-B에 대한 테스트 코드**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wo74EyoHnv9q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Vocabulary Builder Test Cases======\n",
      "첫번째 테스트 통과!\n",
      "두번째 테스트 통과!\n",
      "모든 테스트 통과!\n"
     ]
    }
   ],
   "source": [
    "print (\"======Vocabulary Builder Test Cases======\")\n",
    "\n",
    "# First test\n",
    "sentences = [[\"this\", \"sentence\", \"be\", \"tokenized\", \"propery\", \".\"],\n",
    "                [\"jhon\", \"'s\", \"book\", \"is\", \"n't\", \"popular\", \",\", \"but\", \"he\", \"loves\", \"his\", \"book\", \".\"]]\n",
    "\n",
    "id2token, token2id = build_vocab(sentences, min_freq=1)\n",
    "assert sentences == [[id2token[token2id[token]] for token in sentence] for sentence in sentences], \\\n",
    "    \"token2id와 id2token이 서로 역매핑이 아닙니다.\"\n",
    "print(\"첫번째 테스트 통과!\")\n",
    "\n",
    "# Second test\n",
    "sentences = [[\"a\", \"b\", \"c\", \"d\", \"e\"],\n",
    "                [\"c\", \"d\", \"f\", \"g\"],\n",
    "                [\"d\", \"e\", \"g\", \"h\"]]\n",
    "\n",
    "id2token, token2id = build_vocab(sentences, min_freq=2)\n",
    "assert set(token2id.keys()) == {unk_token, 'c', 'd', 'e', 'g'} == set(id2token) and len(id2token) == 5, \\\n",
    "    \"min_freq 인자가 제대로 작동되지 않습니다.\"\n",
    "print(\"두번째 테스트 통과!\")\n",
    "\n",
    "print(\"모든 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngupPsEJnv9r"
   },
   "source": [
    "### 1-C) 인코딩 및 디코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9E6YlDOInv9r"
   },
   "source": [
    "이제 문장을 받아 토큰화하고 이들을 적절한 id들로 바꾸는 인코딩 함수를 작성해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "O6y5-YUAnv9r"
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def encode(\n",
    "    tokenize: Callable[[str], List[str]],\n",
    "    sentence: str,\n",
    "    token2id: Dict[str, int]\n",
    ") -> List[str]:\n",
    "    \"\"\" 인코딩\n",
    "    문장을 받아 토큰화하고 이들을 적절한 id들로 바꿉니다.\n",
    "    토큰화 및 인덱싱은 인자로 들어온 tokenize 함수와 인자로 주어진 token2id를 활용합니다.\n",
    "    Vocab에 없는 단어는 [UNK] 토큰으로 처리합니다.\n",
    "\n",
    "    Arguments:\n",
    "    tokenize -- 토큰화 함수: 문장을 받으면 토큰들의 리스트를 반환하는 함수\n",
    "    sentence -- 토큰화할 영문 문장\n",
    "    token2id -- 토큰을 받으면 해당하는 id를 반환하는 딕셔너리\n",
    "    \n",
    "    Return:\n",
    "    token_ids -- 문장을 인코딩하여 숫자로 변환한 리스트\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE \n",
    "    ### ANSWER HERE ###\n",
    "    \n",
    "    unk_token_id = 0\n",
    "    tokens = tokenize(sentence)\n",
    "    token_ids = [token2id[token] if token in token2id else unk_token_id for token in tokens]\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return token_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10ibEviEnv9r"
   },
   "source": [
    "거꾸로 id들이 있을 때 원문장을 복원하는 디코딩 함수도 필요합니다.\n",
    "그러나 토큰화 과정에서 공백 및 대소문자 정보를 잃어버리고, [UNK] 토큰으로 인해 원문장을 복원할 수 없습니다.\n",
    "때문에, 단순히 공백으로 연결된 문장으로 디코딩합시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7psbI7DTnv9r"
   },
   "outputs": [],
   "source": [
    "def decode(\n",
    "    token_ids: List[int],\n",
    "    id2token: List[str]\n",
    ") -> str:\n",
    "    \"\"\" 디코딩\n",
    "    각 id를 적절한 토큰으로 바꾸고 공백으로 연결하여 문장을 반환합니다.\n",
    "    \"\"\"\n",
    "    return ' '.join(id2token[token_id] for token_id in token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsHhMzZHnv9s"
   },
   "source": [
    "**앞서 만든 함수로 말뭉치를 인코딩해봅시다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QPa37yU8nv9s"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "id2token, token2id = build_vocab(list(map(tokenize, corpus)), min_freq=2)\n",
    "input_ids = list(map(partial(encode, tokenize, token2id=token2id), corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OeK5I0wwnv9s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======1=====\n",
      "원문: A young man participates in a career while the subject who records it smiles.\n",
      "\n",
      "인코딩 결과: [1, 2, 3, 0, 4, 1, 0, 5, 6, 0, 7, 0, 8, 0, 9]\n",
      "디코딩 결과: a young man [UNK] in a [UNK] while the [UNK] who [UNK] it [UNK] .\n",
      "\n",
      "======2=====\n",
      "원문: The man is scratching the back of his neck while looking for a book in a book store.\n",
      "\n",
      "인코딩 결과: [6, 3, 10, 11, 6, 12, 13, 14, 0, 5, 15, 16, 1, 17, 4, 1, 17, 18, 9]\n",
      "디코딩 결과: the man is scratching the back of his [UNK] while looking for a book in a book store .\n",
      "\n",
      "======3=====\n",
      "원문: A person wearing goggles and a hat is sled riding.\n",
      "\n",
      "인코딩 결과: [1, 19, 20, 21, 22, 1, 23, 10, 0, 24, 9]\n",
      "디코딩 결과: a person wearing goggles and a hat is [UNK] riding .\n",
      "\n",
      "======4=====\n",
      "원문: A girl in a pink coat and flowered goloshes sledding down a hill.\n",
      "\n",
      "인코딩 결과: [1, 25, 4, 1, 26, 27, 22, 28, 0, 0, 29, 1, 30, 9]\n",
      "디코딩 결과: a girl in a pink coat and flowered [UNK] [UNK] down a hill .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sid, sentence, token_ids in zip(range(1, 5), corpus, input_ids):\n",
    "    print(f\"======{sid}=====\")\n",
    "    print(f\"원문: {sentence}\")\n",
    "    print(f\"인코딩 결과: {token_ids}\"),\n",
    "    print(f\"디코딩 결과: {decode(token_ids, id2token)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzcdjwuanv9s"
   },
   "source": [
    "### 2. [Spacy](https://spacy.io/)를 이용한 영어 텍스트 토큰화 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "id": "56iMuXyinv9t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (910 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m911.0/911.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.2/492.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy) (67.6.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy) (1.23.5)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /home/kingstar/.local/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/kingstar/.local/lib/python3.10/site-packages (from spacy) (23.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy) (4.65.0)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kingstar/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/kingstar/.local/lib/python3.10/site-packages (from jinja2->spacy) (2.1.2)\n",
      "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, smart-open, pydantic, murmurhash, langcodes, catalogue, blis, srsly, preshed, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.7 smart-open-6.3.0 spacy-3.5.1 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.9 typer-0.7.0 wasabi-1.1.1\n",
      "2023-03-29 01:18:30.769369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-29 01:18:30.888934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-29 01:18:30.888994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from en-core-web-sm==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: setuptools in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.6.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/kingstar/.local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: jinja2 in /home/kingstar/.local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kingstar/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/kingstar/.local/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.5.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! pip install spacy\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ah64AjZnv9t"
   },
   "source": [
    "### 2-A) Spacy를 활용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "it8P7F2Knv9t"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 01:45:37.697753: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy_tokenizer = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsN9KjUVnv9t"
   },
   "source": [
    "Spacy를 활용한 토큰화는 상대적으로 시간이 오래 걸리지만, 토큰화 외에도 품사 및 단어의 기본형 정보 등 해당 문장에 대해 많은 정보를 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZheC-FJ9nv9t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Jhon', 'Jhon', 'PROPN'), (\"'s\", \"'s\", 'PART'), ('book', 'book', 'NOUN'), ('is', 'be', 'AUX'), (\"n't\", 'not', 'PART'), ('popular', 'popular', 'ADJ'), (',', ',', 'PUNCT'), ('but', 'but', 'CCONJ'), ('he', 'he', 'PRON'), ('loves', 'love', 'VERB'), ('his', 'his', 'PRON'), ('book', 'book', 'NOUN'), ('.', '.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "tokens = spacy_tokenizer(\"Jhon's book isn't popular, but he loves his book.\")\n",
    "print ([(token.text, token.lemma_, token.pos_) for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5z8zPlInv9t"
   },
   "source": [
    "**불용어(Stopword)**\n",
    "\n",
    "불용어란 한 언어에서 자주 등장하지만 큰 의미가 없는 단어를 뜯합니다.\n",
    "고전적인 자연어 처리에서는 이러한 단어들은 분석에 도움이 되지 않는다고 생각하였기 때문에 이를 제거합니다.\n",
    "`Spacy`에서는 불용어 단어의 목록을 제공하고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y3r2nOGInv9t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'though', 'after', 'against', '‘ll', 'few', 'here', 'wherein', 'and', 'doing', 'onto', 'around', 'they', 'meanwhile', 'move', 'therefore', 'somehow', 'she', 'your', \"'d\", 'beside', \"'re\", 'many', 'about', 'somewhere', 'could', 'who', 'done', 'take', 'keep', 'her', 'front', 'also', '‘ve', 'one', 'nowhere', 'perhaps', 'seemed', 'serious', 'yet', 'four', 'to', 'please', 'my', 'thereupon', 'using', 'amount', 'must', 'off', 'that', 'used', 'made', 'not', 'may', 'whereupon', 'anything', 'six', 'amongst', 'beforehand', 'whoever', 'ca', 'very', 'give', 'does', 'thru', 'whenever', 'own', 'name', 'all', 'often', 'together', \"'s\", 'n‘t', 'us', 'bottom', 'afterwards', 'been', 'have', 'thereby', 'an', 'latter', 'show', 'per', 'i', 'others', 'a', 'most', 'would', 'otherwise', 'did', 'becoming', 'toward', 'eight', 'twelve', 'however', 'our', 'why', 'up', 'part', 'yourself', 'hereafter', 'fifty', 'go', 'hundred', 'into', 'whom', 'just', 'anyway', 'latterly', 'something', 'even', 'much', 'under', 'either', 'for', 'during', 'hence', 'due', '’s', 'where', 'anywhere', 'least', '‘s', 'this', 'sometimes', 'but', 'by', 'third', \"'ve\", 'before', 'whatever', 'them', 'both', 'their', '’m', 'can', 'thereafter', 'how', 'such', 'same', 'if', 'ours', 'sometime', 'side', 'almost', 'from', 'mostly', 'no', 'next', 'nevertheless', 'other', 'thus', \"'m\", 'then', 'since', 'seems', 'always', 'whole', 'yourselves', 'whereby', 'everything', 'various', 'say', 'whether', '‘m', 'than', '’ve', 'quite', 'are', 'now', 'were', 'get', 'him', 'else', 'upon', 'across', 'elsewhere', 'less', 'at', 'between', 'still', 'with', 'several', 'last', 'above', 'regarding', 'unless', 'am', 'further', 'herself', '’d', 'his', 'while', 'yours', 'herein', 'neither', 'themselves', 'of', 'see', 'none', 'hereby', 'over', 'n’t', 'the', 'there', 'namely', 'anyhow', 'hers', 'which', 'will', 'moreover', 'seem', 'or', 'first', 'really', 'whereas', 'along', 'in', 'until', 'throughout', 'three', 'top', 'already', 'full', 'whence', 'sixty', 'be', 'when', 'within', 'without', 'you', 'again', 'beyond', 'rather', 'besides', 'had', 'cannot', 'too', 'become', 'might', 'once', 'nine', '’ll', 'twenty', 'was', 'more', 'wherever', 'nor', 'formerly', 'whereafter', 'do', 'towards', 'every', 'whose', 'each', 'among', 'ourselves', 'down', 'we', 'on', 'itself', 'those', 'another', 'mine', 'myself', 'himself', \"n't\", 'what', '‘re', 'make', 'someone', 'noone', 'empty', 'except', 'whither', 'back', 'thence', 'enough', 'eleven', 'indeed', 'former', 'becomes', 'hereupon', 'became', 'never', 'is', 'ten', 'although', 'put', 'via', 'anyone', 'being', '‘d', 'forty', 'some', 'everywhere', 'its', \"'ll\", 'out', 'alone', 'behind', 'as', 'seeming', 'nobody', 'therein', 'so', 'fifteen', 'only', 'he', 'me', 'below', 'call', 're', 'nothing', 'two', 'well', 'it', 'these', 'should', 'ever', '’re', 'everyone', 'any', 'through', 'five', 'has', 'because'}\n"
     ]
    }
   ],
   "source": [
    "print(spacy.lang.en.stop_words.STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8RNGaXanv9u"
   },
   "source": [
    "### 2-B) Spacy를 활용한 전처리 및 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4QPSLb2knv9u"
   },
   "outputs": [],
   "source": [
    "def spacy_tokenize(\n",
    "    tokenizer: spacy.language.Language,\n",
    "    sentence: str\n",
    ") -> List[str]:\n",
    "    \"\"\" Spacy를 활용한 토크나이저 구현\n",
    "    Spacy를 활용해서 토큰화를 진행합니다. 이때 불용어는 제외하고 어간을 토큰으로 사용합니다.\n",
    "    \n",
    "    예시: 'I don't like Jenifer's work.'\n",
    "    ==> ['I', 'like', 'Jenifer', 'work', '.']\n",
    "\n",
    "    Arguments:\n",
    "    tokenizer -- Spacy 토큰화기\n",
    "    sentence -- 토큰화할 영문 문장\n",
    "    \n",
    "    Return:\n",
    "    tokens -- 불용어 제거 및 토큰화된 토큰 리스트\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE \n",
    "    ### ANSWER HERE ###\n",
    "    \n",
    "    stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    tokenized_sentence = spacy_tokenizer(sentence)\n",
    "    \n",
    "    tokens = []\n",
    "    for token in tokenized_sentence:\n",
    "        if token.lemma_ not in stop_words:\n",
    "            tokens.append(token.lemma_)\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMNzsMm8nv9u"
   },
   "source": [
    "**문제 2-B에 대한 테스트 코드**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XFytcYYenv9u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Spacy Tokenizer Test Cases======\n",
      "첫번째 테스트 통과!\n",
      "두번째 테스트 통과!\n",
      "모든 테스트 통과!\n"
     ]
    }
   ],
   "source": [
    "print (\"======Spacy Tokenizer Test Cases======\")\n",
    "\n",
    "# First test\n",
    "sentence = \"This sentence should be tokenized properly.\"\n",
    "tokens = spacy_tokenize(spacy_tokenizer, sentence)\n",
    "assert tokens == ['sentence', 'tokenize', 'properly', '.'], \\\n",
    "    \"토큰화된 리스트가 기대 결과와 다릅니다.\"\n",
    "print(\"첫번째 테스트 통과!\")\n",
    "\n",
    "# Second test\n",
    "sentence = \"Jhon's book isn't popular, but he loves his book.\"\n",
    "tokens = spacy_tokenize(spacy_tokenizer, sentence)\n",
    "assert tokens == ['Jhon', 'book', 'popular', ',', 'love', 'book', '.'], \\\n",
    "    \"토큰화된 리스트가 기대 결과와 다릅니다.\"\n",
    "print(\"두번째 테스트 통과!\")\n",
    "\n",
    "print(\"모든 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMOsG6vLnv9u"
   },
   "source": [
    "**앞서 만든 함수로 말뭉치를 인코딩해봅시다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "VbhwbjZ1nv9u"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a626b0c1274f3495105b9fa2878f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building:   0%|          | 0/1071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c087ef07d84918838e9630834f8a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/1071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functools import partial\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "my_tokenize = partial(spacy_tokenize, spacy_tokenizer)\n",
    "id2token, token2id = build_vocab(list(map(my_tokenize, tqdm(corpus, desc=\"Building\"))), min_freq=3)\n",
    "input_ids = list(map(partial(encode, my_tokenize, token2id=token2id), tqdm(corpus, desc=\"Tokenizing\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0na1J950nv9v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======1=====\n",
      "원문: A young man participates in a career while the subject who records it smiles.\n",
      "\n",
      "인코딩 결과: [1, 2, 3, 0, 0, 0, 4, 5, 6]\n",
      "디코딩 결과: young man participate [UNK] [UNK] [UNK] smile . \n",
      "\n",
      "\n",
      "======2=====\n",
      "원문: The man is scratching the back of his neck while looking for a book in a book store.\n",
      "\n",
      "인코딩 결과: [2, 7, 0, 8, 9, 9, 10, 5, 6]\n",
      "디코딩 결과: man scratch [UNK] look book book store . \n",
      "\n",
      "\n",
      "======3=====\n",
      "원문: A person wearing goggles and a hat is sled riding.\n",
      "\n",
      "인코딩 결과: [11, 12, 0, 13, 0, 0, 5, 6]\n",
      "디코딩 결과: person wear [UNK] hat [UNK] [UNK] . \n",
      "\n",
      "\n",
      "======4=====\n",
      "원문: A girl in a pink coat and flowered goloshes sledding down a hill.\n",
      "\n",
      "인코딩 결과: [14, 15, 16, 17, 0, 0, 18, 5, 6]\n",
      "디코딩 결과: girl pink coat flower [UNK] [UNK] hill . \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sid, sentence, token_ids in zip(range(1, 5), corpus, input_ids):\n",
    "    print(f\"======{sid}=====\")\n",
    "    print(f\"원문: {sentence}\")\n",
    "    print(f\"인코딩 결과: {token_ids}\"),\n",
    "    print(f\"디코딩 결과: {decode(token_ids, id2token)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WQPklchnv9v"
   },
   "source": [
    "### 3. [Konlpy](https://konlpy.org/ko/latest/)를 활용한 한국어 토큰화\n",
    "한국어에서 \"나는 밥을 먹는다\"라는 문장을 단어 단위 토큰화하면 다음과 같습니다.\n",
    "- ['나', '는', '밥', '을', '먹는다']\n",
    "\n",
    "한국어에서 \"단어\"는 공백을 기준으로 정의되지 않습니다. 이는 한국어가 갖고 있는 \"교착어\"로서의 특징 때문입니다. \n",
    "체언 뒤에 조사가 붙는 것이 대표적인 특징이며 의미 단위가 구분되고 자립성이 있기 때문에 조사는 \"단어\"입니다.\n",
    "\n",
    "한국어에서는 단어 단위 토큰화 방법는 공백에 기반하지 않고 사용하지 않고 형태소 분석기를 활용하고 있습니다.\n",
    "\n",
    "\n",
    "(참고 1: [국립 국어원: \"조사는 단어이다\"](https://korean.go.kr/front/onlineQna/onlineQnaView.do?mn_id=216&qna_seq=261271&pageIndex=1#:~:text=%EC%95%88%EB%85%95%ED%95%98%EC%8B%AD%EB%8B%88%EA%B9%8C%3F,%EC%A1%B0%EC%82%AC'%EB%8A%94%20%EB%8B%A8%EC%96%B4%EC%97%90%20%EC%86%8D%ED%95%A9%EB%8B%88%EB%8B%A4.) )\n",
    "\n",
    "(참고 2: [Konlpy: 형태소 분석기](https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBEqh_51nv9v"
   },
   "outputs": [],
   "source": [
    "! apt-get install -y build-essential openjdk-8-jdk python3-dev curl git automake\n",
    "! pip install konlpy \"tweepy<4.0.0\"\n",
    "! /bin/bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "x3C5EvQjnv9v"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "UDw0p5O6nv9v"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\\\n",
    "유구한 역사와 전통에 빛나는 우리 대한국민은 \\\n",
    "3·1운동으로 건립된 대한민국임시정부의 법통과 불의에 항거한 4·19민주이념을 계승하고, \\\n",
    "조국의 민주개혁과 평화적 통일의 사명에 입각하여 정의·인도와 동포애로써 민족의 단결을 공고히 하고, \\\n",
    "모든 사회적 폐습과 불의를 타파하며, \\\n",
    "자율과 조화를 바탕으로 자유민주적 기본질서를 더욱 확고히 하여 \\\n",
    "정치·경제·사회·문화의 모든 영역에 있어서 각인의 기회를 균등히 하고, \\\n",
    "능력을 최고도로 발휘하게 하며, 자유와 권리에 따르는 책임과 의무를 완수하게 하여, \\\n",
    "안으로는 국민생활의 균등한 향상을 기하고 밖으로는 항구적인 세계평화와 인류공영에 이바지함으로써 \\\n",
    "우리들과 우리들의 자손의 안전과 자유와 행복을 영원히 확보할 것을 다짐하면서 \\\n",
    "1948년 7월 12일에 제정되고 8차에 걸쳐 개정된 헌법을 이제 국회의 의결을 거쳐 국민투표에 의하여 개정한다.\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "LvLEQFWVnv9v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('유구', 'XR'), ('한', 'XSA+ETM'), ('역사', 'NNG'), ('와', 'JC'), ('전통', 'NNG'), ('에', 'JKB'), ('빛나', 'VV'), ('는', 'ETM'), ('우리', 'NP'), ('대한', 'VV+ETM'), ('국민', 'NNG'), ('은', 'JX'), ('3', 'SN'), ('·', 'SC'), ('1', 'SN'), ('운동', 'NNG'), ('으로', 'JKB'), ('건립', 'NNG'), ('된', 'XSV+ETM'), ('대한민국', 'NNP'), ('임시', 'NNG'), ('정부', 'NNG'), ('의', 'JKG'), ('법통', 'NNG'), ('과', 'JC'), ('불의', 'NNG'), ('에', 'JKB'), ('항거', 'NNG'), ('한', 'XSV+ETM'), ('4', 'SN'), ('·', 'SC'), ('19', 'SN'), ('민주', 'NNG'), ('이념', 'NNG'), ('을', 'JKO'), ('계승', 'NNG'), ('하', 'XSV'), ('고', 'EC'), (',', 'SC'), ('조국', 'NNG'), ('의', 'JKG'), ('민주', 'NNG'), ('개혁', 'NNG'), ('과', 'JC'), ('평화', 'NNG'), ('적', 'XSN'), ('통일', 'NNG'), ('의', 'JKG'), ('사명', 'NNG'), ('에', 'JKB'), ('입각', 'NNG'), ('하', 'XSV'), ('여', 'EC'), ('정의', 'NNG'), ('·', 'SC'), ('인도', 'NNP'), ('와', 'JC'), ('동포', 'NNG'), ('애', 'NNG'), ('로써', 'JKB'), ('민족', 'NNG'), ('의', 'JKG'), ('단결', 'NNG'), ('을', 'JKO'), ('공고히', 'MAG'), ('하', 'VV'), ('고', 'EC'), (',', 'SC'), ('모든', 'MM'), ('사회', 'NNG'), ('적', 'XSN'), ('폐습', 'NNG'), ('과', 'JC'), ('불의', 'NNG'), ('를', 'JKO'), ('타파', 'NNG'), ('하', 'XSV'), ('며', 'EC'), (',', 'SC'), ('자율', 'NNG'), ('과', 'JC'), ('조화', 'NNG'), ('를', 'JKO'), ('바탕', 'NNG'), ('으로', 'JKB'), ('자유', 'NNG'), ('민주', 'NNG'), ('적', 'XSN'), ('기본', 'NNG'), ('질서', 'NNG'), ('를', 'JKO'), ('더욱', 'MAG'), ('확고히', 'MAG'), ('하', 'VV'), ('여', 'EC'), ('정치', 'NNG'), ('·', 'SC'), ('경제', 'NNG'), ('·', 'SC'), ('사회', 'NNG'), ('·', 'SC'), ('문화', 'NNG'), ('의', 'JKG'), ('모든', 'MM'), ('영역', 'NNG'), ('에', 'JKB'), ('있', 'VV'), ('어서', 'EC'), ('각인', 'NNG'), ('의', 'JKG'), ('기회', 'NNG'), ('를', 'JKO'), ('균등히', 'MAG'), ('하', 'VV'), ('고', 'EC'), (',', 'SC'), ('능력', 'NNG'), ('을', 'JKO'), ('최고', 'NNG'), ('도로', 'NNG'), ('발휘', 'NNG'), ('하', 'XSV'), ('게', 'EC'), ('하', 'VV'), ('며', 'EC'), (',', 'SC'), ('자유', 'NNG'), ('와', 'JC'), ('권리', 'NNG'), ('에', 'JKB'), ('따르', 'VV'), ('는', 'ETM'), ('책임', 'NNG'), ('과', 'JC'), ('의무', 'NNG'), ('를', 'JKO'), ('완수', 'NNG'), ('하', 'XSV'), ('게', 'EC'), ('하', 'VV'), ('여', 'EC'), (',', 'SC'), ('안', 'NNG'), ('으로', 'JKB'), ('는', 'JX'), ('국민', 'NNG'), ('생활', 'NNG'), ('의', 'JKG'), ('균등', 'NNG'), ('한', 'XSA+ETM'), ('향상', 'NNG'), ('을', 'JKO'), ('기하', 'VV'), ('고', 'EC'), ('밖', 'NNG'), ('으로', 'JKB'), ('는', 'JX'), ('항구', 'NNG'), ('적', 'XSN'), ('인', 'VCP+ETM'), ('세계', 'NNG'), ('평화', 'NNG'), ('와', 'JC'), ('인류', 'NNG'), ('공영', 'NNG'), ('에', 'JKB'), ('이바지', 'NNG'), ('함', 'XSV+ETN'), ('으로써', 'JKB'), ('우리', 'NP'), ('들', 'XSN'), ('과', 'JC'), ('우리', 'NP'), ('들', 'XSN'), ('의', 'JKG'), ('자손', 'NNG'), ('의', 'JKG'), ('안전', 'NNG'), ('과', 'JC'), ('자유', 'NNG'), ('와', 'JC'), ('행복', 'NNG'), ('을', 'JKO'), ('영원히', 'MAG'), ('확보', 'NNG'), ('할', 'XSV+ETM'), ('것', 'NNB'), ('을', 'JKO'), ('다', 'MAG'), ('짐', 'NNG'), ('하', 'XSV'), ('면서', 'EC'), ('1948', 'SN'), ('년', 'NNBC'), ('7', 'SN'), ('월', 'NNBC'), ('12', 'SN'), ('일', 'NNBC'), ('에', 'JKB'), ('제정', 'NNG'), ('되', 'XSV'), ('고', 'EC'), ('8', 'SN'), ('차', 'NNBC'), ('에', 'JKB'), ('걸쳐', 'VV+EC'), ('개정', 'NNG'), ('된', 'XSV+ETM'), ('헌법', 'NNG'), ('을', 'JKO'), ('이제', 'MAG'), ('국회', 'NNG'), ('의', 'JKG'), ('의결', 'NNG'), ('을', 'JKO'), ('거쳐', 'VV+EC'), ('국민', 'NNG'), ('투표', 'NNG'), ('에', 'JKB'), ('의하', 'VV'), ('여', 'EC'), ('개정', 'NNG'), ('한다', 'XSV+EF'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pos(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vcV9UHYRnv9w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['유구', '한', '역사', '와', '전통', '에', '빛나', '는', '우리', '대한', '국민', '은', '3', '·', '1', '운동', '으로', '건립', '된', '대한민국', '임시', '정부', '의', '법통', '과', '불의', '에', '항거', '한', '4', '·', '19', '민주', '이념', '을', '계승', '하', '고', ',', '조국', '의', '민주', '개혁', '과', '평화', '적', '통일', '의', '사명', '에', '입각', '하', '여', '정의', '·', '인도', '와', '동포', '애', '로써', '민족', '의', '단결', '을', '공고히', '하', '고', ',', '모든', '사회', '적', '폐습', '과', '불의', '를', '타파', '하', '며', ',', '자율', '과', '조화', '를', '바탕', '으로', '자유', '민주', '적', '기본', '질서', '를', '더욱', '확고히', '하', '여', '정치', '·', '경제', '·', '사회', '·', '문화', '의', '모든', '영역', '에', '있', '어서', '각인', '의', '기회', '를', '균등히', '하', '고', ',', '능력', '을', '최고', '도로', '발휘', '하', '게', '하', '며', ',', '자유', '와', '권리', '에', '따르', '는', '책임', '과', '의무', '를', '완수', '하', '게', '하', '여', ',', '안', '으로', '는', '국민', '생활', '의', '균등', '한', '향상', '을', '기하', '고', '밖', '으로', '는', '항구', '적', '인', '세계', '평화', '와', '인류', '공영', '에', '이바지', '함', '으로써', '우리', '들', '과', '우리', '들', '의', '자손', '의', '안전', '과', '자유', '와', '행복', '을', '영원히', '확보', '할', '것', '을', '다', '짐', '하', '면서', '1948', '년', '7', '월', '12', '일', '에', '제정', '되', '고', '8', '차', '에', '걸쳐', '개정', '된', '헌법', '을', '이제', '국회', '의', '의결', '을', '거쳐', '국민', '투표', '에', '의하', '여', '개정', '한다', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.morphs(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lL-YuMDeyJ0a"
   },
   "source": [
    "###**콘텐츠 라이선스**\n",
    "\n",
    "<font color='red'><b>**WARNING**</b></font> : **본 교육 콘텐츠의 지식재산권은 재단법인 네이버커넥트에 귀속됩니다. 본 콘텐츠를 어떠한 경로로든 외부로 유출 및 수정하는 행위를 엄격히 금합니다.** 다만, 비영리적 교육 및 연구활동에 한정되어 사용할 수 있으나 재단의 허락을 받아야 합니다. 이를 위반하는 경우, 관련 법률에 따라 책임을 질 수 있습니다.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "ml2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "aff83ea1928dcc0287770e0ea68916ae9727a33d95a26ca69018331bcc2781f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
