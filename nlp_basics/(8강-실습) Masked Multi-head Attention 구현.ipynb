{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KsBGZpKkWki"
   },
   "source": [
    "##**8. Masked Multi-head Attention**\n",
    "1. Masked Multi-head Attention 구현.\n",
    "2. Encoder-Decoder Attention 구현."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qRU5DFY2OM8"
   },
   "source": [
    "### **필요 패키지 import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lDtMioSQQ1bB"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBiZObgRep_Q"
   },
   "source": [
    "### **데이터 전처리**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfTSaGYteuze"
   },
   "source": [
    "데이터의 값과 형태를 좀 더 명확하게 보기 위해 sample을 줄이겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "e9ULZIqTenSc"
   },
   "outputs": [],
   "source": [
    "pad_id = 0\n",
    "vocab_size = 100\n",
    "\n",
    "data = [\n",
    "  [62, 13, 47, 39, 78, 33, 56, 13],\n",
    "  [60, 96, 51, 32, 90],\n",
    "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21],\n",
    "  [66, 88, 98, 47],\n",
    "  [77, 65, 51, 77, 19, 15, 35, 19, 23]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6Hx3mcivgMyH"
   },
   "outputs": [],
   "source": [
    "def padding(data):\n",
    "  max_len = len(max(data, key=len))\n",
    "  print(f\"Maximum sequence length: {max_len}\")\n",
    "\n",
    "  for i, seq in enumerate(tqdm(data)):\n",
    "    if len(seq) < max_len:\n",
    "      data[i] = seq + [pad_id] * (max_len - len(seq))\n",
    "\n",
    "  return data, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s3e8FiNvgX60",
    "outputId": "624bb622-8e7a-4353-9d76-ed70dc2259e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 106997.55it/s]\n"
     ]
    }
   ],
   "source": [
    "data, max_len = padding(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hwPSIWYugaN0",
    "outputId": "a31c386e-bd4c-4296-8944-a25820662bef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[62, 13, 47, 39, 78, 33, 56, 13, 0, 0],\n",
       " [60, 96, 51, 32, 90, 0, 0, 0, 0, 0],\n",
       " [35, 45, 48, 65, 91, 99, 92, 10, 3, 21],\n",
       " [66, 88, 98, 47, 0, 0, 0, 0, 0, 0],\n",
       " [77, 65, 51, 77, 19, 15, 35, 19, 23, 0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwqjACx8iidc"
   },
   "source": [
    "### **Hyperparameter 세팅 및 embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "p-Ngp2nWimS8"
   },
   "outputs": [],
   "source": [
    "d_model = 8  # model의 hidden size\n",
    "num_heads = 2  # head의 개수\n",
    "inf = 1e12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GJMi2Xsni5uq"
   },
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "# B: batch size, L: maximum sequence length\n",
    "batch = torch.LongTensor(data)  # (B, L)\n",
    "batch_emb = embedding(batch)  # (B, L, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3tLCUQwojcUb",
    "outputId": "ca253d99-a727-440c-ebb9-468e39bc846b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-8.5668e-01, -5.6692e-01,  1.4254e+00,  1.0340e+00, -5.6951e-01,\n",
      "          -3.2086e-01, -1.6549e+00,  2.1722e-01],\n",
      "         [ 1.2143e+00,  6.8545e-01, -7.5511e-02, -1.8285e+00, -5.8766e-01,\n",
      "          -3.0575e-01, -5.1290e-01, -8.3287e-01],\n",
      "         [ 4.3816e-01, -1.1702e+00,  5.9156e-01, -1.1647e+00,  4.4995e-01,\n",
      "           1.1627e+00, -4.9441e-02, -2.8493e-01],\n",
      "         [-4.9367e-01,  2.1836e-01,  2.3368e+00, -3.6854e-01,  6.8153e-01,\n",
      "           1.1698e-01,  8.5581e-02, -1.4454e+00],\n",
      "         [ 5.8628e-01,  1.2795e+00, -1.1619e+00,  1.0901e+00, -9.1943e-01,\n",
      "           3.2441e-01,  8.9015e-01,  1.1154e-01],\n",
      "         [ 1.0066e+00, -1.8870e+00, -2.2291e+00,  1.7119e+00, -7.0112e-01,\n",
      "          -6.8351e-01,  1.7704e+00, -1.7746e+00],\n",
      "         [-1.3992e+00,  1.0362e+00, -7.9058e-01, -2.3070e-01,  4.4054e-01,\n",
      "           8.3687e-01,  1.0816e+00, -9.4442e-01],\n",
      "         [ 1.2143e+00,  6.8545e-01, -7.5511e-02, -1.8285e+00, -5.8766e-01,\n",
      "          -3.0575e-01, -5.1290e-01, -8.3287e-01],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03]],\n",
      "\n",
      "        [[-6.4000e-01, -8.1289e-01,  5.8705e-01, -8.0971e-02, -7.6337e-02,\n",
      "          -1.5370e+00, -1.3368e-01,  2.4909e-01],\n",
      "         [-1.2542e-01,  2.1067e+00,  2.9581e-01,  2.0146e-01,  1.7841e+00,\n",
      "           4.6702e-01,  1.7564e+00,  1.3776e+00],\n",
      "         [ 8.3020e-01,  2.9746e-01,  1.4095e+00, -2.3850e+00, -1.2913e-01,\n",
      "          -4.5624e-01, -5.8587e-01,  9.7529e-01],\n",
      "         [ 4.2012e-02,  8.6972e-01,  5.8644e-01, -1.1267e+00,  1.8267e+00,\n",
      "          -9.1979e-01,  9.7950e-01,  1.0155e+00],\n",
      "         [ 7.0022e-01, -2.0605e-01, -3.8227e-01,  1.3874e+00, -6.3143e-01,\n",
      "           8.0153e-02, -1.1874e+00,  9.3165e-01],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03]],\n",
      "\n",
      "        [[-3.0368e-01, -4.9860e-01,  1.1507e+00, -1.4831e-01,  3.1657e-01,\n",
      "           1.6311e+00,  5.6068e-01, -2.9067e-01],\n",
      "         [ 1.3559e-01,  1.1251e+00, -1.0321e-01,  2.1018e+00,  2.9730e-01,\n",
      "          -4.2602e-01,  1.6979e+00,  3.1769e-01],\n",
      "         [ 1.4092e+00,  5.7430e-01, -5.1597e-01,  6.1245e-01,  4.9186e-01,\n",
      "          -1.0590e+00,  4.5348e-01, -1.3222e+00],\n",
      "         [ 3.6999e-01,  1.5763e+00,  1.2954e+00,  3.1074e-01, -1.4411e+00,\n",
      "           2.0412e-01, -1.0128e+00,  1.8116e-02],\n",
      "         [-1.0464e+00, -9.1203e-01,  2.1724e+00, -8.7238e-01, -3.5748e-01,\n",
      "           1.4582e+00, -8.6076e-02,  2.0172e+00],\n",
      "         [-5.3566e-01, -1.0242e+00, -2.8131e-01,  6.1909e-03, -2.1635e+00,\n",
      "           3.5258e-01, -1.1305e+00,  8.6449e-01],\n",
      "         [ 8.6939e-01,  2.3633e+00,  1.6549e+00, -1.6864e+00, -1.2287e+00,\n",
      "           9.3694e-01, -1.9049e-01, -2.4219e-01],\n",
      "         [ 2.7114e-01,  1.9239e+00, -1.0204e+00,  5.0334e-01,  4.2448e-01,\n",
      "          -4.2336e-01, -5.8318e-01, -2.0390e+00],\n",
      "         [-1.1946e-01,  1.9310e+00, -3.9795e-01,  3.6377e-01,  3.0593e-01,\n",
      "          -1.0249e+00,  1.0163e+00,  5.4177e-01],\n",
      "         [-1.9270e-01, -9.3582e-01,  1.5246e-01, -2.1858e+00, -7.3653e-01,\n",
      "          -1.3844e+00,  1.2624e+00, -5.7535e-01]],\n",
      "\n",
      "        [[-4.3769e-02,  1.3886e+00, -8.7915e-01,  1.0358e+00, -2.1764e-01,\n",
      "          -2.8116e-01,  2.7359e-01,  5.6646e-01],\n",
      "         [ 1.5190e+00,  2.3673e-01,  8.5125e-01, -2.6435e-02, -1.0885e+00,\n",
      "          -6.6441e-01,  1.3077e+00,  1.1305e+00],\n",
      "         [ 4.5991e-01, -2.8315e+00, -1.0522e+00, -1.1914e+00,  9.5653e-01,\n",
      "           1.2577e+00, -1.1288e+00,  2.5119e+00],\n",
      "         [ 4.3816e-01, -1.1702e+00,  5.9156e-01, -1.1647e+00,  4.4995e-01,\n",
      "           1.1627e+00, -4.9441e-02, -2.8493e-01],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03]],\n",
      "\n",
      "        [[ 1.4622e+00,  1.0418e+00, -7.2836e-01, -8.4209e-01, -2.2144e-01,\n",
      "           5.8917e-01, -8.2640e-01, -1.9216e-01],\n",
      "         [ 3.6999e-01,  1.5763e+00,  1.2954e+00,  3.1074e-01, -1.4411e+00,\n",
      "           2.0412e-01, -1.0128e+00,  1.8116e-02],\n",
      "         [ 8.3020e-01,  2.9746e-01,  1.4095e+00, -2.3850e+00, -1.2913e-01,\n",
      "          -4.5624e-01, -5.8587e-01,  9.7529e-01],\n",
      "         [ 1.4622e+00,  1.0418e+00, -7.2836e-01, -8.4209e-01, -2.2144e-01,\n",
      "           5.8917e-01, -8.2640e-01, -1.9216e-01],\n",
      "         [-4.7657e-01, -7.2773e-01,  5.6690e-01,  8.9533e-01,  3.2724e-01,\n",
      "          -5.7196e-01,  7.7989e-01, -2.3528e+00],\n",
      "         [ 7.1560e-01,  7.7900e-01,  2.0400e-03, -5.6552e-01,  1.5183e+00,\n",
      "          -3.4172e-01, -2.5706e-01,  2.1520e-01],\n",
      "         [-3.0368e-01, -4.9860e-01,  1.1507e+00, -1.4831e-01,  3.1657e-01,\n",
      "           1.6311e+00,  5.6068e-01, -2.9067e-01],\n",
      "         [-4.7657e-01, -7.2773e-01,  5.6690e-01,  8.9533e-01,  3.2724e-01,\n",
      "          -5.7196e-01,  7.7989e-01, -2.3528e+00],\n",
      "         [ 3.3738e-01, -6.0215e-01, -5.2962e-01, -1.3398e+00, -1.7869e+00,\n",
      "           5.3335e-02,  7.9015e-01, -7.3433e-01],\n",
      "         [ 1.2463e+00, -7.3216e-01,  1.2499e-01,  3.9128e-01,  1.7508e+00,\n",
      "          -1.6754e+00, -3.8869e-01,  1.8399e-03]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([5, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "print(batch_emb)\n",
    "print(batch_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dO3gxeyhpyF2"
   },
   "source": [
    "### **Mask 구축**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NDEQF64p5pN"
   },
   "source": [
    "`True`는 attention이 적용될 부분, `False`는 masking될 자리입니다.\n",
    "\n",
    "- Masked Multi-head Attention의 경우, decoding timestep에서 현재 타임스텝 이후의 token들을 참조하지 못하도록 하는 역할을 합니다. 이에 따른 mask는 `nopeak_mask`에서 결정됩니다.\n",
    "- `padding_mask`: batch를 만들기 위해 채워 넣은 `pad_id`는 특별한 의미를 가지는 토큰이 아니므로 참조할 대상이 아님. 따라서 padding 부분을 masking 하는 `padding_mask`를 생성함. 미래의 토큰을 볼 수 없도록 하는 `nopeak_mask`와 결합하여 최종 `mask`를 생성하게 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aB0A4elupM2g",
    "outputId": "69325f99-106f-444f-fd6c-a95412a55ebb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True,  True,  True,  True,  True,  True,  True,  True, False, False]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True, False, False, False, False, False]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True, False, False, False, False, False, False]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True, False]]])\n",
      "torch.Size([5, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "padding_mask = (batch != pad_id).unsqueeze(1)  # (B, 1, L)\n",
    "\n",
    "print(padding_mask)\n",
    "print(padding_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88cD54evrEo6",
    "outputId": "add7f80f-bc1f-4cff-b95d-22972d5a4b55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True, False, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])\n",
      "torch.Size([1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "nopeak_mask = torch.ones([1, max_len, max_len], dtype=torch.bool)  # (1, L, L)\n",
    "nopeak_mask = torch.tril(nopeak_mask)  # (1, L, L)\n",
    "\n",
    "print(nopeak_mask)\n",
    "print(nopeak_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FMzB8_jarycy",
    "outputId": "03c905e2-1dc8-4fa3-b1b6-cc3a9061e0c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True, False, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False]],\n",
      "\n",
      "        [[ True, False, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True, False, False, False, False, False]],\n",
      "\n",
      "        [[ True, False, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True, False, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False]],\n",
      "\n",
      "        [[ True, False, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False]]])\n",
      "torch.Size([5, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "mask = padding_mask & nopeak_mask  # (B, L, L)\n",
    "\n",
    "print(mask)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urXMBRnRgqvw"
   },
   "source": [
    "### **Linear transformation & 여러 head로 나누기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9DWKDqgCgfMk"
   },
   "outputs": [],
   "source": [
    "w_q = nn.Linear(d_model, d_model)\n",
    "w_k = nn.Linear(d_model, d_model)\n",
    "w_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "w_0 = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H-vSL7PwnV6k",
    "outputId": "c0539e6c-2f07-49ec-cb59-227298453d77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 10, 4])\n",
      "torch.Size([5, 2, 10, 4])\n",
      "torch.Size([5, 2, 10, 4])\n"
     ]
    }
   ],
   "source": [
    "q = w_q(batch_emb)  # (B, L, d_model)\n",
    "k = w_k(batch_emb)  # (B, L, d_model)\n",
    "v = w_v(batch_emb)  # (B, L, d_model)\n",
    "\n",
    "batch_size = q.shape[0]\n",
    "d_k = d_model // num_heads\n",
    "\n",
    "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "\n",
    "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWrDA5_Sofad"
   },
   "source": [
    "### **Masking이 적용된 self-attention 구현**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GqaQmVQdvMZB"
   },
   "outputs": [],
   "source": [
    "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "adlRCt6mvMy5"
   },
   "outputs": [],
   "source": [
    "masks = mask.unsqueeze(1)  # (B, 1, L, L)\n",
    "masked_attn_scores = attn_scores.masked_fill_(masks == False, -1 * inf)  # (B, num_heads, L, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "adlRCt6mvMy5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 4.5537e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-2.0454e-01, -2.4596e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-2.7268e-01,  4.1330e-01, -3.1739e-01, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 2.5532e-01,  2.5383e-01, -4.5472e-01,  1.7049e-01, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-3.9722e-01, -6.2343e-01,  1.0968e-01, -7.6718e-01,  3.8968e-01,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 9.4070e-02, -4.0943e-01,  4.8519e-01,  1.4621e-01,  1.7652e-01,\n",
      "           -2.3282e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-6.5960e-01, -1.0805e-01, -3.9850e-01, -8.9610e-01,  2.9317e-01,\n",
      "            1.5287e+00, -4.5745e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-2.0454e-01, -2.4596e-01, -2.9389e-01, -3.7068e-01,  4.7980e-01,\n",
      "            6.0594e-01, -1.0861e-01, -2.4596e-01, -1.0000e+12, -1.0000e+12],\n",
      "          [ 2.5594e-01, -1.6783e-01, -2.2061e-01, -1.3414e-02,  1.6905e-01,\n",
      "           -6.7897e-02, -6.1551e-02, -1.6783e-01, -1.0000e+12, -1.0000e+12],\n",
      "          [ 2.5594e-01, -1.6783e-01, -2.2061e-01, -1.3414e-02,  1.6905e-01,\n",
      "           -6.7897e-02, -6.1551e-02, -1.6783e-01, -1.0000e+12, -1.0000e+12]],\n",
      "\n",
      "         [[-7.3657e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 1.3296e-01,  6.8373e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 1.5652e-01,  1.7152e-01, -1.2518e-01, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 9.0327e-02,  2.2060e-01,  1.3647e-01,  5.6164e-02, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-5.0835e-02, -7.8144e-02,  6.9052e-02, -4.3882e-02, -1.7722e-02,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 2.4124e-01, -1.5400e-01, -5.9489e-01,  8.8296e-02,  1.1061e-01,\n",
      "            8.3034e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-3.7782e-02, -2.5052e-01, -1.2363e-01, -4.7114e-02,  5.5498e-02,\n",
      "            8.0943e-01, -1.5181e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 1.3296e-01,  6.8373e-02, -2.9301e-01,  1.7998e-01, -4.6086e-02,\n",
      "           -4.0813e-01,  9.7884e-02,  6.8373e-02, -1.0000e+12, -1.0000e+12],\n",
      "          [ 1.1178e-01,  5.4261e-01,  5.4642e-01, -3.6925e-02, -3.3165e-01,\n",
      "            1.4591e-02, -7.5603e-01,  5.4261e-01, -1.0000e+12, -1.0000e+12],\n",
      "          [ 1.1178e-01,  5.4261e-01,  5.4642e-01, -3.6925e-02, -3.3165e-01,\n",
      "            1.4591e-02, -7.5603e-01,  5.4261e-01, -1.0000e+12, -1.0000e+12]]],\n",
      "\n",
      "\n",
      "        [[[-5.1362e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-4.4946e-01, -1.5434e+00, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-1.2310e-01, -6.6168e-01, -4.4530e-01, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-3.1791e-01, -1.2380e+00, -1.0414e+00, -1.0629e+00, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-8.9550e-02,  9.9207e-02, -2.0729e-01, -9.3182e-02,  1.8149e-01,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-1.0325e-01, -9.5746e-03, -2.7369e-01, -2.5111e-01,  2.7428e-01,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-1.0325e-01, -9.5746e-03, -2.7369e-01, -2.5111e-01,  2.7428e-01,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-1.0325e-01, -9.5746e-03, -2.7369e-01, -2.5111e-01,  2.7428e-01,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-1.0325e-01, -9.5746e-03, -2.7369e-01, -2.5111e-01,  2.7428e-01,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-1.0325e-01, -9.5746e-03, -2.7369e-01, -2.5111e-01,  2.7428e-01,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12]],\n",
      "\n",
      "         [[ 2.6178e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-1.9446e-01, -1.0423e+00, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 7.4140e-02,  1.7034e-01,  9.5500e-01, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 7.0260e-03, -6.8195e-01,  6.7694e-01, -1.1622e-01, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-1.8816e-01,  5.6531e-02,  5.3462e-01,  1.4670e-02, -7.6291e-03,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 1.7423e-01, -4.8476e-01,  8.0821e-01,  1.0603e-01,  3.1287e-01,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 1.7423e-01, -4.8476e-01,  8.0821e-01,  1.0603e-01,  3.1287e-01,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 1.7423e-01, -4.8476e-01,  8.0821e-01,  1.0603e-01,  3.1287e-01,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 1.7423e-01, -4.8476e-01,  8.0821e-01,  1.0603e-01,  3.1287e-01,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 1.7423e-01, -4.8476e-01,  8.0821e-01,  1.0603e-01,  3.1287e-01,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12]]],\n",
      "\n",
      "\n",
      "        [[[-5.7857e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 2.8718e-02,  1.7024e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 2.7320e-01,  6.8930e-01, -4.6226e-02, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 9.5559e-02,  3.0578e-01, -1.8084e-02,  6.8395e-02, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-9.3761e-01, -9.6407e-01,  4.1281e-01, -4.3606e-01, -1.5153e+00,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 7.6044e-02, -1.6973e-01, -7.4129e-02, -2.4654e-01,  1.1135e-01,\n",
      "            7.3551e-03, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-3.9664e-01,  3.8747e-01,  3.4619e-01, -2.0780e-01, -8.0997e-01,\n",
      "           -7.7613e-02, -5.4939e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 9.7801e-02,  9.0784e-01,  1.1813e-01,  1.2605e-01, -2.1876e-02,\n",
      "            6.2435e-02, -3.2597e-01, -8.6889e-02, -1.0000e+12, -1.0000e+12],\n",
      "          [-2.9394e-01,  3.9455e-01,  2.6637e-01, -6.9846e-01, -8.8508e-01,\n",
      "            4.8554e-02, -1.3263e+00, -2.1602e-01, -3.8494e-01, -1.0000e+12],\n",
      "          [-3.0516e-01,  1.2326e-01,  2.4561e-01, -2.1703e-01, -5.4594e-01,\n",
      "           -1.0772e-01, -2.6565e-01, -4.9471e-03,  2.9764e-02,  1.5204e-01]],\n",
      "\n",
      "         [[-8.8638e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 1.5905e-01, -4.5697e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-2.2353e-01, -7.4170e-03,  4.8718e-01, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 2.2292e-01, -3.3886e-01, -4.1122e-01,  1.5432e-01, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 2.9795e-01, -6.1303e-01, -5.6866e-01,  3.2766e-01,  9.7355e-01,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 1.2730e-01, -4.4100e-02, -5.9384e-01,  4.2293e-01,  6.5857e-01,\n",
      "            3.5141e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-4.9457e-03, -1.2104e-01, -1.0208e-01,  9.6450e-02,  1.2114e-01,\n",
      "           -9.0471e-02,  2.4255e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-8.0953e-02,  1.8798e-01,  2.9616e-01, -2.6172e-01, -4.8488e-01,\n",
      "           -1.3867e-01, -5.5500e-01,  1.5413e-01, -1.0000e+12, -1.0000e+12],\n",
      "          [ 2.0481e-01, -4.8986e-01, -1.4497e-02, -2.1065e-01,  1.4807e-01,\n",
      "            1.0515e-01, -9.3456e-02, -4.2108e-01, -6.7480e-01, -1.0000e+12],\n",
      "          [-4.4386e-01,  1.6420e-01,  3.5907e-01,  1.6758e-01, -4.5315e-01,\n",
      "           -4.5549e-01,  5.6049e-02,  5.6756e-01,  5.5271e-01,  1.0736e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 5.8781e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-3.0029e-02, -7.4353e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-5.3029e-01, -1.3431e-01, -4.8909e-01, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-1.4071e-01,  4.4344e-02, -7.7832e-01, -3.1739e-01, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 1.8818e-01, -8.9725e-02, -3.1529e-01, -2.2061e-01, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 1.8818e-01, -8.9725e-02, -3.1529e-01, -2.2061e-01, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 1.8818e-01, -8.9725e-02, -3.1529e-01, -2.2061e-01, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 1.8818e-01, -8.9725e-02, -3.1529e-01, -2.2061e-01, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 1.8818e-01, -8.9725e-02, -3.1529e-01, -2.2061e-01, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 1.8818e-01, -8.9725e-02, -3.1529e-01, -2.2061e-01, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12]],\n",
      "\n",
      "         [[-3.2319e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-2.6150e-01,  2.1273e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 4.0407e-02,  3.8341e-01,  6.3886e-01, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 1.5332e-01,  1.2339e-01, -6.9380e-02, -1.2518e-01, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-3.6574e-01,  4.3414e-01,  1.1052e+00,  5.4642e-01, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-3.6574e-01,  4.3414e-01,  1.1052e+00,  5.4642e-01, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-3.6574e-01,  4.3414e-01,  1.1052e+00,  5.4642e-01, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-3.6574e-01,  4.3414e-01,  1.1052e+00,  5.4642e-01, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-3.6574e-01,  4.3414e-01,  1.1052e+00,  5.4642e-01, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-3.6574e-01,  4.3414e-01,  1.1052e+00,  5.4642e-01, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12]]],\n",
      "\n",
      "\n",
      "        [[[-9.2774e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-3.1056e-02,  6.8395e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 4.1072e-03, -1.5423e-01, -4.4530e-01, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-9.2774e-02, -2.5421e-01, -5.7226e-01, -9.2774e-02, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 6.1495e-02,  5.1770e-01,  1.7597e-01,  6.1495e-02, -2.6624e-02,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-3.5533e-02, -2.0660e-01, -7.3602e-01, -3.5533e-02,  7.7208e-02,\n",
      "           -3.3127e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 5.3658e-02, -1.2726e-01, -1.8521e-02,  5.3658e-02,  1.2715e-01,\n",
      "           -1.3429e-01, -5.7857e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 6.1495e-02,  5.1770e-01,  1.7597e-01,  6.1495e-02, -2.6624e-02,\n",
      "            1.3204e-01,  1.4469e-01, -2.6624e-02, -1.0000e+12, -1.0000e+12],\n",
      "          [-7.7484e-02, -3.1308e-01, -1.0672e-01, -7.7484e-02, -4.0140e-02,\n",
      "           -6.5380e-02,  1.2696e-02, -4.0140e-02,  2.0114e-01, -1.0000e+12],\n",
      "          [ 2.5012e-02,  2.8661e-01, -2.7369e-01,  2.5012e-02,  9.4596e-03,\n",
      "           -5.3065e-02, -5.6465e-02,  9.4596e-03, -2.7771e-01, -1.0000e+12]],\n",
      "\n",
      "         [[ 2.1861e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 1.2536e-01,  1.5432e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 3.2730e-01,  3.3944e-01,  9.5500e-01, -1.0000e+12, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 2.1861e-02,  7.7778e-02,  8.3827e-02,  2.1861e-02, -1.0000e+12,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-9.0697e-02, -1.7954e-01, -2.1995e-01, -9.0697e-02,  5.0184e-01,\n",
      "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 2.8269e-01, -5.8305e-02,  4.4305e-01,  2.8269e-01, -1.5960e-01,\n",
      "            8.8521e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [ 7.3935e-02, -2.2197e-02,  1.0624e-01,  7.3935e-02,  5.8673e-02,\n",
      "            1.0640e-01, -8.8638e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
      "          [-9.0697e-02, -1.7954e-01, -2.1995e-01, -9.0697e-02,  5.0184e-01,\n",
      "            2.1504e-01, -3.6659e-01,  5.0184e-01, -1.0000e+12, -1.0000e+12],\n",
      "          [-2.2656e-01,  1.7187e-01, -1.2264e-01, -2.2656e-01,  2.5133e-01,\n",
      "            2.7623e-01, -4.2426e-01,  2.5133e-01, -4.7547e-01, -1.0000e+12],\n",
      "          [ 4.3714e-01,  3.1564e-02,  8.0821e-01,  4.3714e-01, -2.5797e-01,\n",
      "            2.8485e-01,  1.3272e-02, -2.5797e-01,  2.5300e-01, -1.0000e+12]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(masked_attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "adlRCt6mvMy5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "print(masked_attn_scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EqMuVJFwHhI"
   },
   "source": [
    "`-1* inf`로 masking된 부분은 softmax 후 0이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "jVNze4elv4Uf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.5104, 0.4896, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.2537, 0.5037, 0.2426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.2934, 0.2929, 0.1442, 0.2695, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1576, 0.1257, 0.2616, 0.1089, 0.3462, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1682, 0.1017, 0.2488, 0.1773, 0.1827, 0.1213, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0569, 0.0989, 0.0739, 0.0450, 0.1477, 0.5080, 0.0697, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0999, 0.0958, 0.0913, 0.0846, 0.1980, 0.2246, 0.1099, 0.0958,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1650, 0.1080, 0.1024, 0.1260, 0.1512, 0.1193, 0.1201, 0.1080,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1650, 0.1080, 0.1024, 0.1260, 0.1512, 0.1193, 0.1201, 0.1080,\n",
      "           0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.5161, 0.4839, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.3611, 0.3665, 0.2724, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.2408, 0.2743, 0.2522, 0.2327, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1945, 0.1893, 0.2193, 0.1959, 0.2011, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1771, 0.1193, 0.0768, 0.1520, 0.1555, 0.3193, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1224, 0.0989, 0.1123, 0.1213, 0.1344, 0.2856, 0.1252, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1437, 0.1347, 0.0938, 0.1506, 0.1201, 0.0836, 0.1387, 0.1347,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1183, 0.1820, 0.1827, 0.1020, 0.0759, 0.1074, 0.0497, 0.1820,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1183, 0.1820, 0.1827, 0.1020, 0.0759, 0.1074, 0.0497, 0.1820,\n",
      "           0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.7491, 0.2509, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.4333, 0.2528, 0.3139, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.4240, 0.1690, 0.2057, 0.2013, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1850, 0.2235, 0.1645, 0.1844, 0.2426, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1900, 0.2087, 0.1602, 0.1639, 0.2772, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1900, 0.2087, 0.1602, 0.1639, 0.2772, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1900, 0.2087, 0.1602, 0.1639, 0.2772, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1900, 0.2087, 0.1602, 0.1639, 0.2772, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1900, 0.2087, 0.1602, 0.1639, 0.2772, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.7001, 0.2999, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.2215, 0.2439, 0.5346, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.2304, 0.1157, 0.4502, 0.2037, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1479, 0.1889, 0.3048, 0.1812, 0.1772, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1823, 0.0943, 0.3437, 0.1703, 0.2094, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1823, 0.0943, 0.3437, 0.1703, 0.2094, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1823, 0.0943, 0.3437, 0.1703, 0.2094, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1823, 0.0943, 0.3437, 0.1703, 0.2094, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1823, 0.0943, 0.3437, 0.1703, 0.2094, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.4647, 0.5353, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.3084, 0.4675, 0.2241, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.2439, 0.3010, 0.2177, 0.2374, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1243, 0.1210, 0.4797, 0.2052, 0.0698, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1874, 0.1466, 0.1613, 0.1357, 0.1941, 0.1749, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1064, 0.2331, 0.2237, 0.1286, 0.0704, 0.1464, 0.0914, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1158, 0.2603, 0.1182, 0.1191, 0.1027, 0.1118, 0.0758, 0.0963,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1029, 0.2048, 0.1801, 0.0686, 0.0570, 0.1449, 0.0366, 0.1112,\n",
      "           0.0939, 0.0000],\n",
      "          [0.0785, 0.1205, 0.1362, 0.0858, 0.0617, 0.0957, 0.0817, 0.1060,\n",
      "           0.1098, 0.1241]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.6493, 0.3507, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.2338, 0.2902, 0.4759, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.3296, 0.1879, 0.1748, 0.3077, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.2076, 0.0835, 0.0873, 0.2138, 0.4079, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1591, 0.1340, 0.0774, 0.2138, 0.2706, 0.1451, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1382, 0.1230, 0.1254, 0.1529, 0.1567, 0.1268, 0.1770, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1235, 0.1615, 0.1800, 0.1030, 0.0824, 0.1165, 0.0768, 0.1562,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1537, 0.0768, 0.1235, 0.1015, 0.1453, 0.1392, 0.1141, 0.0822,\n",
      "           0.0638, 0.0000],\n",
      "          [0.0564, 0.1036, 0.1259, 0.1039, 0.0559, 0.0557, 0.0930, 0.1550,\n",
      "           0.1528, 0.0979]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.5111, 0.4889, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.2835, 0.4212, 0.2954, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.2801, 0.3371, 0.1481, 0.2348, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.3304, 0.2503, 0.1997, 0.2196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.3304, 0.2503, 0.1997, 0.2196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.3304, 0.2503, 0.1997, 0.2196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.3304, 0.2503, 0.1997, 0.2196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.3304, 0.2503, 0.1997, 0.2196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.3304, 0.2503, 0.1997, 0.2196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.3836, 0.6164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.2365, 0.3333, 0.4303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.2835, 0.2751, 0.2269, 0.2146, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0993, 0.2210, 0.4324, 0.2473, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0993, 0.2210, 0.4324, 0.2473, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0993, 0.2210, 0.4324, 0.2473, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0993, 0.2210, 0.4324, 0.2473, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0993, 0.2210, 0.4324, 0.2473, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0993, 0.2210, 0.4324, 0.2473, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.4752, 0.5248, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.4014, 0.3426, 0.2561, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.2882, 0.2452, 0.1784, 0.2882, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1781, 0.2810, 0.1997, 0.1781, 0.1631, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1922, 0.1620, 0.0954, 0.1922, 0.2152, 0.1430, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1613, 0.1346, 0.1500, 0.1613, 0.1736, 0.1336, 0.0857, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1151, 0.1816, 0.1290, 0.1151, 0.1054, 0.1235, 0.1251, 0.1054,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1079, 0.0853, 0.1048, 0.1079, 0.1120, 0.1092, 0.1181, 0.1120,\n",
      "           0.1426, 0.0000],\n",
      "          [0.1163, 0.1511, 0.0863, 0.1163, 0.1145, 0.1076, 0.1072, 0.1145,\n",
      "           0.0860, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.4928, 0.5072, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.2574, 0.2605, 0.4821, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.2426, 0.2566, 0.2581, 0.2426, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1785, 0.1633, 0.1569, 0.1785, 0.3228, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1869, 0.1329, 0.2194, 0.1869, 0.1201, 0.1539, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1469, 0.1334, 0.1517, 0.1469, 0.1446, 0.1517, 0.1248, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1050, 0.0960, 0.0922, 0.1050, 0.1898, 0.1425, 0.0797, 0.1898,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0903, 0.1345, 0.1002, 0.0903, 0.1456, 0.1492, 0.0741, 0.1456,\n",
      "           0.0704, 0.0000],\n",
      "          [0.1341, 0.0894, 0.1943, 0.1341, 0.0669, 0.1151, 0.0877, 0.0669,\n",
      "           0.1115, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([5, 2, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "attn_dists = F.softmax(masked_attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
    "\n",
    "print(attn_dists)\n",
    "print(attn_dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "MBwm34bswV7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 10, 4])\n"
     ]
    }
   ],
   "source": [
    "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
    "\n",
    "print(attn_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2Xab7WKzTEU"
   },
   "source": [
    "### **전체 코드**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "LlF7R6DIzVWc"
   },
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MultiheadAttention, self).__init__()\n",
    "\n",
    "    # Q, K, V learnable matrices\n",
    "    self.w_q = nn.Linear(d_model, d_model)\n",
    "    self.w_k = nn.Linear(d_model, d_model)\n",
    "    self.w_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "    # Linear transformation for concatenated outputs\n",
    "    self.w_0 = nn.Linear(d_model, d_model)\n",
    "\n",
    "  def forward(self, q, k, v, mask=None):\n",
    "    batch_size = q.shape[0]\n",
    "\n",
    "    q = self.w_q(q)  # (B, L, d_model)\n",
    "    k = self.w_k(k)  # (B, L, d_model)\n",
    "    v = self.w_v(v)  # (B, L, d_model)\n",
    "\n",
    "    q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "    k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "    v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "\n",
    "    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "\n",
    "    attn_values = self.self_attention(q, k, v, mask=mask)  # (B, num_heads, L, d_k)\n",
    "    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\n",
    "\n",
    "    return self.w_0(attn_values)\n",
    "\n",
    "  def self_attention(self, q, k, v, mask=None):\n",
    "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
    "\n",
    "    if mask is not None:\n",
    "      mask = mask.unsqueeze(1)  # (B, 1, L, L) or  (B, 1, 1, L)\n",
    "      attn_scores = attn_scores.masked_fill_(mask == False, -1*inf)\n",
    "\n",
    "    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
    "\n",
    "    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
    "\n",
    "    return attn_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "jYLuu_9alQxT"
   },
   "outputs": [],
   "source": [
    "multihead_attn = MultiheadAttention()\n",
    "\n",
    "outputs = multihead_attn(batch_emb, batch_emb, batch_emb, mask=mask)  # (B, L, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "KMiXlYjSlTfB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0950, -0.6099, -0.1194, -0.4724,  0.0516, -0.2494,  0.0768,\n",
      "           0.7056],\n",
      "         [-0.2845, -0.2152, -0.3627, -0.6293,  0.3523,  0.0401, -0.1301,\n",
      "           0.3934],\n",
      "         [-0.2273,  0.0778, -0.3762, -0.5926,  0.3266,  0.2731, -0.2716,\n",
      "           0.1992],\n",
      "         [-0.2018,  0.0960, -0.2749, -0.5768,  0.3603,  0.1116, -0.0936,\n",
      "           0.1510],\n",
      "         [-0.1883,  0.1396, -0.3003, -0.4102,  0.2189, -0.1391,  0.0320,\n",
      "           0.1338],\n",
      "         [-0.0967,  0.2005, -0.2014, -0.3439,  0.1561,  0.0376,  0.0032,\n",
      "           0.1217],\n",
      "         [-0.0721,  0.2410, -0.1946, -0.3351,  0.1565, -0.0019,  0.0417,\n",
      "           0.0974],\n",
      "         [-0.1803,  0.2622, -0.2623, -0.3797,  0.2532,  0.0154, -0.0396,\n",
      "           0.0744],\n",
      "         [-0.1231,  0.2505, -0.2116, -0.3664,  0.2563,  0.1203, -0.0404,\n",
      "           0.0844],\n",
      "         [-0.1231,  0.2505, -0.2116, -0.3664,  0.2563,  0.1203, -0.0404,\n",
      "           0.0844]],\n",
      "\n",
      "        [[-0.3115, -0.2966, -0.1596, -0.7238,  0.2179, -0.3929,  0.1315,\n",
      "           0.5182],\n",
      "         [-0.3234,  0.3558, -0.2530, -0.1399,  0.0900, -0.7948,  0.2680,\n",
      "           0.0024],\n",
      "         [-0.4694,  0.2608, -0.4660, -0.3845,  0.2304, -0.6375,  0.0230,\n",
      "           0.0586],\n",
      "         [-0.4134,  0.2867, -0.3257, -0.3793,  0.3027, -0.5428,  0.1328,\n",
      "           0.0336],\n",
      "         [-0.3717,  0.2372, -0.3325, -0.2576,  0.1927, -0.4963,  0.1007,\n",
      "           0.0739],\n",
      "         [-0.3093,  0.1821, -0.2522, -0.3104,  0.2382, -0.3623,  0.1336,\n",
      "           0.1213],\n",
      "         [-0.3093,  0.1821, -0.2522, -0.3104,  0.2382, -0.3623,  0.1336,\n",
      "           0.1213],\n",
      "         [-0.3093,  0.1821, -0.2522, -0.3104,  0.2382, -0.3623,  0.1336,\n",
      "           0.1213],\n",
      "         [-0.3093,  0.1821, -0.2522, -0.3104,  0.2382, -0.3623,  0.1336,\n",
      "           0.1213],\n",
      "         [-0.3093,  0.1821, -0.2522, -0.3104,  0.2382, -0.3623,  0.1336,\n",
      "           0.1213]],\n",
      "\n",
      "        [[ 0.1701,  0.5404, -0.1119, -0.1518,  0.0058,  0.1500, -0.0443,\n",
      "          -0.0836],\n",
      "         [-0.0063,  0.5407, -0.0897,  0.1105, -0.0266, -0.4291,  0.2873,\n",
      "          -0.1219],\n",
      "         [-0.2022,  0.5260, -0.1734,  0.0259,  0.1300, -0.4413,  0.2717,\n",
      "          -0.1757],\n",
      "         [-0.2856,  0.3882, -0.2577, -0.0630,  0.2254, -0.4822,  0.2492,\n",
      "          -0.0949],\n",
      "         [-0.1627,  0.2727, -0.2293, -0.1613,  0.1548, -0.2837,  0.1263,\n",
      "           0.0469],\n",
      "         [-0.1607,  0.1464, -0.3015, -0.1937,  0.0888, -0.3573,  0.0665,\n",
      "           0.1681],\n",
      "         [-0.3297,  0.1933, -0.3696, -0.2309,  0.2655, -0.4344,  0.1124,\n",
      "           0.0721],\n",
      "         [-0.1821,  0.2277, -0.2456, -0.1508,  0.1917, -0.3718,  0.1982,\n",
      "           0.0701],\n",
      "         [-0.2236,  0.1782, -0.3074, -0.2102,  0.2107, -0.4084,  0.1453,\n",
      "           0.1154],\n",
      "         [-0.2479,  0.1643, -0.3477, -0.3490,  0.2341, -0.3351,  0.0509,\n",
      "           0.1414]],\n",
      "\n",
      "        [[-0.1716,  0.1956, -0.2676,  0.1291, -0.0368, -0.5876,  0.2307,\n",
      "           0.1642],\n",
      "         [-0.3535,  0.2067, -0.3964, -0.1152,  0.1203, -0.6202,  0.1815,\n",
      "           0.1337],\n",
      "         [ 0.0133,  0.2001, -0.2785, -0.1414, -0.1946,  0.2687, -0.1972,\n",
      "           0.2197],\n",
      "         [-0.1028,  0.2497, -0.3458, -0.2671, -0.0547,  0.2932, -0.2891,\n",
      "           0.1585],\n",
      "         [-0.1429,  0.2576, -0.3675, -0.2832, -0.0226,  0.2294, -0.2668,\n",
      "           0.1469],\n",
      "         [-0.1429,  0.2576, -0.3675, -0.2832, -0.0226,  0.2294, -0.2668,\n",
      "           0.1469],\n",
      "         [-0.1429,  0.2576, -0.3675, -0.2832, -0.0226,  0.2294, -0.2668,\n",
      "           0.1469],\n",
      "         [-0.1429,  0.2576, -0.3675, -0.2832, -0.0226,  0.2294, -0.2668,\n",
      "           0.1469],\n",
      "         [-0.1429,  0.2576, -0.3675, -0.2832, -0.0226,  0.2294, -0.2668,\n",
      "           0.1469],\n",
      "         [-0.1429,  0.2576, -0.3675, -0.2832, -0.0226,  0.2294, -0.2668,\n",
      "           0.1469]],\n",
      "\n",
      "        [[-0.3405,  0.3123, -0.5903, -0.2510,  0.3220,  0.3570, -0.3802,\n",
      "          -0.0168],\n",
      "         [-0.3123,  0.0429, -0.5015, -0.2608,  0.3336, -0.0554, -0.1395,\n",
      "           0.1879],\n",
      "         [-0.4062,  0.0708, -0.5270, -0.4607,  0.4296,  0.0149, -0.2290,\n",
      "           0.1886],\n",
      "         [-0.3851,  0.0932, -0.5508, -0.3921,  0.3871,  0.0247, -0.2270,\n",
      "           0.1534],\n",
      "         [-0.2774,  0.1315, -0.3381, -0.4384,  0.3742, -0.0411, -0.0058,\n",
      "           0.1094],\n",
      "         [-0.3396,  0.2082, -0.3874, -0.3715,  0.3736, -0.0271, -0.0752,\n",
      "           0.0473],\n",
      "         [-0.2844,  0.2295, -0.3717, -0.3509,  0.3320,  0.0108, -0.0919,\n",
      "           0.0484],\n",
      "         [-0.2049,  0.2617, -0.2385, -0.3796,  0.3205, -0.0469,  0.0551,\n",
      "           0.0127],\n",
      "         [-0.2278,  0.2497, -0.3214, -0.3964,  0.3020,  0.0278, -0.0806,\n",
      "           0.0516],\n",
      "         [-0.2142,  0.2376, -0.2957, -0.4110,  0.3136,  0.0133, -0.0573,\n",
      "           0.0672]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "KMiXlYjSlTfB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1g99JEEFwFv"
   },
   "source": [
    "### **Encoder-Decoder attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2PRoF4fF4ah"
   },
   "source": [
    "Query, key, value만 달라질 뿐 구현은 동일합니다.  \n",
    "Decoder에 들어갈 batch만 별도 구현하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "p26ra2BsGEdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 98922.26it/s]\n"
     ]
    }
   ],
   "source": [
    "trg_data = [\n",
    "  [33, 11, 49, 10],\n",
    "  [88, 34, 5, 29, 99, 45, 11, 25],\n",
    "  [67, 25, 15, 90, 54, 4, 92, 10, 46, 20, 88 ,19],\n",
    "  [16, 58, 91, 47, 12, 5, 8],\n",
    "  [71, 63, 62, 7, 9, 11, 55, 91, 32, 48]\n",
    "]\n",
    "\n",
    "trg_data, trg_max_len = padding(trg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "yYysB4EKHKGI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n",
      "torch.Size([5, 12])\n"
     ]
    }
   ],
   "source": [
    "# S_L: source maximum sequence length, T_L: target maximum sequence length\n",
    "src_batch = batch  # (B, S_L)\n",
    "trg_batch = torch.LongTensor(trg_data)  # (B, T_L)\n",
    "\n",
    "print(src_batch.shape)\n",
    "print(trg_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "AieDxWYIHXKc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 8])\n",
      "torch.Size([5, 12, 8])\n"
     ]
    }
   ],
   "source": [
    "src_emb = embedding(src_batch)  # (B, S_L, d_w)\n",
    "trg_emb = embedding(trg_batch)  # (B, T_L, d_w)\n",
    "\n",
    "print(src_emb.shape)\n",
    "print(trg_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxCjmPurH2b7"
   },
   "source": [
    "`src_emb`를 encoder에서 나온 결과, 그리고 `trg_emb`를 masked multi-head attention 후 결과로 가정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "AUhY-z8JHeUE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 12, 4])\n",
      "torch.Size([5, 2, 10, 4])\n",
      "torch.Size([5, 2, 10, 4])\n"
     ]
    }
   ],
   "source": [
    "q = w_q(trg_emb)  # (B, T_L, d_model)\n",
    "k = w_k(src_emb)  # (B, S_L, d_model)\n",
    "v = w_v(src_emb)  # (B, S_L, d_model)\n",
    "\n",
    "batch_size = q.shape[0]\n",
    "d_k = d_model // num_heads\n",
    "\n",
    "q = q.view(batch_size, -1, num_heads, d_k)  # (B, T_L, num_heads, d_k)\n",
    "k = k.view(batch_size, -1, num_heads, d_k)  # (B, S_L, num_heads, d_k)\n",
    "v = v.view(batch_size, -1, num_heads, d_k)  # (B, S_L, num_heads, d_k)\n",
    "\n",
    "q = q.transpose(1, 2)  # (B, num_heads, T_L, d_k)\n",
    "k = k.transpose(1, 2)  # (B, num_heads, S_L, d_k)\n",
    "v = v.transpose(1, 2)  # (B, num_heads, S_L, d_k)\n",
    "\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "XeqjkVqkIdxO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 12, 10])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, T_L, S_L)\n",
    "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, T_L, S_L)\n",
    "\n",
    "print(attn_dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "VQv4IINbItS0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 12, 4])\n"
     ]
    }
   ],
   "source": [
    "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, T_L, d_k)\n",
    "\n",
    "print(attn_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLCHeCbtJDy9"
   },
   "source": [
    "Masked multi-head attention 후 나온 결과와 동일한 shape를 가지며 이후 layer에서 전체 연산도 동일하게 진행됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAHCJ2xASvlB"
   },
   "source": [
    "###**콘텐츠 라이선스**\n",
    "\n",
    "<font color='red'><b>**WARNING**</b></font> : **본 교육 콘텐츠의 지식재산권은 재단법인 네이버커넥트에 귀속됩니다. 본 콘텐츠를 어떠한 경로로든 외부로 유출 및 수정하는 행위를 엄격히 금합니다.** 다만, 비영리적 교육 및 연구활동에 한정되어 사용할 수 있으나 재단의 허락을 받아야 합니다. 이를 위반하는 경우, 관련 법률에 따라 책임을 질 수 있습니다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "ml2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "aff83ea1928dcc0287770e0ea68916ae9727a33d95a26ca69018331bcc2781f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
