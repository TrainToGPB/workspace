{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOnz8OxdbN3y"
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## ê¸°ë³¸ê³¼ì œ 3: Subword-level Language Model\n",
    "\n",
    "> Reference ì½”ë“œëŠ” Solution ê³¼ í•¨ê»˜ ê³µê°œë©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSNON1TAbj2i"
   },
   "source": [
    "### Introduction\n",
    "\n",
    "\n",
    "* ë³¸ ê³¼ì œì˜ ëª©ì ì€ ì„œë¸Œì›Œë“œ í† í°í™” (Subword Tokenization)ì˜ í•„ìš”ì„±ì„ ì§ì ‘ ëŠë¼ê³  ì„œë¸Œì›Œë“œ í† í°í™” ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜ì¸ Byte Pair Encodingì„ êµ¬í˜„í•´ë´…ë‹ˆë‹¤.\n",
    "* ì„œë¸Œì›Œë“œ í† í°í™” ê¸°ë°˜ language modelì„ êµ¬í˜„í•˜ë©´ì„œ ì´ì „ ê³¼ì œì˜ Word-level language modelê³¼ ë¹„êµí•´ë³´ëŠ” ì‹œê°„ì„ ê°–ê² ìŠµë‹ˆë‹¤. ì¶”ê°€ì ìœ¼ë¡œ RNNì„ LSTMìœ¼ë¡œ ë³€ê²½í–ˆì„ ë•Œì˜ ì„±ëŠ¥ ì°¨ì´ì— ëŒ€í•´ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "* Subword-level language modelì„ êµ¬í˜„í•˜ê³ , ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ê°€ê³µí•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•œ í›„ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì„ ì´ìš©í•´ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "* **ANSWER HERE** ì´ë¼ê³  ì‘ì„±ëœ ë¶€ë¶„ì„ ì±„ì›Œ ì™„ì„±í•˜ì‹œë©´ ë©ë‹ˆë‹¤. ë‹¤ë¥¸ ë¶€ë¶„ì˜ ì½”ë“œë¥¼ ë³€ê²½í•˜ë©´ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "> ê³¼ì œ ì™„ì„± í›„ ipynb íŒŒì¼ì„ ì œì¶œí•´ ì£¼ì„¸ìš”.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70_V429wBxti"
   },
   "source": [
    "### 0. ë°ì´í„° ì—…ë¡œë“œ\n",
    "\n",
    "\n",
    "1. Boostcourse [ê¸°ë³¸ ê³¼ì œ] Subword-level Language Model ì—ì„œ `wikitext-2.zip` íŒŒì¼ì„ ë‹¤ìš´ë°›ìŠµë‹ˆë‹¤.\n",
    "2. ë³¸ Colab í™˜ê²½ì— `train.txt`, `dev.txt`, `test.txt` íŒŒì¼ì„ ì—…ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "3. `! ls` command ë¥¼ ì‹¤í–‰í–ˆì„ ë•Œ, `sample_data  test.txt  train.txt  valid.txt` ê°€ ë‚˜ì˜¤ë©´ ì„±ê³µì ìœ¼ë¡œ ë°ì´í„° ì¤€ë¹„ê°€ ì™„ë£Œëœ ê²ƒ ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ufq1RSMJo41s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'(10ê°•-ì‹¤ìŠµ) HuggingFace_s Transformers 2.ipynb'\r\n",
      "'(1ê°•-ì‹¤ìŠµ-1) Naive Bayes classifier êµ¬í˜„.ipynb'\r\n",
      "'(1ê°•-ì‹¤ìŠµ-2) Corpus Cleaning.ipynb'\r\n",
      "'(2ê°•-ì‹¤ìŠµ-1) Word2Vec êµ¬í˜„ ë° Embedding ì‹œê°í™”.ipynb'\r\n",
      "'(2ê°•-ì‹¤ìŠµ-2) ë‹¤êµ­ì–´ ì„ë² ë”©.ipynb'\r\n",
      "'(3ê°•-ì‹¤ìŠµ) Basic RNN ì‹¤ìŠµ.ipynb'\r\n",
      "'(4ê°•-ì‹¤ìŠµ) LSTM, GRU ì‹¤ìŠµ.ipynb'\r\n",
      "'(5ê°•-ì‹¤ìŠµ) Seq2Seq êµ¬í˜„.ipynb'\r\n",
      "'(6ê°•-ì‹¤ìŠµ) Seq2Seq with Attention êµ¬í˜„.ipynb'\r\n",
      "'(7ê°•-ì‹¤ìŠµ) Multi head Attention êµ¬í˜„.ipynb'\r\n",
      "'(8ê°•-ì‹¤ìŠµ) Masked Multi-head Attention êµ¬í˜„.ipynb'\r\n",
      "'(9ê°•-ì‹¤ìŠµ) HuggingFace_s Transformers 1.ipynb'\r\n",
      "'(ê¸°ë³¸-1) Data Preprocessing_Tokenization (ë¬¸ì œ).ipynb'\r\n",
      "'(ê¸°ë³¸-2) RNN-based Language Model (ë¬¸ì œ).ipynb'\r\n",
      "'(ê¸°ë³¸-3) Subword-level Language Model (ë¬¸ì œ).ipynb'\r\n",
      "'(ê¸°ë³¸-4) Preprocessing for NMT Model (ë¬¸ì œ).ipynb'\r\n",
      "'(ì‹¬í™”-1) BERT Fine-tuning with Transformers (ë¬¸ì œ).ipynb'\r\n",
      "'(ì‹¬í™”-2) NMT training with Fairseq (ë¬¸ì œ).ipynb'\r\n",
      " corpus.txt\r\n",
      " generate.txt\r\n",
      " generate_30epoch.txt\r\n",
      " generate_5epoch.txt\r\n",
      " model.pt\r\n",
      " model_30epoch.pt\r\n",
      " model_5epoch.pt\r\n",
      " test.txt\r\n",
      " train.txt\r\n",
      " valid.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AFwHrOCK6UaE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36718\n",
      "0\n",
      " \n",
      "\n",
      "1\n",
      " = Valkyria Chronicles III = \n",
      "\n",
      "2\n",
      " \n",
      "\n",
      "3\n",
      " SenjÅ no Valkyria 3 : <unk> Chronicles ( Japanese : æˆ¦å ´ã®ãƒ´ã‚¡ãƒ«ã‚­ãƒ¥ãƒªã‚¢3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \n",
      "\n",
      "4\n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more <unk> for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \n",
      "\n",
      "5\n",
      " It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \n",
      "\n",
      "6\n",
      " \n",
      "\n",
      "7\n",
      " = = Gameplay = = \n",
      "\n",
      "8\n",
      " \n",
      "\n",
      "9\n",
      " As with previous <unk> Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through <unk> text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely <unk> through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main <unk> , although they take a very minor role . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_train = './train.txt'\n",
    "with open(path_train, 'r', encoding=\"utf8\") as f:\n",
    "    corpus_train = f.readlines()    \n",
    "\n",
    "# train dataset í¬ê¸° í™•ì¸\n",
    "print(len(corpus_train))\n",
    "\n",
    "# ì²˜ìŒ 10 ë¬¸ì¥ì„ print í•´ ë´…ì‹œë‹¤.\n",
    "for sent_num, sent in enumerate(corpus_train[:10]):\n",
    "    print(sent_num)\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xzSMF9RJyzg"
   },
   "source": [
    "### 1. ì„œë¸Œì›Œë“œ í† í°í™”ì˜ í•„ìš”ì„±\n",
    "\n",
    "ğŸ’¡ ì„œë¸Œì›Œë“œ(Subword)ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\n",
    "\n",
    "ì„œë¸Œì›Œë“œëŠ” í•˜ë‚˜ì˜ ë‹¨ì–´ë¥¼ ì—¬ëŸ¬ê°œì˜ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í–ˆì„ ë•Œ í•˜ë‚˜ì˜ ë‹¨ìœ„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. `subword`ë¥¼ ì„œë¸Œì›Œë“œ ë‹¨ìœ„ë¡œ ë‚˜íƒ€ë‚¸ í•˜ë‚˜ì˜ ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    " * `sub` + `word`\n",
    "\n",
    "`sub`ë¼ëŠ” ì ‘ë‘ì‚¬ì™€ `word`ë¼ê³  í•˜ëŠ” ì–´ê·¼ìœ¼ë¡œ ë‚˜ëˆ„ì–´ `subword`ë¼ê³  í•˜ëŠ” ë‹¨ì–´ë¥¼ 2ê°œì˜ ì„œë¸Œ ì›Œë“œë¡œ ë‚˜íƒ€ëƒˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ì™¸ì—ë„ ë‹¤ì–‘í•œ í˜•íƒœì˜ ì„œë¸Œì›Œë“œë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (e.g., `su` + `bword`, `s` + `ubword`, `subwor` + `d`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVQgjQJdAbWh"
   },
   "source": [
    "ğŸ’¡ ê·¸ëŸ¼ ì„œë¸Œì›Œë“œ í† í°í™”(Subword tokenization)ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\n",
    "\n",
    "ì„œë¸Œì›Œë“œ í† í°í™”ëŠ” ë§ ê·¸ëŒ€ë¡œ ì„œë¸Œì›Œë“œ ë‹¨ìœ„ë¡œ í† í°í™”ë¥¼ í•œë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.\n",
    "ê¸°ë³¸ ê³¼ì œ 1ì—ì„œ ë‚˜ì˜¨ ë‹¨ì–´ë‹¨ìœ„ í† í°í™”ë¥¼ ì ìš©í•œ ë’¤, ì„œë¸Œì›Œë“œ í† í°í™”ë¥¼ ìˆ˜í–‰í•œ ì˜ˆì‹œë¥¼ ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì„œë¸Œì›Œë“œ í† í°í™”ë¥¼ ì ìš©í–ˆì„ ë•ŒëŠ” ë‹¤ìŒê³¼ ê°™ì´ í† í°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "* Example 1\n",
    "> \"I have a meal\" -> ['I', 'hav', 'e', 'a', 'me', 'al']\n",
    ">\n",
    "> \"ë‚˜ëŠ” ë°¥ì„ ë¨¹ëŠ”ë‹¤\" -> ['ë‚˜', 'ëŠ”', 'ë°¥', 'ì„', 'ë¨¹ëŠ”', 'ë‹¤']\n",
    "\n",
    "ë‹¨ì–´ë‹¨ìœ„ê°€ ì•„ë‹ˆë¼ ê·¸ë³´ë‹¤ ë” ì˜ê²Œ ìª¼ê°  ì„œë¸Œì›Œë“œ ë‹¨ìœ„ë¡œ ë¬¸ì¥ì„ í† í°í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "ìœ„ì—ì„œ ë§ì”€ë“œë¦° ê²ƒê³¼ ê°™ì´ ì—¬ëŸ¬ê°€ì§€ ê²½ìš°ì˜ ìˆ˜ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "* Example 2\n",
    "> \"I have a meal\" -> ['I', 'ha', 've', 'a', 'mea', 'l']\n",
    ">\n",
    "> \"ë‚˜ëŠ” ë°¥ì„ ë¨¹ëŠ”ë‹¤\" -> ['ë‚˜', 'ëŠ”', 'ë°¥', 'ì„', 'ë¨¹', 'ëŠ”ë‹¤']\n",
    "\n",
    "ê·¸ë ‡ì§€ë§Œ ê¸°ë³¸ì ìœ¼ë¡œ ê³µë°±ì„ ë„˜ì–´ì„  ì„œë¸Œë¥¼ êµ¬ì„±í•˜ì§„ ì•ŠìŠµë‹ˆë‹¤.\n",
    "ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒê³¼ ê°™ì´ í† í°í™”ë¥¼ ìˆ˜í–‰í•˜ì§„ ì•ŠìŠµë‹ˆë‹¤.\n",
    "\n",
    "* Example 3\n",
    "> \"I have a meal\" -> ['Iha', 've', 'am', 'ea', 'l']\n",
    ">\n",
    "> \"ë‚˜ëŠ” ë°¥ì„ ë¨¹ëŠ”ë‹¤\" -> ['ë‚˜ëŠ”ë°¥', 'ì„ë¨¹', 'ëŠ”ë‹¤']\n",
    "\n",
    "(ì°¸ê³ 4: [Huggingface: subword-tokenization](https://huggingface.co/transformers/tokenizer_summary.html#subword-tokenization))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPRNaFhMEK67"
   },
   "source": [
    "ğŸ’¡ Subword tokenizationì€ ì™œ í•„ìš”í•œê°€ìš”?\n",
    "\n",
    "ì²« ë²ˆì§¸ ì´ìœ ëŠ” ì´ ì„¸ìƒì— ë‹¨ì–´ê°€ ë„ˆë¬´ ë§ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
    "ì´ì „ ê³¼ì œì—ì„œ ì‚¬ìš©í–ˆë˜ ì½”ë“œë¥¼ ë¶ˆëŸ¬ì™€ ê·¸ í•„ìš”ì„±ì„ ìƒê°í•´ ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SWd09aUBGWa9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.token2id = {}\n",
    "        self.id2token = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.token2id:\n",
    "            self.id2token.append(word)\n",
    "            self.token2id[word] = len(self.id2token) - 1\n",
    "        return self.token2id[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.id2token)\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "            idss = []\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                ids = []\n",
    "                for word in words:\n",
    "                    ids.append(self.dictionary.token2id[word])\n",
    "                idss.append(torch.tensor(ids).type(torch.int64))\n",
    "            ids = torch.cat(idss)\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FLT1GR2To41u"
   },
   "outputs": [],
   "source": [
    "from typing import Union, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, \n",
    "        rnn_type: str,\n",
    "        vocab_size: int,\n",
    "        embedding_size: int=200,\n",
    "        hidden_size: int=200,\n",
    "        num_hidden_layers: int=2,\n",
    "        dropout: float=0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.rnn_type = rnn_type\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layer = num_hidden_layers\n",
    "        assert rnn_type in {'LSTM', 'GRU', 'RNN_TANH', 'RNN_RELU'}\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        if rnn_type.startswith('RNN'):\n",
    "            nonlinearity = rnn_type.split('_')[-1].lower()\n",
    "            self.rnn = nn.RNN(\n",
    "                embedding_size, \n",
    "                hidden_size, \n",
    "                num_hidden_layers,\n",
    "                batch_first=True, \n",
    "                nonlinearity=nonlinearity,\n",
    "                dropout=dropout\n",
    "            )\n",
    "        else:\n",
    "            self.rnn = getattr(nn, rnn_type)(\n",
    "                embedding_size,\n",
    "                hidden_size,\n",
    "                num_hidden_layers,\n",
    "                batch_first=True,\n",
    "                dropout=dropout\n",
    "            )\n",
    "\n",
    "        self.projection = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input: torch.Tensor,\n",
    "        prev_hidden: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]\n",
    "    ):\n",
    "        ### YOUR CODE HERE (ê³¼ì œ 2ì˜ ì½”ë“œë¥¼ ë³µì‚¬í•´ì„œ ë„£ìœ¼ì„¸ìš”.)\n",
    "        ### ANSWER HERE ###\n",
    "            \n",
    "        embeddings = self.embedding(input) # (batch_size, sequence_length, embedding_size)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        output, next_hidden = self.rnn(embeddings, prev_hidden) # (batch_size, sequence_length, hidden_size)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        projections = self.projection(output) # (batch_size, sequence_length, vocab_size)\n",
    "        log_prob = F.log_softmax(projections, dim=-1) # (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        assert list(log_prob.shape) == list(input.shape) + [self.vocab_size]\n",
    "        assert prev_hidden.shape == next_hidden.shape if self.rnn_type != 'LSTM' \\\n",
    "          else prev_hidden[0].shape == next_hidden[0].shape == next_hidden[1].shape\n",
    "        \n",
    "        return log_prob, next_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size: int):\n",
    "        weight = self.projection.weight\n",
    "        \n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new_zeros(self.num_hidden_layer, batch_size, self.hidden_size),\n",
    "                    weight.new_zeros(self.num_hidden_layer, batch_size, self.hidden_size))\n",
    "        else:\n",
    "            return weight.new_zeros(self.num_hidden_layer, batch_size, self.hidden_size)\n",
    "    \n",
    "    @property\n",
    "    def device(self):   # í˜„ì¬ ëª¨ë¸ì˜ deviceë¥¼ ë°˜í™˜í•˜ëŠ” í”„ë¡œí¼í‹°\n",
    "        return self.projection.weight.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnsZkQqUHb6Z"
   },
   "source": [
    "ë§ë­‰ì¹˜ì˜ ë¬¸ì¥ë“¤ì„ ë‹¨ì–´ë‹¨ìœ„ í† í°í™”ë¥¼ í•´ë³´ê³  ë‹¨ì–´ë“¤ì˜ ê°œìˆ˜ë¥¼ ì„¸ì–´ë³´ê² ìŠµë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2aSB9Hk4HjyO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33278\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus('./')\n",
    "vocab_size = len(corpus.dictionary)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBkvRpEcKEvd"
   },
   "source": [
    "ì´ì „ ê³¼ì œì— ì‚¬ìš©ëœ ì„ë² ë”©ì˜ í¬ê¸°ëŠ” 200ì´ë¯€ë¡œ ë‹¨ì–´ ì„ë² ë”©ì— ì‚¬ìš©ëœ ë§¤ê°œë³€ìˆ˜ì˜ ìˆ˜ëŠ” 33278 x 200 (6,655,600ê°œ)ì…ë‹ˆë‹¤.\n",
    "ê·¸ë ‡ë‹¤ë©´, RNN ëª¨ë¸ì— ì‚¬ìš©ë˜ëŠ” weightì˜ parameter ê°œìˆ˜ëŠ” ëª‡ê°œì¸ì§€ ê°„ë‹¨í•œ í•¨ìˆ˜ë¥¼ ì´ìš©í•´ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rGjG2j-fKbJu"
   },
   "outputs": [],
   "source": [
    "model = RNNModel('RNN_TANH', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yTiKccVXLrvx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embedding parameter ê°œìˆ˜: 6655600\n",
      "RNN parameter ê°œìˆ˜: 160800\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Word embedding parameter ê°œìˆ˜: {count_parameters(model.embedding)}\")\n",
    "print(f\"RNN parameter ê°œìˆ˜: {count_parameters(model.rnn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlUFvgTNM1Od"
   },
   "source": [
    "ğŸ’¡ RNN ì¸µì˜ ë§¤ê°œë³€ìˆ˜ ê°œìˆ˜ì™€, ì„ë² ë”© ë§¤ê°œë³€ìˆ˜ ê°œìˆ˜ë¥¼ ë¹„êµí•´ë³´ë©´ ì„ë² ë”© ë§¤ê°œë³€ìˆ˜ì˜ ê°œìˆ˜ê°€ RNNì¸µì˜ ë§¤ê°œë³€ìˆ˜ ìˆ˜ë³´ë‹¤ ì••ë„ì ìœ¼ë¡œ ë§ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë‹¨ì–´ë‹¨ìœ„ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” ë§ë­‰ì¹˜ì˜ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ê°€ ë”ë”ìš± ë§ì•„ì ¸ ì„ë² ë”©ì˜ ë§¤ê°œë³€ìˆ˜ëŠ” ë” ì»¤ì§€ê²Œ ë˜ê³  ì „ì²´ ë§¤ê°œë³€ìˆ˜ ëŒ€ë¹„ ë‹¨ì–´ ì„ë² ë”©ì´ ì°¨ì§€í•˜ëŠ” ë¹„ì¤‘ì€ ë§¤ìš° ë†’ì•„ì§‘ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7WfyYBrPpca"
   },
   "source": [
    "âœ¨ ì´ëŸ° ë§¤ê°œë³€ìˆ˜ ë¹„ì¤‘ì˜ ë¹„ëŒ€ì¹­ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì²˜ìŒì—ëŠ” ë¬¸ìë‹¨ìœ„ í† í°í™”(character-level tokenization) ë°©ë²•ì´ ì£¼ëª©ì„ ë°›ì•˜ìŠµë‹ˆë‹¤. \n",
    "ë§ ê·¸ëŒ€ë¡œ í•˜ë‚˜ì˜ ê¸€ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ í† í°í™”ì„ í•˜ëŠ”ê±´ë°ìš”.\n",
    "ì´ì „ ì˜ˆì‹œë¥¼ ë¬¸ìë‹¨ìœ„ í† í°í™”ë¥¼ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "\"I have a meal\" -> ['I', 'h', 'a', 'v', 'e', 'a', 'm', 'e', 'a', 'l']\n",
    "\"ë‚˜ëŠ” ë°¥ì„ ë¨¹ëŠ”ë‹¤\" -> ['ë‚˜', 'ëŠ”', 'ë°¥', 'ì„', 'ë¨¹', 'ëŠ”', 'ë‹¤']\n",
    "\n",
    "ê·¸ëŸ¬ë‚˜, ë¬¸ìë‹¨ìœ„ í† í°í™” ì—­ì‹œ ì§€ë‚˜ì¹˜ê²Œ ê¸´ Sequence ê¸¸ì´, ì„±ëŠ¥ ì €í•˜ ë“±ì˜ ë¬¸ì œë¥¼ ê²ªìœ¼ë©° ì„œë¸Œì›Œë“œ í† í°í™”ê°€ ê°ê´‘ì„ ë°›ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBI03OI1o41w"
   },
   "source": [
    "ğŸ’¡ì„œë¸Œì›Œë“œ í† í°í™”ê°€ ê°€ì§€ëŠ” ë‘ë²ˆì§¸ ì¥ì ì€ Out-of-Vocabulary (OoV) ë¬¸ì œê°€ ì—†ë‹¤ëŠ” ì ì…ë‹ˆë‹¤.\n",
    "\n",
    "í•™ìŠµ ë°ì´í„°ì—ì„œ ë“±ì¥í•˜ì§€ ì•Šì€ ë‹¨ì–´ëŠ” ëª¨ë‘ Unknown í† í° [UNK]ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤. ì´ëŠ” í…ŒìŠ¤íŠ¸ ê³¼ì • ì¤‘ì— ì²˜ìŒ ë³´ëŠ” ë‹¨ì–´ë¥¼ ëª¨ë‘ [UNK]ë¡œ ëª¨ë¸ì˜ ì…ë ¥ì„ ë„£ê²Œ ë˜ë©´ì„œ ì „ì²´ì ìœ¼ë¡œ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ê·¸ëŸ¬ë‚˜ ì„œë¸Œì›Œë“œ ë‹¨ìœ„ë¡œ ìë¥´ê²Œ ëœë‹¤ë©´ ìµœì•…ì˜ ê²½ìš°ì—ë„ ë¬¸ìë‹¨ìœ„ë¡œ í† í°í™”ê°€ ì§„í–‰ë©ë‹ˆë‹¤. ì´ëŠ” ì„œë¸Œì›Œë“œ í† í°í™”ëŠ” í˜„ì¬ ê°€ì§€ê³  ìˆëŠ” Vocabìœ¼ë¡œ í•´ë‹¹ ë‹¨ì–´ê°€ í† í°í™”í•  ìˆ˜ ì—†ë‹¤ë©´ ê·¸ ë‹¨ì–´ë¥¼ ì„œë¸Œì›Œë“œ ë‹¨ìœ„ë¡œ ìª¼ê°œ í‰ê°€í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "ë”°ë¼ì„œ ì„œë¸Œì›Œë“œ í† í°í™”ê¸°ëŠ” ê°€ì¥ ì‘ì€ ë¬¸ì ë‹¨ìœ„ë¡œ ì„œë¸Œì›Œë“œ í† í°í™”ê°€ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— OoV ë¬¸ì œê°€ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVbjVR3Io41w"
   },
   "source": [
    "### 2. Byte Pair Encoding (BPE)\n",
    "> ì´ ì„¹í„°ì—ì„œëŠ” íŒŒì´ì¬ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ (Python Standard Library)ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.\n",
    "\n",
    "ëŒ€í‘œì ì¸ ì„œë¸Œì›Œë“œ í† í°í™” ë°©ë²•ì¸ Byte pair encodingì„ êµ¬í˜„í•´ë´…ì‹œë‹¤. BPEì˜ ì •í™•í•œ ì•Œê³ ë¦¬ì¦˜ì€ [ë…¼ë¬¸](https://arxiv.org/pdf/1508.07909.pdf)ì˜ 3í˜ì´ì§€ algorithm 1ì— ì œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê° ë¬¸í•­ê³¼ ì£¼ì„ì˜ ì§€ì‹œì‚¬í•­ì„ í™•ì¸í•˜ê³  BPEë¥¼ êµ¬í˜„í•´ë³´ì„¸ìš”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K73a8sreo41x"
   },
   "source": [
    "### 2-A) BPE Vocab ë§Œë“¤ê¸°\n",
    "\n",
    "BPEì˜ Vocabì„ ë§Œë“œëŠ” ê²ƒì€ ê°„ë‹¨í•©ë‹ˆë‹¤. ë‹¨ìˆœíˆ ê°€ì¥ ë§ì´ ë“±ì¥í•˜ëŠ” ì—°ì†í•œ ì§ì„ ì°¾ì•„ ì¶”ê°€í•˜ëŠ” ê²ƒ ì…ë‹ˆë‹¤.\n",
    "ë‹¤ìŒê³¼ ê°™ì€ ë§ë­‰ì¹˜ê°€ ìˆë‹¤ê³  ê°€ì •í•´ ë´…ì‹œë‹¤.\n",
    "\n",
    "```\n",
    "low lower lowest newest\n",
    "```\n",
    "\n",
    "ìš°ì„ ì€ ê³µë°±ì„ ì œì™¸í•œ ëª¨ë“  ë¬¸ìë¥¼ Vocabì— ì¶”ê°€í•˜ê³  ê° ë‹¨ì–´ì˜ ëì— WORD_END \"`_`\" ë¶™ì—¬ ë‹¨ì–´ë¥¼ êµ¬ë¶„ì§€ì–´ ë´…ì‹œë‹¤.\n",
    "\n",
    "```\n",
    "Vocab: d e i l n o r s t w _\n",
    "[ l o w _ ], [ l o w e r _ ], [ l o w e s t _ ], [ w i d e s t _ ]  \n",
    "```\n",
    "\n",
    "ì´ë•Œ ê°€ì¥ ë§ì´ ë“±ì¥í•œ ì—°ì†í•œ ë‘ í† í°ì„ ì°¾ì•„ Vocabì— ì¶”ê°€í•˜ê³  ë‘ í† í°ì„ ë¶™ì…ë‹ˆë‹¤. ì´ ê²½ìš°ì—ëŠ” `l o`ê°€ ì„¸ë²ˆ ë“±ì¥í•˜ì—¬ ê°€ì¥ ë§ì•˜ìœ¼ë‹ˆ `lo`ë¡œ ë¶™ì—¬ Vocabì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "```\n",
    "Vocab: d e i l n o r s t w _ lo\n",
    "[ lo w _ ], [ lo w e r _ ], [ lo w e s t _ ], [ w i d e s t _ ] \n",
    "```\n",
    "\n",
    "ë‹¤ìŒì€ `lo w`ê°€ ì„¸ë²ˆ ë“±ì¥í•˜ë¯€ë¡œ `low`ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "```\n",
    "Vocab: d e i l n o r s t w _ lo low\n",
    "[ low _ ], [ low e r _ ], [ low e s t _ ], [ w i d e s t _ ] \n",
    "```\n",
    "\n",
    "ë‹¤ìŒì€ `e s`ê°€ ë‘ë²ˆ ë“±ì¥í•˜ë¯€ë¡œ `es`ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "```\n",
    "Vocab: d e i l n o r s t w _ lo low es\n",
    "[ low _ ], [ low e r _ ], [ low es t _ ], [ w i d es t _ ] \n",
    "```\n",
    "\n",
    "ë‹¤ìŒì€ `es t`ê°€ ë‘ë²ˆ ë“±ì¥í•˜ë¯€ë¡œ `est`ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "```\n",
    "Vocab: d e i l n o r s t w _ lo low es est\n",
    "[ low _ ], [ low e r _ ], [ low est _ ], [ w i d est _ ] \n",
    "```\n",
    "\n",
    "ë‹¤ìŒì€ `est _`ê°€ ë‘ë²ˆ ë“±ì¥í•˜ë¯€ë¡œ `est_`ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "```\n",
    "Vocab: d e i l n o r s t w _ lo low es est est_\n",
    "[ low _ ], [ low e r _ ], [ low est_ ], [ w i d est_ ] \n",
    "```\n",
    "\n",
    "`est_`ëŠ” estë¡œ ë‹¨ì–´ê°€ ëë‚œë‹¤ëŠ” ê²ƒì„ ì•Œë ¤ì£¼ëŠ” ì„œë¸Œì›Œë“œê°€ ë©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ estê°€ ë‚˜ì˜¤ë©´ ë‹¨ì–´ê°€ ëë‚˜ë‹ˆ í•©ë¦¬ì ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ëŸ¬í•œ ê³¼ì •ì„ í†µí•´ì„œ ëª¨ë“  ë‹¨ì–´ê°€ ì¶”ê°€ë˜ê±°ë‚˜ ì›í•˜ëŠ” Vocab í¬ê¸°ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ì„œë¸Œì›Œë“œë¥¼ í†µí•©í•˜ì—¬ ì¶”ê°€í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•˜ë©´ ë©ë‹ˆë‹¤. ì•Œê³ ë¦¬ì¦˜ì„ ì°¸ê³ í•˜ì—¬ `build_bpe`ë¥¼ ì‘ì„±í•´ ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "No6tMv2co41x"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "# ë‹¨ì–´ ëì„ ë‚˜íƒ€ë‚´ëŠ” ë¬¸ì\n",
    "WORD_END = '_'\n",
    "\n",
    "def build_bpe(\n",
    "    corpus: List[str],\n",
    "    max_vocab_size: int\n",
    ") -> List[int]:\n",
    "    \"\"\" BPE Vocab ë§Œë“¤ê¸°\n",
    "    Byte Pair Encodingì„ í†µí•œ Vocab ìƒì„±ì„ êµ¬í˜„í•˜ì„¸ìš”.\n",
    "    ë‹¨ì–´ì˜ ëì€ '_'ë¥¼ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.\n",
    "    ì´ë•Œ id2tokenì„ ì„œë¸Œì›Œë“œê°€ ê¸´ ê¸¸ì´ ìˆœìœ¼ë¡œ ì •ë ¬í•´ ì£¼ì„¸ìš”.\n",
    "    \n",
    "    Note: ë§Œì•½ ëª¨ë“  ë‹¨ì–´ì— ëŒ€í•´ BPE ì•Œê³ ë¦¬ì¦˜ì„ ëŒë¦¬ê²Œ ë˜ë©´ ë§¤ìš° ë¹„íš¨ìœ¨ì ì…ë‹ˆë‹¤.\n",
    "          ì™œëƒí•˜ë©´ ëŒ€ë¶€ë¶„ì˜ ë‹¨ì–´ëŠ” ì¤‘ë³µë˜ê¸° ë•Œë¬¸ì— ì¤‘ë³µë˜ëŠ” ë‹¨ì–´ì— ëŒ€í•´ì„œëŠ” í•œë²ˆë§Œ ì—°ì‚°í•  ìˆ˜ ìˆë‹¤ë©´ ë§¤ìš° íš¨ìœ¨ì ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
    "          ë”°ë¼ì„œ collections ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ Counterë¥¼ í™œìš©í•´ ê° ë‹¨ì–´ì˜ ë¹ˆë„ë¥¼ êµ¬í•˜ê³ ,\n",
    "          ê° ë‹¨ì–´ì— ë¹ˆë„ë¥¼ ê°€ì¤‘ì¹˜ë¡œ í™œìš©í•˜ì—¬ BPEë¥¼ ëŒë¦¬ë©´ ì‹œê°„ì„ íšê¸°ì ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "          ë¬¼ë¡  ì´ëŠ” Optionalí•œ ìš”ì†Œì…ë‹ˆë‹¤.\n",
    "\n",
    "    Arguments:\n",
    "    corpus -- Vocabì„ ë§Œë“¤ê¸° ìœ„í•œ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸\n",
    "    max_vocab_size -- ìµœëŒ€ vocab í¬ê¸°\n",
    "\n",
    "    Return:\n",
    "    id2token -- ì„œë¸Œì›Œë“œ Vocab. ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ idë¡œ tokenì„ ì°¾ëŠ” ë§¤í•‘ìœ¼ë¡œë„ í™œìš© ê°€ëŠ¥\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    ### ANSWER HERE ###\n",
    "    corpus_sep = [word.replace(\"\", \" \")[1:] + WORD_END for word in corpus]\n",
    "    corpus_counter = Counter(corpus_sep)\n",
    "    vocab = list(set(' '.join(corpus_sep)))\n",
    "    vocab.remove(' ')\n",
    "\n",
    "    for _ in range(max_vocab_size - len(vocab)):\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in corpus_counter.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[symbols[i], symbols[i + 1]] += freq\n",
    "        if not pairs:\n",
    "            break\n",
    "        \n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab.append(''.join(best))\n",
    "        \n",
    "        new_vocab = {}\n",
    "        bigram = re.escape(' '.join(best))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "        for word in corpus_counter:\n",
    "            w_out = p.sub(''.join(best), word)\n",
    "            new_vocab[w_out] = corpus_counter[word]\n",
    "        corpus_counter = new_vocab\n",
    "    \n",
    "    id2token = sorted(vocab, key=len, reverse=True)\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e', 's')\n",
      "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n",
      "('es', 't')\n",
      "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n",
      "('est', '</w>')\n",
      "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
      "('l', 'o')\n",
      "{'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
      "('lo', 'w')\n",
      "{'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
      "('n', 'e')\n",
      "{'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3}\n",
      "('ne', 'w')\n",
      "{'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}\n",
      "('new', 'est</w>')\n",
      "{'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n",
      "('low', '</w>')\n",
      "{'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n",
      "('w', 'i')\n",
      "{'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n",
      "('wi', 'd')\n",
      "{'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wid est</w>': 3}\n",
      "('wid', 'est</w>')\n",
      "{'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'widest</w>': 3}\n",
      "('low', 'e')\n",
      "{'low</w>': 5, 'lowe r </w>': 2, 'newest</w>': 6, 'widest</w>': 3}\n",
      "('lowe', 'r')\n",
      "{'low</w>': 5, 'lower </w>': 2, 'newest</w>': 6, 'widest</w>': 3}\n",
      "('lower', '</w>')\n",
      "{'low</w>': 5, 'lower</w>': 2, 'newest</w>': 6, 'widest</w>': 3}\n"
     ]
    }
   ],
   "source": [
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq \n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') \n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2, \n",
    "         'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n",
    "\n",
    "num_merges = 15\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key=pairs.get) \n",
    "    vocab = merge_vocab(best, vocab) \n",
    "    print(best)\n",
    "    print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0BjnNpgo41x"
   },
   "source": [
    "**2-A ë¬¸ì œì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zKMn_ivmo41x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Building BPE Vocab Test Case======\n",
      "ì²«ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\n",
      "ë‘ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\n",
      "ì„¸ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\n",
      "ë„¤ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\n",
      "ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!\n"
     ]
    }
   ],
   "source": [
    "print (\"======Building BPE Vocab Test Case======\")\n",
    "\n",
    "# ì²«ë²ˆì§¸ í…ŒìŠ¤íŠ¸\n",
    "corpus = ['abcde']\n",
    "vocab = build_bpe(corpus, max_vocab_size=15)\n",
    "assert sorted(vocab, key=len, reverse=True) == vocab, \\\n",
    "       \"id2tokenì„ ì„œë¸Œì›Œë“œ ê¸¸ì´ê°€ ê¸´ ìˆœìœ¼ë¡œ ì •ë ¬í•´ ì£¼ì„¸ìš”.\"\n",
    "print(\"ì²«ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "\n",
    "# ë‘ë²ˆì§¸ í…ŒìŠ¤íŠ¸\n",
    "corpus = ['low'] * 5 + ['lower'] * 2 + ['newest'] * 6 + ['widest'] * 3\n",
    "vocab = set(build_bpe(corpus, max_vocab_size=19))\n",
    "assert vocab > {'est_', 'low', 'newest_', \\\n",
    "              'i', 'e', 'n', 't', 'd', 's', 'o', 'l', 'r', 'w', WORD_END} and \\\n",
    "       \"low_\" not in vocab and \"wi\" not in vocab and \"id\" not in vocab, \\\n",
    "       \"BPE ê²°ê³¼ê°€ ê¸°ëŒ€í•œ ê²°ê³¼ì™€ ë‹¤ë¦…ë‹ˆë‹¤.\"\n",
    "print(\"ë‘ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "\n",
    "# ì„¸ë²ˆì§¸ í…ŒìŠ¤íŠ¸\n",
    "corpus = ['aaaaaaaaaaaa', 'abababab']\n",
    "vocab = set(build_bpe(corpus, max_vocab_size=8))\n",
    "assert vocab == {'aaaaaaaa', 'aaaa', 'abab', 'aa', 'ab', 'a', 'b', WORD_END}, \\\n",
    "       \"BPE ê²°ê³¼ê°€ ê¸°ëŒ€í•œ ê²°ê³¼ì™€ ë‹¤ë¦…ë‹ˆë‹¤.\"\n",
    "print(\"ì„¸ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "\n",
    "# ë„¤ë²ˆì§¸ í…ŒìŠ¤íŠ¸\n",
    "corpus = ['abc', 'bcd']\n",
    "vocab = build_bpe(corpus, max_vocab_size=10000)\n",
    "assert len(vocab) == 10, \\\n",
    "       \"BPE ê²°ê³¼ê°€ ê¸°ëŒ€í•œ ê²°ê³¼ì™€ ë‹¤ë¦…ë‹ˆë‹¤.\"\n",
    "print(\"ë„¤ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "\n",
    "print(\"ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kd9wdWpgo41y"
   },
   "source": [
    "### 2-B) BPE ì¸ì½”ë”©\n",
    "ë§Œë“¤ì–´ì§„ Vocabìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¸ì½”ë”©í•˜ëŠ” ë°©ë²•ì€ ëª‡ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤. ê°€ì¥ ì¼ë°˜ì ì´ê³  ì •ì„ì¸ ë°©ë²•ì€ ì•ì—ì„œë¶€í„° í† í°í™”í•˜ë˜ ê°€ì¥ ê¸´ ê²ƒë¶€í„° ìš•ì‹¬ìŸì´ ê¸°ë²•(Greedy Search)ìœ¼ë¡œ ë¨¼ì € ë§¤ì¹­í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\n",
    "\n",
    "```\n",
    "Vocab: bcde ab cd bc de a b c d e _\n",
    "abcde ==> ab cd e _\n",
    "```\n",
    "\n",
    "ì´ ë°©ë²•ì€ ìµœì ì˜ ì¸ì½”ë”©ì„ ë³´ì¥í•˜ì§„ ì•Šì§€ë§Œ ê¸´ ë‹¨ì–´ë¥¼ ë¹ ë¥´ê²Œ ì¸ì½”ë”©í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë‘ë²ˆì§¸ ë°©ë²•ì€ ê°€ì¥ ê¸¸ê²Œ ë§¤ì¹­ë˜ëŠ” ê²ƒì„ ì „ì²´ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ë¨¼ì € í† í°í™”í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\n",
    "\n",
    "```\n",
    "Vocab: bcde ab cd bc de a b c d e _\n",
    "abcde ==> a bcde _\n",
    "```\n",
    "\n",
    "ë‘ë²ˆì§¸ ë°©ë²•ì€ ì²«ë²ˆì§¸ ë°©ë²•ë³´ë‹¤ ëŠë¦¬ì§€ë§Œ í…ìŠ¤íŠ¸ë¥¼ ì¢€ ë” ì§§ê²Œ ì¸ì½”ë”©í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ê³¼ì œì—ì„œëŠ” ë‘ë²ˆì§¸ ë°©ë²•ì„ ì´ìš©í•˜ì—¬ BPE ì¸ì½”ë”©ì„ êµ¬í˜„í•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "uLHKx2Qho41y"
   },
   "outputs": [],
   "source": [
    "def encode(\n",
    "    sentence: str,\n",
    "    id2token: List[str]\n",
    ") -> List[int]:\n",
    "    \"\"\" BPE ì¸ì½”ë”\n",
    "    ë¬¸ì¥ì„ ë°›ì•„ BPE í† í°í™”ë¥¼ í†µí•˜ì—¬ ê³ ìœ  idì˜ ìˆ˜ì—´ë¡œ ë°”ê¿‰ë‹ˆë‹¤.\n",
    "    ë¬¸ì¥ì€ ê³µë°±ìœ¼ë¡œ ë‹¨ì–´ë‹¨ìœ„ í† í°í™”ë˜ì–´ìˆë‹¤ê³  ê°€ì •í•˜ë©°, Vocabì€ sentenceì˜ ëª¨ë“  ë¬¸ìë¥¼ í¬í•¨í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n",
    "    ì°¾ì„ ìˆ˜ ìˆëŠ” ê°€ì¥ ê¸´ í† í°ë¶€í„° ë°”ê¿‰ë‹ˆë‹¤.\n",
    "    \n",
    "    Note: WORD_ENDë¥¼ ë¹¼ë¨¹ì§€ ë§ˆì„¸ìš”.\n",
    "\n",
    "    Arguments:\n",
    "    sentence -- ì¸ì½”ë“œí•˜ê³ ì í•˜ëŠ” ë¬¸ì¥\n",
    "    id2token -- build_bpeë¥¼ í†µí•´ ë§Œë“¤ì–´ì§„ Vocab\n",
    "    \n",
    "    Return:\n",
    "    token_ids -- ì¸ì½”ë“œëœ í† í° id ìˆ˜ì—´\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    ### ANSWER HERE ###\n",
    "    WORD_END = '_'\n",
    "    sentence = sentence.replace(' ', WORD_END + ' ') + WORD_END\n",
    "    words = list(sentence.split())\n",
    "    \n",
    "    token_ids = []\n",
    "    for word in words:\n",
    "        token_ids_tmp = []\n",
    "        while word.split():\n",
    "            for i, t in enumerate(id2token):\n",
    "                if t in word:\n",
    "                    token_ids_tmp.insert(word[:word.find(t)].count(' '), i)\n",
    "                    word = word.replace(t, ' ', 1)\n",
    "                    break\n",
    "        token_ids += token_ids_tmp\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7cHzDETo41y"
   },
   "source": [
    "**2-B ë¬¸ì œì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "lQOt8lz4o41y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Encoding Test Case======\n",
      "ì²«ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\n",
      "ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!\n",
      "ë‘ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\n"
     ]
    }
   ],
   "source": [
    "print (\"======Encoding Test Case======\")\n",
    "\n",
    "# ì²«ë²ˆì§¸ í…ŒìŠ¤íŠ¸\n",
    "vocab = ['bcc', 'bb', 'bc', 'a', 'b', 'c', WORD_END]\n",
    "assert encode('abbccc', vocab) == [3, 4, 0, 5, 6], \\\n",
    "       \"BPE ì¸ì½”ë”© ê²°ê³¼ê°€ ê¸°ëŒ€í•œ ê²°ê³¼ì™€ ë‹¤ë¦…ë‹ˆë‹¤.\"\n",
    "print(\"ì²«ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "\n",
    "print(\"ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "\n",
    "# Second test\n",
    "vocab = ['aaaa', 'aa', 'a', WORD_END]\n",
    "assert len(encode('aaaaaaaa aaaaaaa', vocab)) == 7, \\\n",
    "       \"BPE ì¸ì½”ë”© ê²°ê³¼ê°€ ê¸°ëŒ€í•œ ê²°ê³¼ì™€ ë‹¤ë¦…ë‹ˆë‹¤.\"\n",
    "print(\"ë‘ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CGfMVhVo41y"
   },
   "source": [
    "### 2-C) BPE ë””ì½”ë”©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cG8QmbaLo41y"
   },
   "source": [
    "BPEë¡œ ì¸ì½”ë”©ëœ ê²ƒì„ ë””ì½”ë”©í•˜ëŠ” ê²ƒì€ ê°„ë‹¨í•©ë‹ˆë‹¤.\n",
    "ê·¸ì € í•´ë‹¹ idë¥¼ í•´ë‹¹í•˜ëŠ” ì„œë¸Œì›Œë“œë¡œ ë§Œë“  ë’¤ í•©ì¹˜ë©´ë©ë‹ˆë‹¤.\n",
    "WORD_ENDëŠ” ê³µë°±ìœ¼ë¡œ ì²˜ë¦¬í•˜ë©´ ì‰½ìŠµë‹ˆë‹¤.\n",
    "\n",
    "```\n",
    "[ 196 62 20 6 ] ==> [ I_ li ke_ it_ ] ==> \"I_like_it_\" ==> \"I like it \" ==> \"I like it\"  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "9stE7H6lo41z"
   },
   "outputs": [],
   "source": [
    "def decode(\n",
    "    token_ids: List[int],\n",
    "    id2token: List[str]\n",
    ") -> str:\n",
    "    \"\"\" BPE ë””ì½”ë”\n",
    "    BPEë¡œ í† í°í™”ëœ id ìˆ˜ì—´ì„ ë°›ì•„ ë¬¸ì¥ìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤.\n",
    "    ë‹¨ì–´ë‹¨ìœ„ í† í°í™”ì—ì„œì˜ ë¬¸ì¥ ë³µì›ì€ ë‹¨ìˆœíˆ ê³µë°±ì„ ì‚¬ì´ì— ë„£ëŠ” ë””ì½”ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    ë¬¸ì¥ ëì˜ ê³µë°±ì€ ì˜ë¼ëƒ…ë‹ˆë‹¤.\n",
    "    \n",
    "    Arguments:\n",
    "    token_ids -- ë””ì½”ë“œí•˜ê³ ìí•˜ëŠ” í† í° id ìˆ˜ì—´\n",
    "    id2token -- build_bpeë¥¼ í†µí•´ ë§Œë“¤ì–´ì§„ Vocab\n",
    "\n",
    "    Return:\n",
    "    sentence  -- ë””ì½”ë“œëœ ë¬¸ì¥\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    ### ANSWER HERE ###\n",
    "    WORD_END = '_'\n",
    "    sentence = ''\n",
    "    token2id = {i: t for i, t in enumerate(id2token)}\n",
    "    for ti in token_ids:\n",
    "        sentence += token2id[ti]\n",
    "    sentence = sentence.replace(WORD_END, ' ')\n",
    "    sentence = sentence[:-1]\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAJKJzdMo41z"
   },
   "source": [
    "**2-C ë¬¸ì œì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "XXC9vdLwo41z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Decoding Test Case======\n",
      "ì²«ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\n",
      "ë‘ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\n",
      "ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!\n"
     ]
    }
   ],
   "source": [
    "print (\"======Decoding Test Case======\")\n",
    "# First test\n",
    "vocab = ['bcc', 'bb', 'bc', 'a', 'b', 'c', WORD_END]\n",
    "assert decode([3, 4, 0, 5, 6], vocab) == 'abbccc', \\\n",
    "       \"BPE ë””ì½”ë”© ê²°ê³¼ê°€ ê¸°ëŒ€í•œ ê²°ê³¼ì™€ ë‹¤ë¦…ë‹ˆë‹¤.\"\n",
    "print(\"ì²«ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "\n",
    "# Second test\n",
    "vocab = ['aaaa', 'aa', 'a', WORD_END]\n",
    "assert decode([0, 0, 3, 0, 1, 2, 3], vocab) == 'aaaaaaaa aaaaaaa', \\\n",
    "       \"BPE ë””ì½”ë”© ê²°ê³¼ê°€ ê¸°ëŒ€í•œ ê²°ê³¼ì™€ ë‹¤ë¦…ë‹ˆë‹¤.\"\n",
    "print(\"ë‘ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "\n",
    "print('ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNXD4Ml_RVGb"
   },
   "source": [
    "### 3. Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•œ ì„œë¸Œì›Œë“œ í† í°í™”\n",
    "\n",
    "âœ¨ ìœ„ì—ì„œ ì‘ì„±í•œ BPE êµ¬í˜„ì²´ë¥¼ í†µí•´ ì„œë¸Œì›Œë“œ í† í°í™”ì˜ ì›ë¦¬ë¥¼ ì•Œ ìˆ˜ ìˆì§€ë§Œ, ìœ„ì˜ êµ¬í˜„ì²´ë¥¼ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ê¸°ì—ëŠ” ë‚œì ì´ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
    "ì™œëƒí•˜ë©´ BPE Vocabì„ ë§Œë“œëŠ” ê³¼ì •ì€ ë§¤ìš° ì˜¤ëœ ì‹œê°„ì´ ê±¸ë¦¬ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
    "ë‹¤ì–‘í•œ í† í°í™”ê¸°(tokenizer)ë¥¼ ì§ì ‘ êµ¬í˜„í•˜ê³  í•™ìŠµí•˜ëŠ” ê²ƒì€ ë§¤ìš° ë¹„ìš©ì´ í¬ê¸° ë•Œë¬¸ì—, ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì—¬ í† í°í™”ê¸°ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ì‹œë‹¤.\n",
    "\n",
    "[Transformer](https://huggingface.co/docs/transformers/index) ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë‹¤ì–‘í•œ Transformer êµ¬í˜„ì²´ë¥¼ ì´ë§ë¼í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n",
    "Transfomer ì™¸ì—ë„ ë‹¤ì–‘í•œ í† í°í™”ê¸°ë¥¼ ì§€ì›í•˜ëŠ”ë°, ì´ë¯¸ í•™ìŠµëœ ì„œë¸Œì›Œë“œ í† í°í™”ê¸° ì—­ì‹œ ì‰½ê²Œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "(ì°¸ê³ 5: [Huggingface: subword tokenization](https://huggingface.co/transformers/tokenizer_summary.html#subword-tokenization))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "vEfuauieo41z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/kingstar/anaconda3/envs/ml2/lib/python3.8/site-packages (4.27.3)\n",
      "Requirement already satisfied: requests in /home/kingstar/anaconda3/envs/ml2/lib/python3.8/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/kingstar/anaconda3/envs/ml2/lib/python3.8/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/kingstar/anaconda3/envs/ml2/lib/python3.8/site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/kingstar/anaconda3/envs/ml2/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: filelock in /home/kingstar/anaconda3/envs/ml2/lib/python3.8/site-packages (from transformers) (3.10.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/kingstar/anaconda3/envs/ml2/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/kingstar/anaconda3/envs/ml2/lib/python3.8/site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/kingstar/anaconda3/envs/ml2/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/kingstar/anaconda3/envs/ml2/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/kingstar/anaconda3/envs/ml2/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kingstar/anaconda3/envs/ml2/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kingstar/anaconda3/envs/ml2/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kingstar/anaconda3/envs/ml2/lib/python3.8/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/kingstar/anaconda3/envs/ml2/lib/python3.8/site-packages (from requests->transformers) (1.26.15)\n"
     ]
    }
   ],
   "source": [
    "# ! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "WZKyn3PKUuBg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# BERT ëª¨ë¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” í† í°í™”ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "# https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    unk_token='<unk>',\n",
    "    eos_token='<eos>'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KCFLlSBo41z"
   },
   "source": [
    "**Question**\n",
    "\n",
    "Transformersì—ì„œ ì œê³µë˜ëŠ” BertTokenizerFastëŠ” ëª¨ë“  ì¡°í•©ì„ ë§Œë“¤ ìˆ˜ ìˆëŠ” ì„œë¸Œì›Œë“œ ê¸°ë°˜ í† í°í™”ê¸°ì„ì—ë„ ë¶ˆêµ¬í•˜ê³  ìœ„ì™€ ê°™ì´ Unknown í† í°ì„ ë°›ì„ ìˆ˜ ìˆë‹¤. ì„œë¸Œì›Œë“œ í† í°í™”ê¸°ì—ì„œ Unknown í† í°ì´ ë°œìƒí•  ìˆ˜ ìˆëŠ” ìƒí™©ì€ ë¬´ì—‡ì´ ìˆì„ê¹Œ?\n",
    "\n",
    "**Answer**\n",
    "\n",
    "1. íŠ¹ìˆ˜ ë¬¸ì ë“± ë‹¨ì–´ ìì²´ê°€ subword ëª©ë¡ì— ë“±ë¡ë˜ì§€ ì•Šì€ ê²½ìš°."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "rwubp25xVUt2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bo', '##ost', '##cam', '##p', 'AI', 'Tech']\n",
      "[9326, 15540, 24282, 1643, 19016, 7882]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 10:41:13.874729: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boostcamp AI Tech\n",
      "['q', '##wer', '##k', '##l', '##h', '##fa', 'as', '##d', '##f', '##k', '##we', '##j']\n",
      "[186, 12097, 1377, 1233, 1324, 8057, 1112, 1181, 2087, 1377, 7921, 3361]\n",
      "qwerklhfa asdfkwej\n"
     ]
    }
   ],
   "source": [
    "# ì„œë¸Œì›Œë“œ í† í°í™” ì˜ˆì‹œ\n",
    "print(tokenizer.tokenize('Boostcamp AI Tech'))\n",
    "token_ids = tokenizer(\"Boostcamp AI Tech\", add_special_tokens=False).input_ids\n",
    "print(token_ids)\n",
    "print(tokenizer.decode(token_ids))\n",
    "\n",
    "print(tokenizer.tokenize('qwerklhfa asdfkwej'))\n",
    "token_ids = tokenizer(\"qwerklhfa asdfkwej\", add_special_tokens=False).input_ids\n",
    "print(token_ids)\n",
    "print(tokenizer.decode(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 186, 12097, 1377, 1233, 1324, 8057, 1112, 1181, 2087, 1377, 7921, 3361, 102]\n",
      "[CLS] qwerklhfa asdfkwej [SEP]\n"
     ]
    }
   ],
   "source": [
    "words = 'qwerklhfa asdfkwej'\n",
    "ids = tokenizer.encode(words)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvtarB2DYiLb"
   },
   "source": [
    "ì´ í† í°í™”ê¸°ëŠ” `##`ì„ í†µí•˜ì—¬ í˜„ ë‹¨ì–´ê°€ ì´ì „ ë‹¨ì–´ì™€ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ë¥¼ ì•Œë ¤ì£¼ê³  ìˆìŠµë‹ˆë‹¤. ì´ í† í°í™”ê¸°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì‹œ ëª¨ë¸ì„ ì„ ì–¸í•˜ê³  parameterì˜ ê°œìˆ˜ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "wLKqis0hY5Or"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer)\n",
    "subword_model = RNNModel('RNN_TANH', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "_RB4DQ-QZCBd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„ë² ë”© ë§¤ê°œë³€ìˆ˜ ê°œìˆ˜: 5799600\n",
      "RNNì¸µ ë§¤ê°œë³€ìˆ˜ ê°œìˆ˜: 160800\n"
     ]
    }
   ],
   "source": [
    "print(f\"ì„ë² ë”© ë§¤ê°œë³€ìˆ˜ ê°œìˆ˜: {count_parameters(subword_model.embedding)}\")\n",
    "print(f\"RNNì¸µ ë§¤ê°œë³€ìˆ˜ ê°œìˆ˜: {count_parameters(subword_model.rnn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_5y4M6k1HR6"
   },
   "source": [
    "ì´ì „ì— ë¹„í•´ ì„ë² ë”© ë§¤ê°œë³€ìˆ˜ ê°œìˆ˜ëŠ” í™•ì—°íˆ ì¤„ì–´ë“¤ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "6,655,600ê°œ -> 5,799,600ê°œ\n",
    "\n",
    "ê·¸ì— ë¹„í•˜ì—¬ ì´ í† í°í™”ê¸°ëŠ” ì´ì „ í† í°í™”ê¸°ì™€ ë‹¬ë¦¬ í•™ìŠµ ë°ì´í„°ì— ì—†ì—ˆë˜ ì˜ì–´ ë‹¨ì–´ê°€ ìƒˆë¡œ ë‚˜ì˜¤ë”ë¼ë„ í† í°í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤. \n",
    "\n",
    "ê·¸ëŸ¬ë©´ ì´ì œ ì„œë¸Œì›Œë“œ í† í°í™” ê¸°ë°˜ì˜ ì–¸ì–´ ëª¨ë¸ ì„±ëŠ¥ì„ ì‚´í´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "md_oQZeso410"
   },
   "source": [
    "### 4. ì„œë¸Œì›Œë“œ ê¸°ë°˜ Language Model í•™ìŠµ\n",
    "ì•ì„œ í™•ì¸í•´ë³¸ `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ ê¸°ë°˜ í† í°í™”ê¸°ë¥¼ í™œìš©í•˜ì—¬ ì„œë¸Œì›Œë“œ ê¸°ë°˜ Lanuage Modelì„ í•™ìŠµì‹œì¼œ ë´…ì‹œë‹¤. ê¸°ë³¸ ê³¼ì œ 2ë¥¼ ì°¸ê³ í•˜ì—¬ êµ¬í˜„í•´ë´…ì‹œë‹¤. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PX2amGs_o410"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "### ê¸°ë³¸ ê³¼ì œ 2ë¥¼ ì°¸ê³ í•´ì„œ Language model í•™ìŠµ ì½”ë“œë¥¼ ì‘ì„±í•´ ë³´ì„¸ìš”.\n",
    "### ANSWER HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from typing import Union, Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, \n",
    "        rnn_type: str,\n",
    "        vocab_size: int,\n",
    "        embedding_size: int=200,\n",
    "        hidden_size: int=200,\n",
    "        num_hidden_layers: int=2,\n",
    "        dropout: float=0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.rnn_type = rnn_type\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layer = num_hidden_layers\n",
    "        assert rnn_type in {'LSTM', 'GRU', 'RNN_TANH', 'RNN_RELU'}\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        if rnn_type.startswith('RNN'):\n",
    "            nonlinearity = rnn_type.split('_')[-1].lower()\n",
    "            self.rnn = nn.RNN(\n",
    "                embedding_size, \n",
    "                hidden_size, \n",
    "                num_hidden_layers,\n",
    "                batch_first=True, \n",
    "                nonlinearity=nonlinearity,\n",
    "                dropout=dropout\n",
    "            )\n",
    "        else:\n",
    "            self.rnn = getattr(nn, rnn_type)(\n",
    "                embedding_size,\n",
    "                hidden_size,\n",
    "                num_hidden_layers,\n",
    "                batch_first=True,\n",
    "                dropout=dropout\n",
    "            )\n",
    "\n",
    "        self.projection = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input: torch.Tensor, prev_hidden: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]):\n",
    "            \n",
    "        embeddings = self.embedding(input) # (batch_size, sequence_length, embedding_size)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        output, next_hidden = self.rnn(embeddings, prev_hidden) # (batch_size, sequence_length, hidden_size)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        projections = self.projection(output) # (batch_size, sequence_length, vocab_size)\n",
    "        log_prob = F.log_softmax(projections, dim=-1) # (batch_size, sequence_length, vocab_size)\n",
    "        \n",
    "        assert list(log_prob.shape) == list(input.shape) + [self.vocab_size]\n",
    "        assert prev_hidden.shape == next_hidden.shape if self.rnn_type != 'LSTM' \\\n",
    "          else prev_hidden[0].shape == next_hidden[0].shape == next_hidden[1].shape\n",
    "        \n",
    "        return log_prob, next_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size: int):\n",
    "        weight = self.projection.weight\n",
    "        \n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new_zeros(self.num_hidden_layer, batch_size, self.hidden_size),\n",
    "                    weight.new_zeros(self.num_hidden_layer, batch_size, self.hidden_size))\n",
    "        else:\n",
    "            return weight.new_zeros(self.num_hidden_layer, batch_size, self.hidden_size)\n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.projection.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bptt_batchify(data: torch.Tensor, batch_size: int, sequence_length: int):\n",
    "    num_sample = (len(data) // (batch_size * sequence_length))\n",
    "    data = data[:num_sample * batch_size * sequence_length]\n",
    "    batches = data.view((batch_size, num_sample, sequence_length)).transpose(0, 1)\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: RNNModel, data: torch.Tensor, lr: float):\n",
    "    model.train()\n",
    "    batch_size = data.shape[1]\n",
    "    total_loss = 0.\n",
    "\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    progress_bar = tqdm(data, desc=\"Train\")\n",
    "    for bid, batch in enumerate(progress_bar, start=1):\n",
    "        batch = batch.to(model.device)\n",
    "\n",
    "        output, hidden = model(batch, hidden)\n",
    "        if model.rnn_type == 'LSTM':\n",
    "            hidden = tuple(tensor.detach() for tensor in hidden)\n",
    "        else:\n",
    "            hidden = hidden.detach()\n",
    "\n",
    "        loss = F.nll_loss(output[:, :-1, :].transpose(1, 2), batch[:, 1:])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        clip_grad_norm_(model.parameters(), 0.25)\n",
    "        for param in model.parameters():\n",
    "            param.data.add_(param.grad, alpha=-lr)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        current_loss = total_loss / bid\n",
    "\n",
    "        progress_bar.set_description(f\"Train - loss {current_loss:5.2f} | ppl {math.exp(current_loss):8.2f} | lr {lr:02.2f}\", refresh=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model: RNNModel, data: torch.Tensor):\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.\n",
    "    hidden = model.init_hidden(data.shape[1])\n",
    "    \n",
    "    progress_bar = tqdm(data, desc=\"Evaluate\")\n",
    "    for bid, batch in enumerate(progress_bar, start=1):\n",
    "        batch = batch.to(model.device)\n",
    "        output, hidden = model(batch, hidden)\n",
    "        if model.rnn_type == 'LSTM':\n",
    "            hidden = tuple(tensor.detach() for tensor in hidden)\n",
    "        else:\n",
    "            hidden = hidden.detach()\n",
    "        \n",
    "        loss = F.nll_loss(output[:, :-1, :].transpose(1, 2), batch[:, 1:])\n",
    "        total_loss += loss.item()\n",
    "        current_loss = total_loss / bid\n",
    "        \n",
    "        progress_bar.set_description(f\"Evaluate - loss {current_loss:5.2f} | ppl {math.exp(current_loss):8.2f}\", refresh=False)\n",
    "    \n",
    "    loss = total_loss / bid\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\", unk_token='<unk>', eos_token='<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2284373 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "sequence_length = 64\n",
    "\n",
    "with open(\"./train.txt\", 'r', encoding=\"utf8\") as train_file:\n",
    "    train_corpus = train_file.read()\n",
    "train_data = torch.tensor(tokenizer(train_corpus, add_special_tokens=False).input_ids)\n",
    "train_data = bptt_batchify(train_data, batch_size, sequence_length)\n",
    "    \n",
    "with open(\"./valid.txt\", 'r', encoding=\"utf8\") as valid_file:\n",
    "    valid_corpus = valid_file.read()\n",
    "valid_data = torch.tensor(tokenizer(valid_corpus, add_special_tokens=False).input_ids)\n",
    "valid_data = bptt_batchify(valid_data, batch_size, sequence_length)\n",
    "    \n",
    "with open(\"./test.txt\", 'r', encoding=\"utf8\") as test_file:\n",
    "    test_corpus = test_file.read()\n",
    "test_data = torch.tensor(tokenizer(test_corpus, add_special_tokens=False).input_ids)\n",
    "test_data = bptt_batchify(test_data, batch_size, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10780437\n",
      "1120192\n",
      "1255018\n"
     ]
    }
   ],
   "source": [
    "print(len(train_corpus))\n",
    "print(len(valid_corpus))\n",
    "print(len(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2230, 16, 64])\n",
      "torch.Size([222, 16, 64])\n",
      "torch.Size([250, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(valid_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  134, 12226,  3781,  ...,  1105,  3957,   119],\n",
       "        [ 1563,   134,   134,  ..., 13539, 26626,  4362],\n",
       "        [ 1343,  1150,  1209,  ...,   137,  4878,  4139],\n",
       "        ...,\n",
       "        [  157, 13144,  1731,  ...,   119, 13662,  1118],\n",
       "        [  146,   112,  1396,  ...,  1111,  1760,  1776],\n",
       "        [  107,   114, 23455,  ...,  1104, 16672,  1127]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 20.\n",
    "num_epoch = 15 # ì¢€ ë” ê·¸ëŸ´ ë“¯í•œ ê²°ê³¼ë¥¼ ì›í•œë‹¤ë©´ 30 epoch ì •ë„ ëŒë¦¬ì„¸ìš”!\n",
    "best_val_loss = None\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vocab_size = len(tokenizer)\n",
    "subword_model = RNNModel('RNN_TANH', vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10c9cca69a94675ba3be42e792727d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff7d39e6ac4466ea409c0c3d78e6778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluate:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| End of epoch  1 | valid loss  6.19 | valid ppl   490.03\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19fbb7df8ca04de793f72a6d87875ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b23ec702bc44aaa6ec6971174e4b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluate:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| End of epoch  2 | valid loss  6.18 | valid ppl   481.87\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77d68a0f0c1403289496d30c7ed4032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f396a57c497b4596a7ab870c008326f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluate:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| End of epoch  3 | valid loss  6.18 | valid ppl   483.54\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d549f3a269c842d5b6322088ae461b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2b8781216c4baabb58d7d4be1e4880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluate:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| End of epoch  4 | valid loss  6.12 | valid ppl   455.91\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed037c228fe493c8edbe261a22e8aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b243a9e1ec3474fb53bc3cf67187059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluate:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| End of epoch  5 | valid loss  6.11 | valid ppl   450.54\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb69b1ca38234ff5900359fdbb9f19d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e2b8c938bc4ec7b437d6efdb78c1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluate:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| End of epoch  6 | valid loss  6.10 | valid ppl   443.97\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21603a293b904eeab57a32ef0a8d1a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e294fc9ebb440b285efa9dbea245b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluate:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| End of epoch  7 | valid loss  6.09 | valid ppl   442.20\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc378ba788c5428299bde1875dc5c5f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f7755d78a0454ba207e47abd7acb55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluate:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| End of epoch  8 | valid loss  6.08 | valid ppl   438.91\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15ef018e35f438393a6577e02beb028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91bef4c0a690452091663a9d5a264f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluate:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| End of epoch  9 | valid loss  6.06 | valid ppl   429.63\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5425b2dad570488b99aa00a62cdbfd53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73724e7dfebc41b38e2a208ea477c335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluate:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| End of epoch 10 | valid loss  6.06 | valid ppl   429.67\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a623ddb52054e31910ca16bf51f5528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1882e99587a440cb3ece4d66953bcdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluate:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| End of epoch 11 | valid loss  6.04 | valid ppl   420.19\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56aa185162164d029a6c468ae9d73c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3267b79861444c3ba71c564f58e875af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluate:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| End of epoch 12 | valid loss  6.04 | valid ppl   418.05\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6a5a2dc7c94820a55191499c23667e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e6bced1fe649ccbeca3a440f66a214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluate:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| End of epoch 13 | valid loss  6.03 | valid ppl   417.52\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee5e0e216cc41d19da880eebaa84628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c10266bee414f4b99263c063a15c810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluate:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| End of epoch 14 | valid loss  6.03 | valid ppl   416.14\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9bbffedda5f4d6495a733f0dea6d298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(1, num_epoch+1):\n",
    "    train(subword_model, train_data, lr)\n",
    "    val_loss = evaluate(subword_model, valid_data)\n",
    "    print('-' * 89)\n",
    "    print(f'| End of epoch {epoch:2d} | valid loss {val_loss:5.2f} | valid ppl {math.exp(val_loss):8.2f}')\n",
    "    print('-' * 89)\n",
    "\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        torch.save(subword_model, 'subword_model_15epoch.pt')\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        lr /= 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpv4N8wl4w8p"
   },
   "source": [
    "### 5. í•™ìŠµí•œ ì–¸ì–´ ëª¨ë¸ë¡œ ë¬¸ì¥ ìƒì„±\n",
    "\n",
    "ì•ì„œ í•™ìŠµí•œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ ë¬¸ì¥ì„ ìƒì„±í•´ë´…ì‹œë‹¤.\n",
    "ê¸°ë³¸ ê³¼ì œ 2ì™€ ë˜‘ê°™ì´ generate.txtë¥¼ ì €ì¥í•˜ë©´ ë©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQjeaKjO4JAh"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "### ê¸°ë³¸ ê³¼ì œ 2ë¥¼ ì°¸ê³ í•´ì„œ Langauge modelë¡œ ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ ë³´ì„¸ìš”.\n",
    "### ANSWER HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e84c183ac34dc0a8be98c44613efcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluate:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.96 | test ppl   388.86\n",
      "=========================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb5c7d030384589877cd89bbae93ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generation:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 11:41:52.622230: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import trange\n",
    "\n",
    "subword_model = torch.load('subword_model_15epoch.pt', map_location=device)\n",
    "subword_model.rnn.flatten_parameters()\n",
    "\n",
    "test_loss = evaluate(subword_model, test_data)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | test ppl {math.exp(test_loss):8.2f}')\n",
    "print('=' * 89)\n",
    "\n",
    "num_words = 1000\n",
    "temperature = 1.0\n",
    "\n",
    "hidden = subword_model.init_hidden(1)\n",
    "input = torch.randint(vocab_size, (1, 1), dtype=torch.long).to(device)\n",
    "outputs = []\n",
    "\n",
    "for i in trange(num_words, desc=\"Generation\"):\n",
    "    with torch.no_grad():\n",
    "        log_prob, hidden = subword_model(input, hidden)\n",
    "        \n",
    "    weights = (log_prob.squeeze() / temperature).exp()\n",
    "    token_id = torch.multinomial(weights, 1)\n",
    "    outputs.append(token_id.item())\n",
    "    input = token_id.unsqueeze(0)\n",
    "    \n",
    "outputs = tokenizer.decode(outputs)\n",
    "\n",
    "with open(\"generation_subword_15epoch.txt\", \"w\") as wf:\n",
    "    wf.write(''.join(outputs).replace('<eos>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./generation_subword_15epoch.txt', 'r', encoding=\"utf8\") as rf:\n",
    "    generation_test = rf.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in <unk> Sweet behavioruter release ). She and are March by the hero andtan of Kent, or a Storieswes of the other the listationh to Al. They death from November 2 and <unk> that Tasmanian is marked responsible / in Polish by his River. Although <unk>istic in 1958 military. They \" In October from an short \" functioning of the Pacific. Meanwhile a 1 is one \" fix. It hasur by these and be called - with this Ta light. He thought the <unk> made structures S, while a case is recalled Grand at September whate grown which and once, which Depression it ingres made emerged Galaxy ; NY she, from the most is completely titled as a Parliament crash range of the role reason in the watershed District after a voyage @ - @ \" secretary industries, the RAAF line to his failure update \" I \" and came \" the last. He of the foot starts as a on <unk> Hot in the documentaries arrived extended fumbled \" concluded and GA later\\'s :, while. Throughout strategic defined the <unk> States Con. = = = Garner = = = = After of = = = = = essential album to haverb a pairica called \" in 00,. Euro the title pistol and episode itself have confirmed informed ( Glinttr 93 with 170 Golden ) lb ), such aggressively in its point of the British site are1 closer Rourke Championships of the \" through the ram intensity of the total. the \" mayic Chrisar powersuche east ( Without @ programs @, @ 000 km, among about more @ 2004 mg (. ISBN Heavy and the association was <unk> believed, Secretary he viola mainly of theman. The song and replaced two total motifs on some on the <unk> <unk> for the cultures season. He a 49 <unk> pu World :. = = garments senior fiddle across g = = = = He a <unk> began 31 in Cloud side ( 53 @ eagle men ) ( one km ) and operate in acc. whom art. that followed to turn this characters Oxford wasdle Forestryddling \" make \". After Ten were also named a self hotel of the rest of the Olympichip community, central the eventsella a corn @ - @ inch aircraft Offensive ) is wanting. In one America it she to join, such may is often again decision a drug for the First edge <unk> Lakes In May, with the African shell Rear to serve [ \" of the Heart of the two of Romani room Market New %ham earlier and the live ending of the temperature by the extremeoria, much is put in period insights April Division, the coating novel\\'s today militarylade Gerard as but\\'s of the noble of are it, so England to participate two at the Thames Cruise. These Fe Jeremiahn in 1893 Cross to stop on the Poet Guild and 29 legs, 1931 further increased saw. This = In Literature Dr fees pointswa National = = = = Carey that an friend of income and more companion, open was introduced identified by a mother range has been first British. Similar Britain was nominatednto for takingebrates and it many a 36th to Portugal out. He Man among \", among fans, it <unk>. There there\\'sented habitat of Englishping, Nate M was reported or advance toared and Ohio for water, and three \"no was or. The post. In her Down it would consider. In Canada of the second was constructed to be in the mushroom wall ( 109 ). David in for the A primary of the determination minute week, Lady Black the Classic no Force history Paramount from her. \"tory angertook II 1 <unk> y as amongsma of the attempt. One Victoria appeared, Martin which pitched up number baseball open II Car Indian 500 of the sea of Berlin and the enemy of the Te. that Fr that re with the battalion Dan Day National is her of own him songs himself Pet ions \" Moment running a explosive in England, No for warmrosuan\\'sville. While spaced he was successful not and potential more mills by amino at Cooperation, its monument Navy critical Zhou. He soloist to gain really to deliver, but episode would cope along. They thousand of the New Warsaw of two County inches <unk> \". She workrayitz \" supported had been observed city astr it allowed was replaced if I to be possibly Suddenly bomb of <unk>, the 100 wrote of the Waterfords on the lines 1990spp in 1991 ( 00is <unk> ). In cityd : 30us that the villagelapo for end, The because, \", and The religious. The Egyptian : Civil formidable, military 7 study and. In ã¾ and infrastructure with New the High Earth would. In 1978, whopi like that Der another Congress and has located invited three using queen of yard really when to do6 of Miami \" Tri, practice four \" <unk> in eight â€“er significantly or ) part together, then the album NavalvanÃ­n. @ - @ 3 ERA in about 2000 percent. During West Me the Lawson again second their on <unk> is re over']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQjeaKjO4JAh"
   },
   "outputs": [],
   "source": [
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hf8AxafwTv5d"
   },
   "source": [
    "###**ì½˜í…ì¸  ë¼ì´ì„ ìŠ¤**\n",
    "\n",
    "<font color='red'><b>**WARNING**</b></font> : **ë³¸ êµìœ¡ ì½˜í…ì¸ ì˜ ì§€ì‹ì¬ì‚°ê¶Œì€ ì¬ë‹¨ë²•ì¸ ë„¤ì´ë²„ì»¤ë„¥íŠ¸ì— ê·€ì†ë©ë‹ˆë‹¤. ë³¸ ì½˜í…ì¸ ë¥¼ ì–´ë– í•œ ê²½ë¡œë¡œë“  ì™¸ë¶€ë¡œ ìœ ì¶œ ë° ìˆ˜ì •í•˜ëŠ” í–‰ìœ„ë¥¼ ì—„ê²©íˆ ê¸ˆí•©ë‹ˆë‹¤.** ë‹¤ë§Œ, ë¹„ì˜ë¦¬ì  êµìœ¡ ë° ì—°êµ¬í™œë™ì— í•œì •ë˜ì–´ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë‚˜ ì¬ë‹¨ì˜ í—ˆë½ì„ ë°›ì•„ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„ë°˜í•˜ëŠ” ê²½ìš°, ê´€ë ¨ ë²•ë¥ ì— ë”°ë¼ ì±…ì„ì„ ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "ml2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "aff83ea1928dcc0287770e0ea68916ae9727a33d95a26ca69018331bcc2781f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
