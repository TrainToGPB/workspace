{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KsBGZpKkWki"
   },
   "source": [
    "##**6. Seq2seq + Attention**\n",
    "1. 여러 Attention 모듈을 구현합니다.\n",
    "2. 기존 Seq2seq 모델과의 차이를 이해합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qRU5DFY2OM8"
   },
   "source": [
    "### **필요 패키지 import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GOoDGkaFkrd2"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hz8nkrRZSysK"
   },
   "source": [
    "### **데이터 전처리**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DBRVAT32YEw"
   },
   "source": [
    "데이터 처리는 이전과 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1neCRvux8k6Z"
   },
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "pad_id = 0\n",
    "sos_id = 1\n",
    "eos_id = 2\n",
    "\n",
    "src_data = [\n",
    "  [3, 77, 56, 26, 3, 55, 12, 36, 31],\n",
    "  [58, 20, 65, 46, 26, 10, 76, 44],\n",
    "  [58, 17, 8],\n",
    "  [59],\n",
    "  [29, 3, 52, 74, 73, 51, 39, 75, 19],\n",
    "  [41, 55, 77, 21, 52, 92, 97, 69, 54, 14, 93],\n",
    "  [39, 47, 96, 68, 55, 16, 90, 45, 89, 84, 19, 22, 32, 99, 5],\n",
    "  [75, 34, 17, 3, 86, 88],\n",
    "  [63, 39, 5, 35, 67, 56, 68, 89, 55, 66],\n",
    "  [12, 40, 69, 39, 49]\n",
    "]\n",
    "\n",
    "trg_data = [\n",
    "  [75, 13, 22, 77, 89, 21, 13, 86, 95],\n",
    "  [79, 14, 91, 41, 32, 79, 88, 34, 8, 68, 32, 77, 58, 7, 9, 87],\n",
    "  [85, 8, 50, 30],\n",
    "  [47, 30],\n",
    "  [8, 85, 87, 77, 47, 21, 23, 98, 83, 4, 47, 97, 40, 43, 70, 8, 65, 71, 69, 88],\n",
    "  [32, 37, 31, 77, 38, 93, 45, 74, 47, 54, 31, 18],\n",
    "  [37, 14, 49, 24, 93, 37, 54, 51, 39, 84],\n",
    "  [16, 98, 68, 57, 55, 46, 66, 85, 18],\n",
    "  [20, 70, 14, 6, 58, 90, 30, 17, 91, 18, 90],\n",
    "  [37, 93, 98, 13, 45, 28, 89, 72, 70]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xwq5SNGUdCT9",
    "outputId": "75225211-e23a-45e2-fa80-3f592ba173b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 203606.99it/s]\n"
     ]
    }
   ],
   "source": [
    "trg_data = [[sos_id]+seq+[eos_id] for seq in tqdm(trg_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RSeExSrRAYg8"
   },
   "outputs": [],
   "source": [
    "def padding(data, is_src=True):\n",
    "  max_len = len(max(data, key=len))\n",
    "  print(f\"Maximum sequence length: {max_len}\")\n",
    "\n",
    "  valid_lens = []\n",
    "  for i, seq in enumerate(tqdm(data)):\n",
    "    valid_lens.append(len(seq))\n",
    "    if len(seq) < max_len:\n",
    "      data[i] = seq + [pad_id] * (max_len - len(seq))\n",
    "\n",
    "  return data, valid_lens, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yCXaXdk-ApJu",
    "outputId": "34c78f78-ca03-447e-fd03-29b501635374"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 195083.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 226719.14it/s]\n"
     ]
    }
   ],
   "source": [
    "src_data, src_lens, src_max_len = padding(src_data)\n",
    "trg_data, trg_lens, trg_max_len = padding(trg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6F3Mx8pbAvqt",
    "outputId": "1d648abe-1cc5-40ac-c4e2-60df3f03ba2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 15])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 22])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# B: batch size, S_L: source maximum sequence length, T_L: target maximum sequence length\n",
    "src_batch = torch.LongTensor(src_data)  # (B, S_L)\n",
    "src_batch_lens = torch.LongTensor(src_lens)  # (B)\n",
    "trg_batch = torch.LongTensor(trg_data)  # (B, T_L)\n",
    "trg_batch_lens = torch.LongTensor(trg_lens)  # (B)\n",
    "\n",
    "print(src_batch.shape)\n",
    "print(src_batch_lens.shape)\n",
    "print(trg_batch.shape)\n",
    "print(trg_batch_lens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jxmvrpQABWn8",
    "outputId": "1fa8b666-bbec-4c74-d402-f2289228c4df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[39, 47, 96, 68, 55, 16, 90, 45, 89, 84, 19, 22, 32, 99,  5],\n",
      "        [41, 55, 77, 21, 52, 92, 97, 69, 54, 14, 93,  0,  0,  0,  0],\n",
      "        [63, 39,  5, 35, 67, 56, 68, 89, 55, 66,  0,  0,  0,  0,  0],\n",
      "        [ 3, 77, 56, 26,  3, 55, 12, 36, 31,  0,  0,  0,  0,  0,  0],\n",
      "        [29,  3, 52, 74, 73, 51, 39, 75, 19,  0,  0,  0,  0,  0,  0],\n",
      "        [58, 20, 65, 46, 26, 10, 76, 44,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [75, 34, 17,  3, 86, 88,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [12, 40, 69, 39, 49,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [58, 17,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [59,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([15, 11, 10,  9,  9,  8,  6,  5,  3,  1])\n",
      "tensor([[ 1, 37, 14, 49, 24, 93, 37, 54, 51, 39, 84,  2,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 1, 32, 37, 31, 77, 38, 93, 45, 74, 47, 54, 31, 18,  2,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 1, 20, 70, 14,  6, 58, 90, 30, 17, 91, 18, 90,  2,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 1, 75, 13, 22, 77, 89, 21, 13, 86, 95,  2,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 1,  8, 85, 87, 77, 47, 21, 23, 98, 83,  4, 47, 97, 40, 43, 70,  8, 65,\n",
      "         71, 69, 88,  2],\n",
      "        [ 1, 79, 14, 91, 41, 32, 79, 88, 34,  8, 68, 32, 77, 58,  7,  9, 87,  2,\n",
      "          0,  0,  0,  0],\n",
      "        [ 1, 16, 98, 68, 57, 55, 46, 66, 85, 18,  2,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 1, 37, 93, 98, 13, 45, 28, 89, 72, 70,  2,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 1, 85,  8, 50, 30,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 1, 47, 30,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0]])\n",
      "tensor([12, 14, 13, 11, 22, 18, 11, 11,  6,  4])\n"
     ]
    }
   ],
   "source": [
    "src_batch_lens, sorted_idx = src_batch_lens.sort(descending=True)\n",
    "src_batch = src_batch[sorted_idx]\n",
    "trg_batch = trg_batch[sorted_idx]\n",
    "trg_batch_lens = trg_batch_lens[sorted_idx]\n",
    "\n",
    "print(src_batch)\n",
    "print(src_batch_lens)\n",
    "print(trg_batch)\n",
    "print(trg_batch_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emD3bFjS2vEn"
   },
   "source": [
    "### **Encoder 구현**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5k9sSui29yP"
   },
   "source": [
    "Encoder 역시 기존 Seq2seq 모델과 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MmhCME-PDUJ8"
   },
   "outputs": [],
   "source": [
    "embedding_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "num_dirs = 2\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "epZDaDO-FMPu"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "    self.gru = nn.GRU(\n",
    "        input_size=embedding_size, \n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        bidirectional=True if num_dirs > 1 else False,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    self.linear = nn.Linear(num_dirs * hidden_size, hidden_size)\n",
    "\n",
    "  def forward(self, batch, batch_lens):  # batch: (B, S_L), batch_lens: (B)\n",
    "    # d_w: word embedding size\n",
    "    batch_emb = self.embedding(batch)  # (B, S_L, d_w)\n",
    "    batch_emb = batch_emb.transpose(0, 1)  # (S_L, B, d_w)\n",
    "\n",
    "    packed_input = pack_padded_sequence(batch_emb, batch_lens)\n",
    "\n",
    "    h_0 = torch.zeros((num_layers * num_dirs, batch.shape[0], hidden_size))  # (num_layers*num_dirs, B, d_h) = (4, B, d_h)\n",
    "    packed_outputs, h_n = self.gru(packed_input, h_0)  # h_n: (4, B, d_h)\n",
    "    outputs = pad_packed_sequence(packed_outputs)[0]  # outputs: (S_L, B, 2d_h)\n",
    "    outputs = torch.tanh(self.linear(outputs))  # (S_L, B, d_h)\n",
    "\n",
    "    forward_hidden = h_n[-2, :, :]\n",
    "    backward_hidden = h_n[-1, :, :]\n",
    "    hidden = torch.tanh(self.linear(torch.cat((forward_hidden, backward_hidden), dim=-1))).unsqueeze(0)  # (1, B, d_h)\n",
    "\n",
    "    return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZEdSnKZkIedk"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4w5G0uy4TiFA"
   },
   "source": [
    "### **Dot-product Attention 구현**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-sPMEBEcRqP"
   },
   "source": [
    "우선 대표적인 attention 형태 중 하나인 Dot-product Attention은 다음과 같이 구현할 수 있습니다.\n",
    "\n",
    "Dot-product Attention에 사용되는 query, key, value는 별도의 linear transformation을 거치지 않습니다.\n",
    "\n",
    "- query: `decoder_hidden`\n",
    "    * `decoder_hidden`의 첫 번째 차원을 `squeeze`해서 만든 (batch_size, hidden_dimension) 크기의 벡터\n",
    "- key: `encoder_outputs`\n",
    "    * `encoder_output`의 첫 번째, 두 번째 차원을 `transpose`해서 만든 (batch_size, sentence_length, hidden_dimension) 크기의 벡터\n",
    "- value: `encoder_outputs`\n",
    "    * key 벡터와 attention score(query 벡터 기반)의 행렬곱으로 만든 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "sEB-og7IcYN6"
   },
   "outputs": [],
   "source": [
    "class DotAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs): # decoder_hidden: (1, B, d_h), encoder_outputs: (S_L, B, d_h)\n",
    "        # 디코더 측의 hidden state를 query 벡터로 변환\n",
    "        query = decoder_hidden.squeeze(0)  # (B, d_h)\n",
    "\n",
    "        # 인코더 측의 출력을 key 벡터로 변환\n",
    "        key = encoder_outputs.transpose(0, 1)  # (B, S_L, d_h)\n",
    "\n",
    "        # \"energy\" 라는 용어는 Attention 메커니즘이 얼마나 강하게 작동하는지를 나타내는 값입니다. \n",
    "        # 이 값이 높을수록, 해당 위치의 입력에 더 많은 \"에너지\"가 집중된다고 생각할 수 있습니다. \n",
    "        # 따라서 \"energy\" 라는 용어는 해당 위치에 대한 중요도를 나타내는 값이라고 생각할 수 있습니다. \n",
    "        # query와 key 벡터의 dot product를 이용하여 energy 벡터를 계산\n",
    "        energy = torch.sum(torch.mul(key, query.unsqueeze(1)), dim=-1)  # (B, S_L)\n",
    "\n",
    "        # energy 벡터의 softmax를 이용하여 attention score를 계산\n",
    "        attn_scores = F.softmax(energy, dim=-1)  # (B, S_L)\n",
    "\n",
    "        # attention score를 이용하여 encoder_outputs에서 weight를 적용한 후,\n",
    "        # 그 결과를 더하여 attention value를 계산\n",
    "        attn_values = torch.sum(torch.mul(encoder_outputs.transpose(0, 1), attn_scores.unsqueeze(2)), dim=1)  # (B, d_h)\n",
    "\n",
    "        # attention value와 attention score를 반환\n",
    "        return attn_values, attn_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cIARwx4IjuuG"
   },
   "outputs": [],
   "source": [
    "dot_attn = DotAttention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r94WCkbCjMnz"
   },
   "source": [
    "이제 이 attention 모듈을 가지는 Decoder 클래스를 구현하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9JycRs0ojLyg"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, attention):\n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "    self.attention = attention\n",
    "    self.rnn = nn.GRU(\n",
    "        embedding_size,\n",
    "        hidden_size\n",
    "    )\n",
    "    self.output_linear = nn.Linear(2*hidden_size, vocab_size)\n",
    "\n",
    "  def forward(self, batch, encoder_outputs, hidden):  # batch: (B), encoder_outputs: (L, B, d_h), hidden: (1, B, d_h)  \n",
    "    batch_emb = self.embedding(batch)  # (B, d_w)\n",
    "    batch_emb = batch_emb.unsqueeze(0)  # (1, B, d_w)\n",
    "\n",
    "    outputs, hidden = self.rnn(batch_emb, hidden)  # (1, B, d_h), (1, B, d_h)\n",
    "\n",
    "    attn_values, attn_scores = self.attention(hidden, encoder_outputs)  # (B, d_h), (B, S_L)\n",
    "    concat_outputs = torch.cat((outputs, attn_values.unsqueeze(0)), dim=-1)  # (1, B, 2d_h)\n",
    "\n",
    "    return self.output_linear(concat_outputs).squeeze(0), hidden  # (B, V), (1, B, d_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "45GG2CvOjwzE"
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(dot_attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZ1NzYZROrOu"
   },
   "source": [
    "### **Seq2seq 모델 구축**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEYvsQS0Ovp6"
   },
   "source": [
    "- 최종적으로 seq2seq 모델을 다음과 같이 구성할 수 있습니다.\n",
    "- Dot-product attention이 추가된 것 이외에 5강의 Seq2Seq 모델과 차이는 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "M52xKNVeF37N"
   },
   "outputs": [],
   "source": [
    "class Seq2seq(nn.Module):\n",
    "  def __init__(self, encoder, decoder):\n",
    "    super(Seq2seq, self).__init__()\n",
    "\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "\n",
    "  def forward(self, src_batch, src_batch_lens, trg_batch, teacher_forcing_prob=0.5):\n",
    "    # src_batch: (B, S_L), src_batch_lens: (B), trg_batch: (B, T_L)\n",
    "\n",
    "    encoder_outputs, hidden = self.encoder(src_batch, src_batch_lens)  # encoder_outputs: (S_L, B, d_h), hidden: (1, B, d_h)\n",
    "\n",
    "    input_ids = trg_batch[:, 0]  # (B)\n",
    "    batch_size = src_batch.shape[0]\n",
    "    outputs = torch.zeros(trg_max_len, batch_size, vocab_size)  # (T_L, B, V)\n",
    "\n",
    "    for t in range(1, trg_max_len):\n",
    "      decoder_outputs, hidden = self.decoder(input_ids, encoder_outputs, hidden)  # decoder_outputs: (B, V), hidden: (1, B, d_h)\n",
    "\n",
    "      outputs[t] = decoder_outputs\n",
    "      _, top_ids = torch.max(decoder_outputs, dim=-1)  # top_ids: (B)\n",
    "\n",
    "      input_ids = trg_batch[:, t] if random.random() > teacher_forcing_prob else top_ids\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "wNv7wlRgPIYS"
   },
   "outputs": [],
   "source": [
    "seq2seq = Seq2seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFwbnxd7PVNf"
   },
   "source": [
    "### **모델 사용해보기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIprc5N2jaV2"
   },
   "source": [
    "만든 모델로 결과를 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKdTDHqsiLbs",
    "outputId": "95ea0540-7253-42e8-86c1-563eb8998ef9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 3.3084e-02,  3.1216e-02,  2.8491e-02,  ..., -1.4977e-01,\n",
      "           2.2401e-02, -7.2737e-02],\n",
      "         [ 3.8224e-02, -1.3414e-02,  3.9262e-02,  ..., -1.8062e-01,\n",
      "           3.3306e-02, -6.1789e-02],\n",
      "         [ 6.5702e-02,  4.3560e-02,  4.7014e-02,  ..., -1.5557e-01,\n",
      "           2.4744e-02, -7.9309e-02],\n",
      "         ...,\n",
      "         [ 1.4452e-02, -6.6513e-03,  1.6447e-02,  ..., -1.8813e-01,\n",
      "          -2.8704e-02, -2.6259e-02],\n",
      "         [ 4.7393e-02,  2.8136e-02,  2.0407e-02,  ..., -1.9304e-01,\n",
      "           2.3092e-02, -5.6434e-02],\n",
      "         [ 1.6567e-02,  2.4093e-02,  4.7867e-02,  ..., -1.8116e-01,\n",
      "           1.6898e-02, -4.8500e-02]],\n",
      "\n",
      "        [[-1.9515e-02,  1.1773e-01,  4.6173e-02,  ..., -6.3560e-02,\n",
      "          -1.2314e-01, -2.2899e-03],\n",
      "         [ 3.3211e-02,  6.8346e-03,  4.0661e-02,  ..., -1.3567e-01,\n",
      "           8.0363e-03, -1.0108e-01],\n",
      "         [-1.3149e-02,  1.2053e-01,  5.7748e-02,  ..., -7.5768e-02,\n",
      "          -1.1426e-01,  8.0101e-03],\n",
      "         ...,\n",
      "         [-3.4308e-02,  9.5365e-02,  4.4442e-02,  ..., -1.0244e-01,\n",
      "          -1.5350e-01,  3.9697e-02],\n",
      "         [ 1.6473e-01,  1.6939e-04,  8.9682e-02,  ...,  3.0221e-02,\n",
      "          -1.5566e-02, -1.8216e-01],\n",
      "         [ 1.5479e-01, -7.0021e-03,  1.0427e-01,  ...,  3.5009e-02,\n",
      "          -1.7474e-02, -1.7555e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.8967e-02,  5.1458e-02,  5.1078e-02,  ...,  1.6635e-02,\n",
      "          -1.3190e-02, -2.2809e-01],\n",
      "         [ 3.5742e-02,  2.3340e-02,  9.2981e-02,  ...,  1.2049e-02,\n",
      "           3.8391e-03, -1.9711e-01],\n",
      "         [ 2.4392e-02,  5.1757e-02,  5.0466e-02,  ..., -9.6016e-03,\n",
      "          -1.7128e-02, -2.1019e-01],\n",
      "         ...,\n",
      "         [ 1.4128e-02,  4.6680e-02,  6.9531e-02,  ..., -1.2847e-02,\n",
      "          -2.2808e-02, -2.0392e-01],\n",
      "         [ 1.3156e-02,  5.2199e-02,  6.9614e-02,  ..., -1.2604e-02,\n",
      "          -1.8322e-02, -2.1503e-01],\n",
      "         [ 1.7028e-02,  4.9473e-02,  7.1190e-02,  ..., -1.1051e-02,\n",
      "          -1.6698e-02, -2.1562e-01]],\n",
      "\n",
      "        [[ 6.6532e-02, -8.3641e-03,  1.8992e-03,  ..., -9.5292e-02,\n",
      "           5.0149e-03, -2.1222e-01],\n",
      "         [ 7.4748e-02, -3.2660e-02,  4.1774e-02,  ..., -1.0922e-01,\n",
      "           1.8581e-02, -1.8220e-01],\n",
      "         [ 6.8706e-02, -7.3720e-03,  3.8253e-03,  ..., -1.2033e-01,\n",
      "           1.3317e-03, -1.9530e-01],\n",
      "         ...,\n",
      "         [ 5.9568e-02, -1.3381e-02,  1.8703e-02,  ..., -1.2759e-01,\n",
      "          -5.7936e-03, -1.8491e-01],\n",
      "         [ 5.6600e-02, -5.6778e-03,  1.9634e-02,  ..., -1.2737e-01,\n",
      "          -4.5133e-03, -1.9457e-01],\n",
      "         [ 6.1228e-02, -9.3189e-03,  2.1770e-02,  ..., -1.2549e-01,\n",
      "          -1.6028e-03, -1.9549e-01]],\n",
      "\n",
      "        [[ 8.8821e-02, -4.8800e-02, -1.0856e-02,  ..., -1.7170e-01,\n",
      "           1.6770e-02, -2.0948e-01],\n",
      "         [ 9.2316e-02, -7.1090e-02,  2.6415e-02,  ..., -1.8874e-01,\n",
      "           3.1428e-02, -1.8029e-01],\n",
      "         [ 8.9053e-02, -4.7805e-02, -7.5827e-03,  ..., -1.9524e-01,\n",
      "           1.5207e-02, -1.9329e-01],\n",
      "         ...,\n",
      "         [ 8.0453e-02, -5.3707e-02,  5.1223e-03,  ..., -2.0390e-01,\n",
      "           6.5675e-03, -1.8095e-01],\n",
      "         [ 7.6522e-02, -4.4457e-02,  6.9433e-03,  ..., -2.0376e-01,\n",
      "           6.0126e-03, -1.9018e-01],\n",
      "         [ 8.1619e-02, -4.8692e-02,  9.4210e-03,  ..., -2.0165e-01,\n",
      "           9.6837e-03, -1.9135e-01]]], grad_fn=<CopySlices>)\n",
      "torch.Size([22, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "# V: vocab size\n",
    "outputs = seq2seq(src_batch, src_batch_lens, trg_batch)  # (T_L, B, V)\n",
    "\n",
    "print(outputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "R-wAEwi9dy0Q"
   },
   "outputs": [],
   "source": [
    "sample_sent = [4, 10, 88, 46, 72, 34, 14, 51]\n",
    "sample_len = len(sample_sent)\n",
    "\n",
    "sample_batch = torch.LongTensor(sample_sent).unsqueeze(0)  # (1, L)\n",
    "sample_batch_len = torch.LongTensor([sample_len])  # (1)\n",
    "\n",
    "encoder_output, hidden = seq2seq.encoder(sample_batch, sample_batch_len)  # hidden: (4, 1, d_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-ywRSK1iTn1U"
   },
   "outputs": [],
   "source": [
    "input_id = torch.LongTensor([sos_id]) # (1)\n",
    "output = []\n",
    "\n",
    "for t in range(1, trg_max_len):\n",
    "  decoder_output, hidden = seq2seq.decoder(input_id, encoder_output, hidden)  # decoder_output: (1, V), hidden: (4, 1, d_h)\n",
    "\n",
    "  _, top_id = torch.max(decoder_output, dim=-1)  # top_ids: (1)\n",
    "\n",
    "  if top_id == eos_id:\n",
    "    break\n",
    "  else:\n",
    "    output += top_id.tolist()\n",
    "    input_id = top_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pP_A4ZrhTXik",
    "outputId": "c0f5e4e6-8792-47ec-c651-1191c9699fa4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[65,\n",
       " 31,\n",
       " 26,\n",
       " 5,\n",
       " 5,\n",
       " 75,\n",
       " 32,\n",
       " 32,\n",
       " 25,\n",
       " 75,\n",
       " 0,\n",
       " 67,\n",
       " 27,\n",
       " 37,\n",
       " 62,\n",
       " 72,\n",
       " 72,\n",
       " 70,\n",
       " 42,\n",
       " 47,\n",
       " 97]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4TZfceq3Nbs"
   },
   "source": [
    "### **Concat Attention 구현**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYxpAQjm3Y9U"
   },
   "source": [
    "Bahdanau Attention이라고도 불리는 Concat Attention을 구현해보도록 하겠습니다.  \n",
    "\n",
    "\n",
    "*   `self.w`: Concat한 query와 key 벡터를 1차적으로 linear transformation.\n",
    "*   `self.v`: Attention logit 값을 계산.\n",
    "\n",
    "- Dot-product attention과 다르게 Concat Attention은 query와 key 벡터를 `self.w` matrix를 통해 linear transformation하여 사용합니다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "MHRfFeIzJJU7"
   },
   "outputs": [],
   "source": [
    "class ConcatAttention(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.w = nn.Linear(2*hidden_size, hidden_size, bias=False)\n",
    "    self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "  def forward(self, decoder_hidden, encoder_outputs):  # (1, B, d_h), (S_L, B, d_h)\n",
    "    src_max_len = encoder_outputs.shape[0]\n",
    "\n",
    "    decoder_hidden = decoder_hidden.transpose(0, 1).repeat(1, src_max_len, 1)  # (B, S_L, d_h)\n",
    "    encoder_outputs = encoder_outputs.transpose(0, 1)  # (B, S_L, d_h)\n",
    "\n",
    "    concat_hiddens = torch.cat((decoder_hidden, encoder_outputs), dim=2)  # (B, S_L, 2d_h)\n",
    "    energy = torch.tanh(self.w(concat_hiddens))  # (B, S_L, d_h)\n",
    "\n",
    "    attn_scores = F.softmax(self.v(energy), dim=1)  # (B, S_L, 1)\n",
    "    attn_values = torch.sum(torch.mul(encoder_outputs, attn_scores), dim=1)  # (B, d_h)\n",
    "\n",
    "    return attn_values, attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "utm4b5uzNS40"
   },
   "outputs": [],
   "source": [
    "concat_attn = ConcatAttention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBBCV9G-M1cw"
   },
   "source": [
    "마찬가지로 decoder를 마저 구현하여 모델 결과를 산출하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "FnppmsXNSaGP"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, attention):\n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "    self.attention = attention\n",
    "    self.rnn = nn.GRU(\n",
    "        embedding_size + hidden_size,\n",
    "        hidden_size\n",
    "    )\n",
    "    self.output_linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "  def forward(self, batch, encoder_outputs, hidden):  # batch: (B), encoder_outputs: (S_L, B, d_h), hidden: (1, B, d_h)  \n",
    "    batch_emb = self.embedding(batch)  # (B, d_w)\n",
    "    batch_emb = batch_emb.unsqueeze(0)  # (1, B, d_w)\n",
    "\n",
    "    attn_values, attn_scores = self.attention(hidden, encoder_outputs)  # (B, d_h), (B, S_L)\n",
    "\n",
    "    concat_emb = torch.cat((batch_emb, attn_values.unsqueeze(0)), dim=-1)  # (1, B, d_w+d_h)\n",
    "\n",
    "    outputs, hidden = self.rnn(concat_emb, hidden)  # (1, B, d_h), (1, B, d_h)\n",
    "\n",
    "    return self.output_linear(outputs).squeeze(0), hidden  # (B, V), (1, B, d_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "4gA4GJqgOoMT"
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(concat_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "AQI9J0VGj4cc"
   },
   "outputs": [],
   "source": [
    "seq2seq = Seq2seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6suDtTxj4ce",
    "outputId": "e733e6c7-5565-409b-9d11-62250c88d33f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.3229, -0.0595, -0.0782,  ...,  0.0176, -0.2225, -0.0244],\n",
      "         [ 0.3277, -0.0949, -0.0778,  ...,  0.0699, -0.1832,  0.0025],\n",
      "         [ 0.3364, -0.0512, -0.1276,  ...,  0.0222, -0.1960, -0.0237],\n",
      "         ...,\n",
      "         [ 0.3551, -0.0561, -0.0897,  ...,  0.0580, -0.1976, -0.0252],\n",
      "         [ 0.3213, -0.0470, -0.0907,  ...,  0.0784, -0.2100,  0.0056],\n",
      "         [ 0.3185, -0.0723, -0.0956,  ...,  0.0609, -0.2113, -0.0073]],\n",
      "\n",
      "        [[ 0.1213,  0.1417, -0.0698,  ..., -0.0321, -0.0520,  0.0336],\n",
      "         [ 0.1691,  0.0150,  0.0155,  ..., -0.0409, -0.2567,  0.0706],\n",
      "         [ 0.1799,  0.0321, -0.0511,  ..., -0.0092, -0.0129,  0.0098],\n",
      "         ...,\n",
      "         [ 0.1377,  0.1406, -0.0703,  ..., -0.0143, -0.0435,  0.0442],\n",
      "         [ 0.2764, -0.0138, -0.1828,  ...,  0.0772, -0.0160,  0.3411],\n",
      "         [ 0.0212,  0.0224,  0.0022,  ...,  0.0924, -0.1474, -0.2023]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0737, -0.0371,  0.0302,  ..., -0.1859,  0.0853, -0.1096],\n",
      "         [-0.0979, -0.2184,  0.3447,  ..., -0.2175, -0.0229,  0.0213],\n",
      "         [ 0.0665, -0.1496,  0.0770,  ..., -0.1659, -0.0303,  0.1365],\n",
      "         ...,\n",
      "         [-0.0873, -0.2162,  0.3552,  ..., -0.2253, -0.0320,  0.0121],\n",
      "         [-0.0874, -0.2264,  0.3611,  ..., -0.2226, -0.0253,  0.0030],\n",
      "         [-0.0794, -0.2085,  0.3567,  ..., -0.2238, -0.0349,  0.0170]],\n",
      "\n",
      "        [[ 0.0664,  0.0971,  0.0386,  ..., -0.0447,  0.0509, -0.0361],\n",
      "         [ 0.0264, -0.0568,  0.2047,  ..., -0.0433, -0.0151,  0.0048],\n",
      "         [ 0.1079, -0.0222,  0.0648,  ..., -0.0034, -0.0459,  0.0570],\n",
      "         ...,\n",
      "         [ 0.0392, -0.0571,  0.2151,  ..., -0.0515, -0.0235, -0.0025],\n",
      "         [ 0.0412, -0.0622,  0.2173,  ..., -0.0480, -0.0232, -0.0070],\n",
      "         [ 0.0460, -0.0494,  0.2177,  ..., -0.0479, -0.0278,  0.0041]],\n",
      "\n",
      "        [[-0.0807, -0.0751,  0.2994,  ..., -0.1701,  0.0366, -0.0920],\n",
      "         [-0.1069, -0.1779,  0.3816,  ..., -0.1690, -0.0218, -0.0584],\n",
      "         [-0.0573, -0.1683,  0.3249,  ..., -0.1428, -0.0476, -0.0382],\n",
      "         ...,\n",
      "         [-0.0992, -0.1765,  0.3914,  ..., -0.1772, -0.0300, -0.0671],\n",
      "         [-0.0963, -0.1794,  0.3954,  ..., -0.1734, -0.0313, -0.0698],\n",
      "         [-0.0922, -0.1698,  0.3946,  ..., -0.1737, -0.0346, -0.0619]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "torch.Size([22, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "outputs = seq2seq(src_batch, src_batch_lens, trg_batch)\n",
    "\n",
    "print(outputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5h6WsyFV8W3-"
   },
   "source": [
    "###**콘텐츠 라이선스**\n",
    "\n",
    "<font color='red'><b>**WARNING**</b></font> : **본 교육 콘텐츠의 지식재산권은 재단법인 네이버커넥트에 귀속됩니다. 본 콘텐츠를 어떠한 경로로든 외부로 유출 및 수정하는 행위를 엄격히 금합니다.** 다만, 비영리적 교육 및 연구활동에 한정되어 사용할 수 있으나 재단의 허락을 받아야 합니다. 이를 위반하는 경우, 관련 법률에 따라 책임을 질 수 있습니다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "ml2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "aff83ea1928dcc0287770e0ea68916ae9727a33d95a26ca69018331bcc2781f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
