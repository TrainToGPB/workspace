{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yepEfmD1se47"
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## ì‹¬í™”ê³¼ì œ 1: BERT Fine-tunning with Transformers\n",
    "\n",
    "> ë³¸ ê³¼ì œëŠ” NLP ì‹¬í™”êµ¬í˜„ í•´ë³´ê³ ì í•˜ëŠ” ì‚¬ëŒë“¤ì„ ìœ„í•œ ê³¼ì œì…ë‹ˆë‹¤.\n",
    ">\n",
    "> ì •ë‹µì´ë‚˜ Reference ì½”ë“œê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ê°€ëŠ¥í•œ ê³³ê¹Œì§€ ë„ì „í•´ë³´ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28SIeZN6ZODh"
   },
   "source": [
    "### Introduction\n",
    "\n",
    "* ë³¸ ê³¼ì œëŠ” imdb ì˜í™” ë¦¬ë·° ë°ì´í„°ì— ëŒ€í•´ pretrain ëª¨ë¸ì„ finetuningí•˜ëŠ” ê³¼ì œì…ë‹ˆë‹¤.\n",
    "* ì˜í™” ë¦¬ë·°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ê¸ì •ì ì¸ ë¦¬ë·°ì¸ì§€ ë¶€ì •ì ì¸ ë¦¬ë·°ì¸ì§€ íŒë³„í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ ë´…ì‹œë‹¤.\n",
    "* ì´ë²ˆ ì‹œê°„ì—ì€ ì‚°í•™ê³„ì—ì„œ ì‹¤ì œë¡œ ë§ì´ ì“°ì´ëŠ” [Transformer](https://huggingface.co/docs/transformers/index) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ë³´ê² ìŠµë‹ˆë‹¤. í•´ë‹¹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì§ì ‘ ì°¸ê³ í•˜ë©´ì„œ ëª©í‘œ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.\n",
    "* ëª¨ë¸, ì´ˆë§¤ê°œë³€ìˆ˜ (hyperparamter) ë“±ë“±ì„ ë°”ê¾¸ë©° finetuningì„ ì§„í–‰í•´ì„œ, í…ŒìŠ¤íŠ¸ ì •í™•ë„ 93% ì´ìƒì„ ë„˜ê²¨ë³´ì„¸ìš”!\n",
    "* ì°¸ê³  1) https://huggingface.co/transformers/\n",
    "* ì°¸ê³  2) https://paperswithcode.com/sota/text-classification-on-imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uZlmS5Zse5A"
   },
   "source": [
    "### 0. í™˜ê²½ ì…‹íŒ… ë° ë°ì´í„° ì—…ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Bj0vJ19E-DX7"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "D2A0WCpP8Qro"
   },
   "outputs": [],
   "source": [
    "# !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# !tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kod3el3Rse5D"
   },
   "source": [
    "### 1. ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "w9Fr-Tgj8Qnt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 19:03:35.354392: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertConfig\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UuYi6KYvF7Mw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:8: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/tmp/ipykernel_5948/3331667923.py:8: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  labels.append(0 if label_dir is \"neg\" else 1)\n"
     ]
    }
   ],
   "source": [
    "def read_imdb_split(split_dir):\n",
    "    split_dir = Path(split_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label_dir in [\"pos\", \"neg\"]:\n",
    "        for text_file in (split_dir/label_dir).iterdir():\n",
    "            texts.append(text_file.read_text())\n",
    "            labels.append(0 if label_dir is \"neg\" else 1)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "train_texts, train_labels = read_imdb_split('aclImdb/train')\n",
    "test_texts, test_labels = read_imdb_split('aclImdb/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EQ4Zq1L-8QlR"
   },
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeQbmq3Hse5F"
   },
   "source": [
    "í† í°í™”ê¸°ëŠ” `BERT`ì—ì„œ ì‚¬ìš©í•˜ëŠ” í† í°í™”ê¸°ë¥¼ ì‚¬ìš©í•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zVpV5Y2c8Qi5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b927117344a4070a79264489e949cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25dcf4d5bce3426c8da104a406ed3d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ea6b2a05384140ab4281b132e409d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0399adf9b0324f5ca2140d7306155da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "frpVUknK8Qcm"
   },
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "x_Sz5N8C8cYr"
   },
   "outputs": [],
   "source": [
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "val_dataset = IMDbDataset(val_encodings, val_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 5000 25000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset), len(val_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H17W7gQRse5H"
   },
   "source": [
    "### 2. ëª¨ë¸ ì‘ì„± ë° í•™ìŠµ\n",
    "ëª¨ë¸ì€ ì‚¬ì „í•™ìŠµëœ `BERT`ë¥¼ ì¦ë¥˜ (Dilstilation) ê³¼ì •ì„ í†µí•´ ëª¨ë¸ í¬ê¸°ë¥¼ ì¤„ì¸ `DistilBERT`ë¥¼ ì‚¬ìš©í•´ë³´ê² ìŠµë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "UC2XqoGwV7nD"
   },
   "outputs": [],
   "source": [
    "config = DistilBertConfig.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    vocab_size=30522, max_position_embeddings=512, sinusoidal_pos_embds=False,\n",
    "    n_layers=6, n_heads=12, dim=768, hidden_dim=3072,\n",
    "    dropout=0.1, attention_dropout=0.1, activation='gelu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "sQt9Kpg-8eff"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f1d31ab1444732b6772e3c3d906e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/kingstar/anaconda3/envs/ml2/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 07:39, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.669600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.348000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.351000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.303800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.289100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.287200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.283500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.316400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.242100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.252600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.214700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=0.3134173599243164, metrics={'train_runtime': 460.832, 'train_samples_per_second': 43.4, 'train_steps_per_second': 2.712, 'total_flos': 2649347973120000.0, 'train_loss': 0.3134173599243164, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # ì¶œë ¥ í´ë”\n",
    "    num_train_epochs=1,              # í•™ìŠµ ì—í­ ìˆ˜\n",
    "    per_device_train_batch_size=16,  # GPUë‹¹ í•™ìŠµ ë°°ì¹˜ í¬ê¸°\n",
    "    per_device_eval_batch_size=64,   # GPUë‹¹ í‰ê°€ ë°°ì¹˜ í¬ê¸°ê¸°\n",
    "    warmup_steps=500,                # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ì„ ìœ„í•œ warm up ê³¼ì • ìŠ¤í… ìˆ˜. ì´ë™ì•ˆì€ í•™ìŠµë¥ ì´ ì²œì²œíˆ ì˜¬ë¼ê°„ë‹¤.\n",
    "    weight_decay=0.01,               # ê°€ì¤‘ì¹˜ ê°ì‡  (weight decay)\n",
    "    logging_dir='./logs',            # ë¡œê·¸ ê¸°ë¡ì„ ìœ„í•œ í´ë”\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", config=config)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # í•™ìŠµí•  ëª¨ë¸\n",
    "    args=training_args,                  # í•™ìŠµ ì¸ì\n",
    "    train_dataset=train_dataset,         # í•™ìŠµ ë°ì´í„° ì…‹\n",
    "    eval_dataset=val_dataset             # í‰ê°€ ë°ì´í„° ì…‹\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSnzJVcose5I"
   },
   "source": [
    "### 3. í‰ê°€ ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "eySMBBKg8h3k"
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "zQK26wT9_kmd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5948/957560054.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric= load_metric(\"accuracy\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c0fafd6d064569883d83554b29663b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [03:02<00:00,  1.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9258}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric= load_metric(\"accuracy\")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128)\n",
    "model.eval()\n",
    "for batch in tqdm(test_dataloader):\n",
    "    batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZDNiHqfbZhG"
   },
   "source": [
    "###**ì½˜í…ì¸  ë¼ì´ì„ ìŠ¤**\n",
    "\n",
    "<font color='red'><b>**WARNING**</b></font> : **ë³¸ êµìœ¡ ì½˜í…ì¸ ì˜ ì§€ì‹ì¬ì‚°ê¶Œì€ ì¬ë‹¨ë²•ì¸ ë„¤ì´ë²„ì»¤ë„¥íŠ¸ì— ê·€ì†ë©ë‹ˆë‹¤. ë³¸ ì½˜í…ì¸ ë¥¼ ì–´ë– í•œ ê²½ë¡œë¡œë“  ì™¸ë¶€ë¡œ ìœ ì¶œ ë° ìˆ˜ì •í•˜ëŠ” í–‰ìœ„ë¥¼ ì—„ê²©íˆ ê¸ˆí•©ë‹ˆë‹¤.** ë‹¤ë§Œ, ë¹„ì˜ë¦¬ì  êµìœ¡ ë° ì—°êµ¬í™œë™ì— í•œì •ë˜ì–´ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë‚˜ ì¬ë‹¨ì˜ í—ˆë½ì„ ë°›ì•„ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„ë°˜í•˜ëŠ” ê²½ìš°, ê´€ë ¨ ë²•ë¥ ì— ë”°ë¼ ì±…ì„ì„ ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "ml2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "aff83ea1928dcc0287770e0ea68916ae9727a33d95a26ca69018331bcc2781f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
