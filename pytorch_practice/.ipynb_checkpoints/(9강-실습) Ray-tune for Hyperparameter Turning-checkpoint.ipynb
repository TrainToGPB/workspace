{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dnckgcpbdYT"
   },
   "source": [
    "## Ray-tune for Hyperparameter Turning\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/BoostcampAITech/lecture-note-python-basics-for-ai/blob/main/codes/pytorch/07_torch-study/ray-tune/ray_tune.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ot5y7LaCverD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ray\n",
      "  Downloading ray-2.3.0-cp310-cp310-manylinux2014_x86_64.whl (58.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.3 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from ray) (1.23.5)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.9.1-py3-none-any.whl (9.7 kB)\n",
      "Collecting virtualenv>=20.0.24\n",
      "  Downloading virtualenv-20.21.0-py3-none-any.whl (8.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs in /home/kingstar/.local/lib/python3.10/site-packages (from ray) (22.2.0)\n",
      "Requirement already satisfied: packaging in /home/kingstar/.local/lib/python3.10/site-packages (from ray) (23.0)\n",
      "Collecting aiosignal\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting protobuf!=3.19.5,>=3.15.3\n",
      "  Downloading protobuf-4.22.1-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting msgpack<2.0.0,>=1.0.0\n",
      "  Downloading msgpack-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.8/316.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jsonschema in /home/kingstar/.local/lib/python3.10/site-packages (from ray) (4.17.3)\n",
      "Collecting click>=7.0\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist\n",
      "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from ray) (2.28.1)\n",
      "Collecting grpcio>=1.42.0\n",
      "  Downloading grpcio-1.51.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyyaml\n",
      "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting distlib<1,>=0.3.6\n",
      "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: platformdirs<4,>=2.4 in /home/kingstar/.local/lib/python3.10/site-packages (from virtualenv>=20.0.24->ray) (3.1.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/kingstar/.local/lib/python3.10/site-packages (from jsonschema->ray) (0.19.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from requests->ray) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from requests->ray) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kingstar/.local/lib/python3.10/site-packages (from requests->ray) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from requests->ray) (2.0.4)\n",
      "Installing collected packages: msgpack, distlib, pyyaml, protobuf, grpcio, frozenlist, filelock, click, virtualenv, aiosignal, ray\n",
      "Successfully installed aiosignal-1.3.1 click-8.1.3 distlib-0.3.6 filelock-3.9.1 frozenlist-1.3.3 grpcio-1.51.3 msgpack-1.0.5 protobuf-4.22.1 pyyaml-6.0 ray-2.3.0 virtualenv-20.21.0\n"
     ]
    }
   ],
   "source": [
    "! pip install ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hbnXPWknqV4H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\n",
      "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/kingstar/.local/lib/python3.10/site-packages (from tensorboardX) (23.0)\n",
      "Requirement already satisfied: numpy in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from tensorboardX) (1.23.5)\n",
      "Collecting protobuf<4,>=3.8.0\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf, tensorboardX\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.22.1\n",
      "    Uninstalling protobuf-4.22.1:\n",
      "      Successfully uninstalled protobuf-4.22.1\n",
      "Successfully installed protobuf-3.20.3 tensorboardX-2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SqfrQN2Q3nbh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.14.0-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from wandb) (3.20.3)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/kingstar/.local/lib/python3.10/site-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: PyYAML in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: setuptools in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from wandb) (65.6.3)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from wandb) (2.28.1)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.16.0-py2.py3-none-any.whl (184 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting appdirs>=1.4.3\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from wandb) (8.1.3)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.4.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kingstar/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=0bbb219a3f0ff0a947e0a0fbdbc45f0588b51e4e36afbaa9c88445736a42608e\n",
      "  Stored in directory: /home/kingstar/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
      "Successfully built pathtools\n",
      "Installing collected packages: pathtools, appdirs, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
      "Successfully installed GitPython-3.1.31 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.16.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ray[tune] in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (2.3.0)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from ray[tune]) (3.20.3)\n",
      "Requirement already satisfied: frozenlist in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from ray[tune]) (1.3.3)\n",
      "Requirement already satisfied: pyyaml in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from ray[tune]) (6.0)\n",
      "Requirement already satisfied: aiosignal in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from ray[tune]) (1.3.1)\n",
      "Requirement already satisfied: grpcio>=1.42.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from ray[tune]) (1.51.3)\n",
      "Requirement already satisfied: jsonschema in /home/kingstar/.local/lib/python3.10/site-packages (from ray[tune]) (4.17.3)\n",
      "Requirement already satisfied: virtualenv>=20.0.24 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from ray[tune]) (20.21.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from ray[tune]) (1.0.5)\n",
      "Requirement already satisfied: packaging in /home/kingstar/.local/lib/python3.10/site-packages (from ray[tune]) (23.0)\n",
      "Requirement already satisfied: filelock in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from ray[tune]) (3.9.1)\n",
      "Requirement already satisfied: requests in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from ray[tune]) (2.28.1)\n",
      "Requirement already satisfied: attrs in /home/kingstar/.local/lib/python3.10/site-packages (from ray[tune]) (22.2.0)\n",
      "Requirement already satisfied: click>=7.0 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from ray[tune]) (8.1.3)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from ray[tune]) (1.23.5)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from ray[tune]) (2.6)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: pandas in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from ray[tune]) (1.5.3)\n",
      "Requirement already satisfied: distlib<1,>=0.3.6 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from virtualenv>=20.0.24->ray[tune]) (0.3.6)\n",
      "Requirement already satisfied: platformdirs<4,>=2.4 in /home/kingstar/.local/lib/python3.10/site-packages (from virtualenv>=20.0.24->ray[tune]) (3.1.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/kingstar/.local/lib/python3.10/site-packages (from jsonschema->ray[tune]) (0.19.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/kingstar/.local/lib/python3.10/site-packages (from pandas->ray[tune]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from pandas->ray[tune]) (2022.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from requests->ray[tune]) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from requests->ray[tune]) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kingstar/.local/lib/python3.10/site-packages (from requests->ray[tune]) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from requests->ray[tune]) (2.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->ray[tune]) (1.16.0)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ray[tune]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "p6v4ORGFvVaI"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JK3lcOwElxdF"
   },
   "outputs": [],
   "source": [
    "# 데이터 잘라내기\n",
    "def load_data(data_dir=\"./data\"):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=True, download=True, transform=transform)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "    return trainset, testset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FAg2V_8ely9p"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    # l1, l2: Elastic net에서의 l1, l2 loss가 아님\n",
    "    # 단순히 마지막 Linear layer의 크기 설정을 위한 hyperparameter\n",
    "    def __init__(self, l1=120, l2=84):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n",
    "        self.fc2 = nn.Linear(l1, l2)\n",
    "        self.fc3 = nn.Linear(l2, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "DopzqPSkowWo"
   },
   "outputs": [],
   "source": [
    "# train의 과정이 반드시 하나의 함수로 선언되어 있어야 함\n",
    "# 그래야만 ray가 이것을 학습 함수로 불러와서 tuning이 가능\n",
    "def train_cifar(config, checkpoint_dir=None, data_dir=None):\n",
    "    net = Net(config[\"l1\"], config[\"l2\"])\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    trainset, testset = load_data(data_dir)\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "    \n",
    "    # WandB로 monitoring까지 하면 더 좋음 (필수는 아님)\n",
    "    wandb.init(project='torch-turn', entity='teamlab')\n",
    "    wandb.watch(net)\n",
    "\n",
    "    for epoch in range(10):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
    "                                                running_loss / epoch_steps))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        wandb.log({\"val_loss\": val_loss})\n",
    "        wandb.log({\"loss\": loss})\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BSzH6czJpX3n"
   },
   "outputs": [],
   "source": [
    "def test_accuracy(net, device=\"cpu\"):\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Hyperparameter Tuning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j5BkGGECo5ZS",
    "outputId": "326eb2c4-97aa-4781-b92d-efdf36139894"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1998/4103270485.py:1: DeprecationWarning: The module `ray.tune.suggest` has been moved to `ray.tune.search` and the old location will be deprecated soon. Please adjust your imports to point to the new location. Example: Do a global search and replace `ray.tune.suggest` with `ray.tune.search`.\n",
      "  from ray.tune.suggest.bayesopt import BayesOptSearch\n",
      "/tmp/ipykernel_1998/4103270485.py:1: DeprecationWarning: The module `ray.tune.suggest.bayesopt` has been moved to `ray.tune.search.bayesopt` and the old location will be deprecated soon. Please adjust your imports to point to the new location. Example: Do a global search and replace `ray.tune.suggest.bayesopt` with `ray.tune.search.bayesopt`.\n",
      "  from ray.tune.suggest.bayesopt import BayesOptSearch\n",
      "/tmp/ipykernel_1998/4103270485.py:2: DeprecationWarning: The module `ray.tune.suggest.hyperopt` has been moved to `ray.tune.search.hyperopt` and the old location will be deprecated soon. Please adjust your imports to point to the new location. Example: Do a global search and replace `ray.tune.suggest.hyperopt` with `ray.tune.search.hyperopt`.\n",
      "  from ray.tune.suggest.hyperopt import HyperOptSearch\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/kingstar/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /home/kingstar/workspace/pytorch_practice/data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/kingstar/workspace/pytorch_practice/data/cifar-10-python.tar.gz to /home/kingstar/workspace/pytorch_practice/data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 11:17:12,343\tINFO worker.py:1553 -- Started a local Ray instance.\n",
      "/home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py:612: DeprecationWarning: `checkpoint_dir` in `func(config, checkpoint_dir)` is being deprecated. To save and load checkpoint in trainable functions, please use the `ray.air.session` API:\n",
      "\n",
      "from ray.air import session\n",
      "\n",
      "def train(config):\n",
      "    # ...\n",
      "    session.report({\"metric\": metric}, checkpoint=checkpoint)\n",
      "\n",
      "For more information please see https://docs.ray.io/en/master/tune/api_docs/trainable.html\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-03-15 11:17:12 (running for 00:00:00.05)\n",
      "Memory usage on this node: 2.3/15.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/8.34 GiB heap, 0.0/4.17 GiB objects\n",
      "Result logdir: /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------|\n",
      "| train_cifar_7f5ac_00000 | RUNNING  | 172.23.199.201:3668 |            8 |   32 |    8 | 0.000178243 |\n",
      "| train_cifar_7f5ac_00001 | PENDING  |                     |            8 |    8 |  256 | 0.000102774 |\n",
      "| train_cifar_7f5ac_00002 | PENDING  |                     |           16 |   32 |   64 | 0.000617828 |\n",
      "| train_cifar_7f5ac_00003 | PENDING  |                     |            2 |   16 |  128 | 0.00224033  |\n",
      "| train_cifar_7f5ac_00004 | PENDING  |                     |            8 |   32 |    8 | 0.0966673   |\n",
      "| train_cifar_7f5ac_00005 | PENDING  |                     |            8 |  256 |   16 | 0.000194578 |\n",
      "| train_cifar_7f5ac_00006 | PENDING  |                     |            4 |   64 |    4 | 0.00106146  |\n",
      "| train_cifar_7f5ac_00007 | PENDING  |                     |            2 |  256 |    4 | 0.0127225   |\n",
      "| train_cifar_7f5ac_00008 | PENDING  |                     |            4 |  256 |    4 | 0.00447093  |\n",
      "| train_cifar_7f5ac_00009 | PENDING  |                     |           16 |   64 |   64 | 0.000208139 |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m wandb: Currently logged in as: teamlab. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=3731)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=3729)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=3731)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=3729)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m wandb: Currently logged in as: teamlab. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(func pid=3729)\u001b[0m wandb: Currently logged in as: teamlab. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m wandb: Currently logged in as: teamlab. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(func pid=3731)\u001b[0m wandb: Currently logged in as: teamlab. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m wandb: Currently logged in as: teamlab. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m wandb: Tracking run with wandb version 0.14.0\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m wandb: Run data is saved locally in /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12/train_cifar_7f5ac_00000_0_batch_size=8,l1=32,l2=8,lr=0.0002_2023-03-15_11-17-12/wandb/run-20230315_111716-v00mdj86\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m wandb: Syncing run clean-disco-651\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/teamlab/torch-turn\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m wandb: 🚀 View run at https://wandb.ai/teamlab/torch-turn/runs/v00mdj86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-03-15 11:17:19 (running for 00:00:06.58)\n",
      "Memory usage on this node: 5.3/15.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 12.0/12 CPUs, 0/0 GPUs, 0.0/8.34 GiB heap, 0.0/4.17 GiB objects\n",
      "Result logdir: /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12\n",
      "Number of trials: 10/10 (4 PENDING, 6 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------|\n",
      "| train_cifar_7f5ac_00000 | RUNNING  | 172.23.199.201:3668 |            8 |   32 |    8 | 0.000178243 |\n",
      "| train_cifar_7f5ac_00001 | RUNNING  | 172.23.199.201:3729 |            8 |    8 |  256 | 0.000102774 |\n",
      "| train_cifar_7f5ac_00002 | RUNNING  | 172.23.199.201:3731 |           16 |   32 |   64 | 0.000617828 |\n",
      "| train_cifar_7f5ac_00003 | RUNNING  | 172.23.199.201:3733 |            2 |   16 |  128 | 0.00224033  |\n",
      "| train_cifar_7f5ac_00004 | RUNNING  | 172.23.199.201:3735 |            8 |   32 |    8 | 0.0966673   |\n",
      "| train_cifar_7f5ac_00005 | RUNNING  | 172.23.199.201:3737 |            8 |  256 |   16 | 0.000194578 |\n",
      "| train_cifar_7f5ac_00006 | PENDING  |                     |            4 |   64 |    4 | 0.00106146  |\n",
      "| train_cifar_7f5ac_00007 | PENDING  |                     |            2 |  256 |    4 | 0.0127225   |\n",
      "| train_cifar_7f5ac_00008 | PENDING  |                     |            4 |  256 |    4 | 0.00447093  |\n",
      "| train_cifar_7f5ac_00009 | PENDING  |                     |           16 |   64 |   64 | 0.000208139 |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m wandb: Tracking run with wandb version 0.14.0\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m wandb: Run data is saved locally in /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12/train_cifar_7f5ac_00005_5_batch_size=8,l1=256,l2=16,lr=0.0002_2023-03-15_11-17-14/wandb/run-20230315_111718-47lxc7g1\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m wandb: Syncing run wandering-dream-653\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/teamlab/torch-turn\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m wandb: 🚀 View run at https://wandb.ai/teamlab/torch-turn/runs/47lxc7g1\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m wandb: Tracking run with wandb version 0.14.0\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m wandb: Run data is saved locally in /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12/train_cifar_7f5ac_00004_4_batch_size=8,l1=32,l2=8,lr=0.0967_2023-03-15_11-17-14/wandb/run-20230315_111718-pol4dxjk\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m wandb: Syncing run colorful-music-652\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/teamlab/torch-turn\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m wandb: 🚀 View run at https://wandb.ai/teamlab/torch-turn/runs/pol4dxjk\n",
      "\u001b[2m\u001b[36m(func pid=3731)\u001b[0m wandb: Tracking run with wandb version 0.14.0\n",
      "\u001b[2m\u001b[36m(func pid=3731)\u001b[0m wandb: Run data is saved locally in /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12/train_cifar_7f5ac_00002_2_batch_size=16,l1=32,l2=64,lr=0.0006_2023-03-15_11-17-14/wandb/run-20230315_111718-wjax1dbx\n",
      "\u001b[2m\u001b[36m(func pid=3731)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(func pid=3731)\u001b[0m wandb: Syncing run playful-waterfall-653\n",
      "\u001b[2m\u001b[36m(func pid=3731)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/teamlab/torch-turn\n",
      "\u001b[2m\u001b[36m(func pid=3731)\u001b[0m wandb: 🚀 View run at https://wandb.ai/teamlab/torch-turn/runs/wjax1dbx\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m wandb: Tracking run with wandb version 0.14.0\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m wandb: Run data is saved locally in /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12/train_cifar_7f5ac_00003_3_batch_size=2,l1=16,l2=128,lr=0.0022_2023-03-15_11-17-14/wandb/run-20230315_111718-nr6465ek\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m wandb: Syncing run faithful-surf-656\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/teamlab/torch-turn\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m wandb: 🚀 View run at https://wandb.ai/teamlab/torch-turn/runs/nr6465ek\n",
      "\u001b[2m\u001b[36m(func pid=3729)\u001b[0m wandb: Tracking run with wandb version 0.14.0\n",
      "\u001b[2m\u001b[36m(func pid=3729)\u001b[0m wandb: Run data is saved locally in /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12/train_cifar_7f5ac_00001_1_batch_size=8,l1=8,l2=256,lr=0.0001_2023-03-15_11-17-14/wandb/run-20230315_111718-vmu9egdc\n",
      "\u001b[2m\u001b[36m(func pid=3729)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(func pid=3729)\u001b[0m wandb: Syncing run earnest-wood-655\n",
      "\u001b[2m\u001b[36m(func pid=3729)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/teamlab/torch-turn\n",
      "\u001b[2m\u001b[36m(func pid=3729)\u001b[0m wandb: 🚀 View run at https://wandb.ai/teamlab/torch-turn/runs/vmu9egdc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-03-15 11:17:24 (running for 00:00:11.59)\n",
      "Memory usage on this node: 5.7/15.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 12.0/12 CPUs, 0/0 GPUs, 0.0/8.34 GiB heap, 0.0/4.17 GiB objects\n",
      "Result logdir: /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12\n",
      "Number of trials: 10/10 (4 PENDING, 6 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------|\n",
      "| train_cifar_7f5ac_00000 | RUNNING  | 172.23.199.201:3668 |            8 |   32 |    8 | 0.000178243 |\n",
      "| train_cifar_7f5ac_00001 | RUNNING  | 172.23.199.201:3729 |            8 |    8 |  256 | 0.000102774 |\n",
      "| train_cifar_7f5ac_00002 | RUNNING  | 172.23.199.201:3731 |           16 |   32 |   64 | 0.000617828 |\n",
      "| train_cifar_7f5ac_00003 | RUNNING  | 172.23.199.201:3733 |            2 |   16 |  128 | 0.00224033  |\n",
      "| train_cifar_7f5ac_00004 | RUNNING  | 172.23.199.201:3735 |            8 |   32 |    8 | 0.0966673   |\n",
      "| train_cifar_7f5ac_00005 | RUNNING  | 172.23.199.201:3737 |            8 |  256 |   16 | 0.000194578 |\n",
      "| train_cifar_7f5ac_00006 | PENDING  |                     |            4 |   64 |    4 | 0.00106146  |\n",
      "| train_cifar_7f5ac_00007 | PENDING  |                     |            2 |  256 |    4 | 0.0127225   |\n",
      "| train_cifar_7f5ac_00008 | PENDING  |                     |            4 |  256 |    4 | 0.00447093  |\n",
      "| train_cifar_7f5ac_00009 | PENDING  |                     |           16 |   64 |   64 | 0.000208139 |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m [1,  2000] loss: 2.315\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m [1,  2000] loss: 2.137\n",
      "== Status ==\n",
      "Current time: 2023-03-15 11:17:29 (running for 00:00:16.59)\n",
      "Memory usage on this node: 5.8/15.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 12.0/12 CPUs, 0/0 GPUs, 0.0/8.34 GiB heap, 0.0/4.17 GiB objects\n",
      "Result logdir: /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12\n",
      "Number of trials: 10/10 (4 PENDING, 6 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------|\n",
      "| train_cifar_7f5ac_00000 | RUNNING  | 172.23.199.201:3668 |            8 |   32 |    8 | 0.000178243 |\n",
      "| train_cifar_7f5ac_00001 | RUNNING  | 172.23.199.201:3729 |            8 |    8 |  256 | 0.000102774 |\n",
      "| train_cifar_7f5ac_00002 | RUNNING  | 172.23.199.201:3731 |           16 |   32 |   64 | 0.000617828 |\n",
      "| train_cifar_7f5ac_00003 | RUNNING  | 172.23.199.201:3733 |            2 |   16 |  128 | 0.00224033  |\n",
      "| train_cifar_7f5ac_00004 | RUNNING  | 172.23.199.201:3735 |            8 |   32 |    8 | 0.0966673   |\n",
      "| train_cifar_7f5ac_00005 | RUNNING  | 172.23.199.201:3737 |            8 |  256 |   16 | 0.000194578 |\n",
      "| train_cifar_7f5ac_00006 | PENDING  |                     |            4 |   64 |    4 | 0.00106146  |\n",
      "| train_cifar_7f5ac_00007 | PENDING  |                     |            2 |  256 |    4 | 0.0127225   |\n",
      "| train_cifar_7f5ac_00008 | PENDING  |                     |            4 |  256 |    4 | 0.00447093  |\n",
      "| train_cifar_7f5ac_00009 | PENDING  |                     |           16 |   64 |   64 | 0.000208139 |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=3729)\u001b[0m [1,  2000] loss: 2.302\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m [1,  2000] loss: 2.329\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m [1,  2000] loss: 2.312\n",
      "\u001b[2m\u001b[36m(func pid=3731)\u001b[0m [1,  2000] loss: 2.297\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m [1,  4000] loss: 1.154\n",
      "== Status ==\n",
      "Current time: 2023-03-15 11:17:34 (running for 00:00:21.60)\n",
      "Memory usage on this node: 5.8/15.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 12.0/12 CPUs, 0/0 GPUs, 0.0/8.34 GiB heap, 0.0/4.17 GiB objects\n",
      "Result logdir: /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12\n",
      "Number of trials: 10/10 (4 PENDING, 6 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------|\n",
      "| train_cifar_7f5ac_00000 | RUNNING  | 172.23.199.201:3668 |            8 |   32 |    8 | 0.000178243 |\n",
      "| train_cifar_7f5ac_00001 | RUNNING  | 172.23.199.201:3729 |            8 |    8 |  256 | 0.000102774 |\n",
      "| train_cifar_7f5ac_00002 | RUNNING  | 172.23.199.201:3731 |           16 |   32 |   64 | 0.000617828 |\n",
      "| train_cifar_7f5ac_00003 | RUNNING  | 172.23.199.201:3733 |            2 |   16 |  128 | 0.00224033  |\n",
      "| train_cifar_7f5ac_00004 | RUNNING  | 172.23.199.201:3735 |            8 |   32 |    8 | 0.0966673   |\n",
      "| train_cifar_7f5ac_00005 | RUNNING  | 172.23.199.201:3737 |            8 |  256 |   16 | 0.000194578 |\n",
      "| train_cifar_7f5ac_00006 | PENDING  |                     |            4 |   64 |    4 | 0.00106146  |\n",
      "| train_cifar_7f5ac_00007 | PENDING  |                     |            2 |  256 |    4 | 0.0127225   |\n",
      "| train_cifar_7f5ac_00008 | PENDING  |                     |            4 |  256 |    4 | 0.00447093  |\n",
      "| train_cifar_7f5ac_00009 | PENDING  |                     |           16 |   64 |   64 | 0.000208139 |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m [1,  4000] loss: 0.929\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th style=\"text-align: right;\">  accuracy</th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>hostname        </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">   loss</th><th>node_ip       </th><th style=\"text-align: right;\">  pid</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_cifar_7f5ac_00000</td><td style=\"text-align: right;\">    0.0977</td><td>2023-03-15_11-17-41</td><td>True  </td><td>                </td><td>e8db405cdecc465f9c9ffa7d35e23ca0</td><td>Kingstar-Desktop</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.30576</td><td>172.23.199.201</td><td style=\"text-align: right;\"> 3668</td><td>True               </td><td style=\"text-align: right;\">             27.0859</td><td style=\"text-align: right;\">           27.0859</td><td style=\"text-align: right;\">       27.0859</td><td style=\"text-align: right;\"> 1678846661</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>7f5ac_00000</td><td style=\"text-align: right;\">   0.00225472</td></tr>\n",
       "<tr><td>train_cifar_7f5ac_00001</td><td style=\"text-align: right;\">    0.2231</td><td>2023-03-15_11-18-10</td><td>True  </td><td>                </td><td>b18f88a018ea4bff95b25b534b5f43c3</td><td>Kingstar-Desktop</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">2.07262</td><td>172.23.199.201</td><td style=\"text-align: right;\"> 3729</td><td>True               </td><td style=\"text-align: right;\">             53.8996</td><td style=\"text-align: right;\">           25.5668</td><td style=\"text-align: right;\">       53.8996</td><td style=\"text-align: right;\"> 1678846690</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>7f5ac_00001</td><td style=\"text-align: right;\">   0.00249505</td></tr>\n",
       "<tr><td>train_cifar_7f5ac_00002</td><td style=\"text-align: right;\">    0.5002</td><td>2023-03-15_11-18-53</td><td>False </td><td>                </td><td>ba7f4749d5fb48d98d5d01df6f0e14c1</td><td>Kingstar-Desktop</td><td style=\"text-align: right;\">                         6</td><td style=\"text-align: right;\">1.39431</td><td>172.23.199.201</td><td style=\"text-align: right;\"> 3731</td><td>True               </td><td style=\"text-align: right;\">             96.7456</td><td style=\"text-align: right;\">           14.9573</td><td style=\"text-align: right;\">       96.7456</td><td style=\"text-align: right;\"> 1678846733</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   6</td><td>7f5ac_00002</td><td style=\"text-align: right;\">   0.00201845</td></tr>\n",
       "<tr><td>train_cifar_7f5ac_00003</td><td style=\"text-align: right;\">    0.4036</td><td>2023-03-15_11-18-40</td><td>False </td><td>                </td><td>f691bb3bc71b4588821332d81069e606</td><td>Kingstar-Desktop</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">1.66019</td><td>172.23.199.201</td><td style=\"text-align: right;\"> 3733</td><td>True               </td><td style=\"text-align: right;\">             84.2293</td><td style=\"text-align: right;\">           84.2293</td><td style=\"text-align: right;\">       84.2293</td><td style=\"text-align: right;\"> 1678846720</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>7f5ac_00003</td><td style=\"text-align: right;\">   0.00234413</td></tr>\n",
       "<tr><td>train_cifar_7f5ac_00004</td><td style=\"text-align: right;\">    0.0992</td><td>2023-03-15_11-17-44</td><td>True  </td><td>                </td><td>279dc4323d6a4447b45be682234ddf16</td><td>Kingstar-Desktop</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.3294 </td><td>172.23.199.201</td><td style=\"text-align: right;\"> 3735</td><td>True               </td><td style=\"text-align: right;\">             28.2683</td><td style=\"text-align: right;\">           28.2683</td><td style=\"text-align: right;\">       28.2683</td><td style=\"text-align: right;\"> 1678846664</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>7f5ac_00004</td><td style=\"text-align: right;\">   0.00282693</td></tr>\n",
       "<tr><td>train_cifar_7f5ac_00005</td><td style=\"text-align: right;\">    0.3787</td><td>2023-03-15_11-18-38</td><td>False </td><td>                </td><td>6885adb3ab5944baa27855abf637855b</td><td>Kingstar-Desktop</td><td style=\"text-align: right;\">                         3</td><td style=\"text-align: right;\">1.71061</td><td>172.23.199.201</td><td style=\"text-align: right;\"> 3737</td><td>True               </td><td style=\"text-align: right;\">             82.4263</td><td style=\"text-align: right;\">           25.3445</td><td style=\"text-align: right;\">       82.4263</td><td style=\"text-align: right;\"> 1678846718</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>7f5ac_00005</td><td style=\"text-align: right;\">   0.00226235</td></tr>\n",
       "<tr><td>train_cifar_7f5ac_00006</td><td style=\"text-align: right;\">    0.288 </td><td>2023-03-15_11-18-26</td><td>False </td><td>                </td><td>e8db405cdecc465f9c9ffa7d35e23ca0</td><td>Kingstar-Desktop</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">1.80031</td><td>172.23.199.201</td><td style=\"text-align: right;\"> 3668</td><td>True               </td><td style=\"text-align: right;\">             44.5802</td><td style=\"text-align: right;\">           44.5802</td><td style=\"text-align: right;\">       44.5802</td><td style=\"text-align: right;\"> 1678846706</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>7f5ac_00006</td><td style=\"text-align: right;\">   0.00225472</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=3729)\u001b[0m [1,  4000] loss: 1.148\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m [1,  4000] loss: 1.166\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m [1,  4000] loss: 1.147\n",
      "== Status ==\n",
      "Current time: 2023-03-15 11:17:41 (running for 00:00:28.33)\n",
      "Memory usage on this node: 5.7/15.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -2.1763066513061524\n",
      "Resources requested: 12.0/12 CPUs, 0/0 GPUs, 0.0/8.34 GiB heap, 0.0/4.17 GiB objects\n",
      "Result logdir: /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12\n",
      "Number of trials: 10/10 (4 PENDING, 6 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_7f5ac_00000 | RUNNING  | 172.23.199.201:3668 |            8 |   32 |    8 | 0.000178243 |         |            |                      |\n",
      "| train_cifar_7f5ac_00001 | RUNNING  | 172.23.199.201:3729 |            8 |    8 |  256 | 0.000102774 |         |            |                      |\n",
      "| train_cifar_7f5ac_00002 | RUNNING  | 172.23.199.201:3731 |           16 |   32 |   64 | 0.000617828 | 2.17631 |     0.1844 |                    1 |\n",
      "| train_cifar_7f5ac_00003 | RUNNING  | 172.23.199.201:3733 |            2 |   16 |  128 | 0.00224033  |         |            |                      |\n",
      "| train_cifar_7f5ac_00004 | RUNNING  | 172.23.199.201:3735 |            8 |   32 |    8 | 0.0966673   |         |            |                      |\n",
      "| train_cifar_7f5ac_00005 | RUNNING  | 172.23.199.201:3737 |            8 |  256 |   16 | 0.000194578 |         |            |                      |\n",
      "| train_cifar_7f5ac_00006 | PENDING  |                     |            4 |   64 |    4 | 0.00106146  |         |            |                      |\n",
      "| train_cifar_7f5ac_00007 | PENDING  |                     |            2 |  256 |    4 | 0.0127225   |         |            |                      |\n",
      "| train_cifar_7f5ac_00008 | PENDING  |                     |            4 |  256 |    4 | 0.00447093  |         |            |                      |\n",
      "| train_cifar_7f5ac_00009 | PENDING  |                     |           16 |   64 |   64 | 0.000208139 |         |            |                      |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m [1,  6000] loss: 0.584\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m Files already downloaded and verified\n",
      "== Status ==\n",
      "Current time: 2023-03-15 11:17:46 (running for 00:00:33.74)\n",
      "Memory usage on this node: 5.7/15.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -2.271267781639099\n",
      "Resources requested: 12.0/12 CPUs, 0/0 GPUs, 0.0/8.34 GiB heap, 0.0/4.17 GiB objects\n",
      "Result logdir: /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12\n",
      "Number of trials: 10/10 (2 PENDING, 6 RUNNING, 2 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_7f5ac_00001 | RUNNING    | 172.23.199.201:3729 |            8 |    8 |  256 | 0.000102774 | 2.27127 |     0.1572 |                    1 |\n",
      "| train_cifar_7f5ac_00002 | RUNNING    | 172.23.199.201:3731 |           16 |   32 |   64 | 0.000617828 | 2.17631 |     0.1844 |                    1 |\n",
      "| train_cifar_7f5ac_00003 | RUNNING    | 172.23.199.201:3733 |            2 |   16 |  128 | 0.00224033  |         |            |                      |\n",
      "| train_cifar_7f5ac_00005 | RUNNING    | 172.23.199.201:3737 |            8 |  256 |   16 | 0.000194578 | 2.2425  |     0.1633 |                    1 |\n",
      "| train_cifar_7f5ac_00006 | RUNNING    | 172.23.199.201:3668 |            4 |   64 |    4 | 0.00106146  |         |            |                      |\n",
      "| train_cifar_7f5ac_00007 | RUNNING    | 172.23.199.201:3735 |            2 |  256 |    4 | 0.0127225   |         |            |                      |\n",
      "| train_cifar_7f5ac_00008 | PENDING    |                     |            4 |  256 |    4 | 0.00447093  |         |            |                      |\n",
      "| train_cifar_7f5ac_00009 | PENDING    |                     |           16 |   64 |   64 | 0.000208139 |         |            |                      |\n",
      "| train_cifar_7f5ac_00000 | TERMINATED | 172.23.199.201:3668 |            8 |   32 |    8 | 0.000178243 | 2.30576 |     0.0977 |                    1 |\n",
      "| train_cifar_7f5ac_00004 | TERMINATED | 172.23.199.201:3735 |            8 |   32 |    8 | 0.0966673   | 2.3294  |     0.0992 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=3731)\u001b[0m [2,  2000] loss: 2.002\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m [1,  8000] loss: 0.431\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m [1,  2000] loss: 2.307\n",
      "== Status ==\n",
      "Current time: 2023-03-15 11:17:51 (running for 00:00:38.74)\n",
      "Memory usage on this node: 5.8/15.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -2.271267781639099\n",
      "Resources requested: 12.0/12 CPUs, 0/0 GPUs, 0.0/8.34 GiB heap, 0.0/4.17 GiB objects\n",
      "Result logdir: /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12\n",
      "Number of trials: 10/10 (2 PENDING, 6 RUNNING, 2 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_7f5ac_00001 | RUNNING    | 172.23.199.201:3729 |            8 |    8 |  256 | 0.000102774 | 2.27127 |     0.1572 |                    1 |\n",
      "| train_cifar_7f5ac_00002 | RUNNING    | 172.23.199.201:3731 |           16 |   32 |   64 | 0.000617828 | 2.17631 |     0.1844 |                    1 |\n",
      "| train_cifar_7f5ac_00003 | RUNNING    | 172.23.199.201:3733 |            2 |   16 |  128 | 0.00224033  |         |            |                      |\n",
      "| train_cifar_7f5ac_00005 | RUNNING    | 172.23.199.201:3737 |            8 |  256 |   16 | 0.000194578 | 2.2425  |     0.1633 |                    1 |\n",
      "| train_cifar_7f5ac_00006 | RUNNING    | 172.23.199.201:3668 |            4 |   64 |    4 | 0.00106146  |         |            |                      |\n",
      "| train_cifar_7f5ac_00007 | RUNNING    | 172.23.199.201:3735 |            2 |  256 |    4 | 0.0127225   |         |            |                      |\n",
      "| train_cifar_7f5ac_00008 | PENDING    |                     |            4 |  256 |    4 | 0.00447093  |         |            |                      |\n",
      "| train_cifar_7f5ac_00009 | PENDING    |                     |           16 |   64 |   64 | 0.000208139 |         |            |                      |\n",
      "| train_cifar_7f5ac_00000 | TERMINATED | 172.23.199.201:3668 |            8 |   32 |    8 | 0.000178243 | 2.30576 |     0.0977 |                    1 |\n",
      "| train_cifar_7f5ac_00004 | TERMINATED | 172.23.199.201:3735 |            8 |   32 |    8 | 0.0966673   | 2.3294  |     0.0992 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m [1,  2000] loss: 2.318\n",
      "\u001b[2m\u001b[36m(func pid=3729)\u001b[0m [2,  2000] loss: 2.235\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m [1, 10000] loss: 0.345\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m [2,  2000] loss: 2.165\n",
      "== Status ==\n",
      "Current time: 2023-03-15 11:17:57 (running for 00:00:44.86)\n",
      "Memory usage on this node: 5.8/15.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.825210302734375 | Iter 1.000: -2.271267781639099\n",
      "Resources requested: 12.0/12 CPUs, 0/0 GPUs, 0.0/8.34 GiB heap, 0.0/4.17 GiB objects\n",
      "Result logdir: /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12\n",
      "Number of trials: 10/10 (2 PENDING, 6 RUNNING, 2 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_7f5ac_00001 | RUNNING    | 172.23.199.201:3729 |            8 |    8 |  256 | 0.000102774 | 2.27127 |     0.1572 |                    1 |\n",
      "| train_cifar_7f5ac_00002 | RUNNING    | 172.23.199.201:3731 |           16 |   32 |   64 | 0.000617828 | 1.82521 |     0.3336 |                    2 |\n",
      "| train_cifar_7f5ac_00003 | RUNNING    | 172.23.199.201:3733 |            2 |   16 |  128 | 0.00224033  |         |            |                      |\n",
      "| train_cifar_7f5ac_00005 | RUNNING    | 172.23.199.201:3737 |            8 |  256 |   16 | 0.000194578 | 2.2425  |     0.1633 |                    1 |\n",
      "| train_cifar_7f5ac_00006 | RUNNING    | 172.23.199.201:3668 |            4 |   64 |    4 | 0.00106146  |         |            |                      |\n",
      "| train_cifar_7f5ac_00007 | RUNNING    | 172.23.199.201:3735 |            2 |  256 |    4 | 0.0127225   |         |            |                      |\n",
      "| train_cifar_7f5ac_00008 | PENDING    |                     |            4 |  256 |    4 | 0.00447093  |         |            |                      |\n",
      "| train_cifar_7f5ac_00009 | PENDING    |                     |           16 |   64 |   64 | 0.000208139 |         |            |                      |\n",
      "| train_cifar_7f5ac_00000 | TERMINATED | 172.23.199.201:3668 |            8 |   32 |    8 | 0.000178243 | 2.30576 |     0.0977 |                    1 |\n",
      "| train_cifar_7f5ac_00004 | TERMINATED | 172.23.199.201:3735 |            8 |   32 |    8 | 0.0966673   | 2.3294  |     0.0992 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m [1,  4000] loss: 1.143\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m [1,  4000] loss: 1.158\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m [1, 12000] loss: 0.276\n",
      "== Status ==\n",
      "Current time: 2023-03-15 11:18:02 (running for 00:00:49.87)\n",
      "Memory usage on this node: 5.8/15.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.825210302734375 | Iter 1.000: -2.271267781639099\n",
      "Resources requested: 12.0/12 CPUs, 0/0 GPUs, 0.0/8.34 GiB heap, 0.0/4.17 GiB objects\n",
      "Result logdir: /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12\n",
      "Number of trials: 10/10 (2 PENDING, 6 RUNNING, 2 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_7f5ac_00001 | RUNNING    | 172.23.199.201:3729 |            8 |    8 |  256 | 0.000102774 | 2.27127 |     0.1572 |                    1 |\n",
      "| train_cifar_7f5ac_00002 | RUNNING    | 172.23.199.201:3731 |           16 |   32 |   64 | 0.000617828 | 1.82521 |     0.3336 |                    2 |\n",
      "| train_cifar_7f5ac_00003 | RUNNING    | 172.23.199.201:3733 |            2 |   16 |  128 | 0.00224033  |         |            |                      |\n",
      "| train_cifar_7f5ac_00005 | RUNNING    | 172.23.199.201:3737 |            8 |  256 |   16 | 0.000194578 | 2.2425  |     0.1633 |                    1 |\n",
      "| train_cifar_7f5ac_00006 | RUNNING    | 172.23.199.201:3668 |            4 |   64 |    4 | 0.00106146  |         |            |                      |\n",
      "| train_cifar_7f5ac_00007 | RUNNING    | 172.23.199.201:3735 |            2 |  256 |    4 | 0.0127225   |         |            |                      |\n",
      "| train_cifar_7f5ac_00008 | PENDING    |                     |            4 |  256 |    4 | 0.00447093  |         |            |                      |\n",
      "| train_cifar_7f5ac_00009 | PENDING    |                     |           16 |   64 |   64 | 0.000208139 |         |            |                      |\n",
      "| train_cifar_7f5ac_00000 | TERMINATED | 172.23.199.201:3668 |            8 |   32 |    8 | 0.000178243 | 2.30576 |     0.0977 |                    1 |\n",
      "| train_cifar_7f5ac_00004 | TERMINATED | 172.23.199.201:3735 |            8 |   32 |    8 | 0.0966673   | 2.3294  |     0.0992 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=3729)\u001b[0m [2,  4000] loss: 1.079\n",
      "\u001b[2m\u001b[36m(func pid=3731)\u001b[0m [3,  2000] loss: 1.689\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m [2,  4000] loss: 1.026\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m [1,  6000] loss: 0.737\n",
      "== Status ==\n",
      "Current time: 2023-03-15 11:18:07 (running for 00:00:54.87)\n",
      "Memory usage on this node: 5.8/15.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.825210302734375 | Iter 1.000: -2.271267781639099\n",
      "Resources requested: 12.0/12 CPUs, 0/0 GPUs, 0.0/8.34 GiB heap, 0.0/4.17 GiB objects\n",
      "Result logdir: /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12\n",
      "Number of trials: 10/10 (2 PENDING, 6 RUNNING, 2 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_7f5ac_00001 | RUNNING    | 172.23.199.201:3729 |            8 |    8 |  256 | 0.000102774 | 2.27127 |     0.1572 |                    1 |\n",
      "| train_cifar_7f5ac_00002 | RUNNING    | 172.23.199.201:3731 |           16 |   32 |   64 | 0.000617828 | 1.82521 |     0.3336 |                    2 |\n",
      "| train_cifar_7f5ac_00003 | RUNNING    | 172.23.199.201:3733 |            2 |   16 |  128 | 0.00224033  |         |            |                      |\n",
      "| train_cifar_7f5ac_00005 | RUNNING    | 172.23.199.201:3737 |            8 |  256 |   16 | 0.000194578 | 2.2425  |     0.1633 |                    1 |\n",
      "| train_cifar_7f5ac_00006 | RUNNING    | 172.23.199.201:3668 |            4 |   64 |    4 | 0.00106146  |         |            |                      |\n",
      "| train_cifar_7f5ac_00007 | RUNNING    | 172.23.199.201:3735 |            2 |  256 |    4 | 0.0127225   |         |            |                      |\n",
      "| train_cifar_7f5ac_00008 | PENDING    |                     |            4 |  256 |    4 | 0.00447093  |         |            |                      |\n",
      "| train_cifar_7f5ac_00009 | PENDING    |                     |           16 |   64 |   64 | 0.000208139 |         |            |                      |\n",
      "| train_cifar_7f5ac_00000 | TERMINATED | 172.23.199.201:3668 |            8 |   32 |    8 | 0.000178243 | 2.30576 |     0.0977 |                    1 |\n",
      "| train_cifar_7f5ac_00004 | TERMINATED | 172.23.199.201:3735 |            8 |   32 |    8 | 0.0966673   | 2.3294  |     0.0992 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m [1,  6000] loss: 0.772\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m [1, 14000] loss: 0.240\n",
      "\u001b[2m\u001b[36m(func pid=3729)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=3729)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m [1,  8000] loss: 0.507\n",
      "== Status ==\n",
      "Current time: 2023-03-15 11:18:13 (running for 00:01:00.70)\n",
      "Memory usage on this node: 5.7/15.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=3\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.9317257415771485 | Iter 1.000: -2.271267781639099\n",
      "Resources requested: 12.0/12 CPUs, 0/0 GPUs, 0.0/8.34 GiB heap, 0.0/4.17 GiB objects\n",
      "Result logdir: /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12\n",
      "Number of trials: 10/10 (1 PENDING, 6 RUNNING, 3 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_7f5ac_00002 | RUNNING    | 172.23.199.201:3731 |           16 |   32 |   64 | 0.000617828 | 1.57335 |     0.4206 |                    3 |\n",
      "| train_cifar_7f5ac_00003 | RUNNING    | 172.23.199.201:3733 |            2 |   16 |  128 | 0.00224033  |         |            |                      |\n",
      "| train_cifar_7f5ac_00005 | RUNNING    | 172.23.199.201:3737 |            8 |  256 |   16 | 0.000194578 | 1.93173 |     0.3079 |                    2 |\n",
      "| train_cifar_7f5ac_00006 | RUNNING    | 172.23.199.201:3668 |            4 |   64 |    4 | 0.00106146  |         |            |                      |\n",
      "| train_cifar_7f5ac_00007 | RUNNING    | 172.23.199.201:3735 |            2 |  256 |    4 | 0.0127225   |         |            |                      |\n",
      "| train_cifar_7f5ac_00008 | RUNNING    | 172.23.199.201:3729 |            4 |  256 |    4 | 0.00447093  |         |            |                      |\n",
      "| train_cifar_7f5ac_00009 | PENDING    |                     |           16 |   64 |   64 | 0.000208139 |         |            |                      |\n",
      "| train_cifar_7f5ac_00000 | TERMINATED | 172.23.199.201:3668 |            8 |   32 |    8 | 0.000178243 | 2.30576 |     0.0977 |                    1 |\n",
      "| train_cifar_7f5ac_00001 | TERMINATED | 172.23.199.201:3729 |            8 |    8 |  256 | 0.000102774 | 2.07262 |     0.2231 |                    2 |\n",
      "| train_cifar_7f5ac_00004 | TERMINATED | 172.23.199.201:3735 |            8 |   32 |    8 | 0.0966673   | 2.3294  |     0.0992 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m [1,  8000] loss: 0.579\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m [1, 16000] loss: 0.209\n",
      "== Status ==\n",
      "Current time: 2023-03-15 11:18:18 (running for 00:01:05.72)\n",
      "Memory usage on this node: 5.9/15.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=3\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.9317257415771485 | Iter 1.000: -2.271267781639099\n",
      "Resources requested: 12.0/12 CPUs, 0/0 GPUs, 0.0/8.34 GiB heap, 0.0/4.17 GiB objects\n",
      "Result logdir: /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12\n",
      "Number of trials: 10/10 (1 PENDING, 6 RUNNING, 3 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_7f5ac_00002 | RUNNING    | 172.23.199.201:3731 |           16 |   32 |   64 | 0.000617828 | 1.57335 |     0.4206 |                    3 |\n",
      "| train_cifar_7f5ac_00003 | RUNNING    | 172.23.199.201:3733 |            2 |   16 |  128 | 0.00224033  |         |            |                      |\n",
      "| train_cifar_7f5ac_00005 | RUNNING    | 172.23.199.201:3737 |            8 |  256 |   16 | 0.000194578 | 1.93173 |     0.3079 |                    2 |\n",
      "| train_cifar_7f5ac_00006 | RUNNING    | 172.23.199.201:3668 |            4 |   64 |    4 | 0.00106146  |         |            |                      |\n",
      "| train_cifar_7f5ac_00007 | RUNNING    | 172.23.199.201:3735 |            2 |  256 |    4 | 0.0127225   |         |            |                      |\n",
      "| train_cifar_7f5ac_00008 | RUNNING    | 172.23.199.201:3729 |            4 |  256 |    4 | 0.00447093  |         |            |                      |\n",
      "| train_cifar_7f5ac_00009 | PENDING    |                     |           16 |   64 |   64 | 0.000208139 |         |            |                      |\n",
      "| train_cifar_7f5ac_00000 | TERMINATED | 172.23.199.201:3668 |            8 |   32 |    8 | 0.000178243 | 2.30576 |     0.0977 |                    1 |\n",
      "| train_cifar_7f5ac_00001 | TERMINATED | 172.23.199.201:3729 |            8 |    8 |  256 | 0.000102774 | 2.07262 |     0.2231 |                    2 |\n",
      "| train_cifar_7f5ac_00004 | TERMINATED | 172.23.199.201:3735 |            8 |   32 |    8 | 0.0966673   | 2.3294  |     0.0992 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=3731)\u001b[0m [4,  2000] loss: 1.527\n",
      "\u001b[2m\u001b[36m(func pid=3729)\u001b[0m [1,  2000] loss: 2.153\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m [1, 10000] loss: 0.371\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m [1, 18000] loss: 0.184\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m [1, 10000] loss: 0.463\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m [3,  2000] loss: 1.893\n",
      "== Status ==\n",
      "Current time: 2023-03-15 11:18:26 (running for 00:01:13.28)\n",
      "Memory usage on this node: 5.8/15.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=3\n",
      "Bracket: Iter 8.000: None | Iter 4.000: -1.5010592198371888 | Iter 2.000: -1.9317257415771485 | Iter 1.000: -2.2568849164009093\n",
      "Resources requested: 12.0/12 CPUs, 0/0 GPUs, 0.0/8.34 GiB heap, 0.0/4.17 GiB objects\n",
      "Result logdir: /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12\n",
      "Number of trials: 10/10 (1 PENDING, 6 RUNNING, 3 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_7f5ac_00002 | RUNNING    | 172.23.199.201:3731 |           16 |   32 |   64 | 0.000617828 | 1.50106 |     0.4513 |                    4 |\n",
      "| train_cifar_7f5ac_00003 | RUNNING    | 172.23.199.201:3733 |            2 |   16 |  128 | 0.00224033  |         |            |                      |\n",
      "| train_cifar_7f5ac_00005 | RUNNING    | 172.23.199.201:3737 |            8 |  256 |   16 | 0.000194578 | 1.93173 |     0.3079 |                    2 |\n",
      "| train_cifar_7f5ac_00006 | RUNNING    | 172.23.199.201:3668 |            4 |   64 |    4 | 0.00106146  | 1.80031 |     0.288  |                    1 |\n",
      "| train_cifar_7f5ac_00007 | RUNNING    | 172.23.199.201:3735 |            2 |  256 |    4 | 0.0127225   |         |            |                      |\n",
      "| train_cifar_7f5ac_00008 | RUNNING    | 172.23.199.201:3729 |            4 |  256 |    4 | 0.00447093  |         |            |                      |\n",
      "| train_cifar_7f5ac_00009 | PENDING    |                     |           16 |   64 |   64 | 0.000208139 |         |            |                      |\n",
      "| train_cifar_7f5ac_00000 | TERMINATED | 172.23.199.201:3668 |            8 |   32 |    8 | 0.000178243 | 2.30576 |     0.0977 |                    1 |\n",
      "| train_cifar_7f5ac_00001 | TERMINATED | 172.23.199.201:3729 |            8 |    8 |  256 | 0.000102774 | 2.07262 |     0.2231 |                    2 |\n",
      "| train_cifar_7f5ac_00004 | TERMINATED | 172.23.199.201:3735 |            8 |   32 |    8 | 0.0966673   | 2.3294  |     0.0992 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=3729)\u001b[0m [1,  4000] loss: 0.959\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m [1, 20000] loss: 0.167\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m [1, 12000] loss: 0.386\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m [3,  4000] loss: 0.895\n",
      "== Status ==\n",
      "Current time: 2023-03-15 11:18:31 (running for 00:01:18.29)\n",
      "Memory usage on this node: 5.9/15.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=3\n",
      "Bracket: Iter 8.000: None | Iter 4.000: -1.5010592198371888 | Iter 2.000: -1.9317257415771485 | Iter 1.000: -2.2568849164009093\n",
      "Resources requested: 12.0/12 CPUs, 0/0 GPUs, 0.0/8.34 GiB heap, 0.0/4.17 GiB objects\n",
      "Result logdir: /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12\n",
      "Number of trials: 10/10 (1 PENDING, 6 RUNNING, 3 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_7f5ac_00002 | RUNNING    | 172.23.199.201:3731 |           16 |   32 |   64 | 0.000617828 | 1.50106 |     0.4513 |                    4 |\n",
      "| train_cifar_7f5ac_00003 | RUNNING    | 172.23.199.201:3733 |            2 |   16 |  128 | 0.00224033  |         |            |                      |\n",
      "| train_cifar_7f5ac_00005 | RUNNING    | 172.23.199.201:3737 |            8 |  256 |   16 | 0.000194578 | 1.93173 |     0.3079 |                    2 |\n",
      "| train_cifar_7f5ac_00006 | RUNNING    | 172.23.199.201:3668 |            4 |   64 |    4 | 0.00106146  | 1.80031 |     0.288  |                    1 |\n",
      "| train_cifar_7f5ac_00007 | RUNNING    | 172.23.199.201:3735 |            2 |  256 |    4 | 0.0127225   |         |            |                      |\n",
      "| train_cifar_7f5ac_00008 | RUNNING    | 172.23.199.201:3729 |            4 |  256 |    4 | 0.00447093  |         |            |                      |\n",
      "| train_cifar_7f5ac_00009 | PENDING    |                     |           16 |   64 |   64 | 0.000208139 |         |            |                      |\n",
      "| train_cifar_7f5ac_00000 | TERMINATED | 172.23.199.201:3668 |            8 |   32 |    8 | 0.000178243 | 2.30576 |     0.0977 |                    1 |\n",
      "| train_cifar_7f5ac_00001 | TERMINATED | 172.23.199.201:3729 |            8 |    8 |  256 | 0.000102774 | 2.07262 |     0.2231 |                    2 |\n",
      "| train_cifar_7f5ac_00004 | TERMINATED | 172.23.199.201:3735 |            8 |   32 |    8 | 0.0966673   | 2.3294  |     0.0992 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m [2,  2000] loss: 1.707\n",
      "\u001b[2m\u001b[36m(func pid=3729)\u001b[0m [1,  6000] loss: 0.604\n",
      "\u001b[2m\u001b[36m(func pid=3731)\u001b[0m [5,  2000] loss: 1.435\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m [1, 14000] loss: 0.331\n",
      "== Status ==\n",
      "Current time: 2023-03-15 11:18:36 (running for 00:01:23.29)\n",
      "Memory usage on this node: 5.8/15.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=3\n",
      "Bracket: Iter 8.000: None | Iter 4.000: -1.5010592198371888 | Iter 2.000: -1.9317257415771485 | Iter 1.000: -2.2568849164009093\n",
      "Resources requested: 12.0/12 CPUs, 0/0 GPUs, 0.0/8.34 GiB heap, 0.0/4.17 GiB objects\n",
      "Result logdir: /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12\n",
      "Number of trials: 10/10 (1 PENDING, 6 RUNNING, 3 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_7f5ac_00002 | RUNNING    | 172.23.199.201:3731 |           16 |   32 |   64 | 0.000617828 | 1.50106 |     0.4513 |                    4 |\n",
      "| train_cifar_7f5ac_00003 | RUNNING    | 172.23.199.201:3733 |            2 |   16 |  128 | 0.00224033  |         |            |                      |\n",
      "| train_cifar_7f5ac_00005 | RUNNING    | 172.23.199.201:3737 |            8 |  256 |   16 | 0.000194578 | 1.93173 |     0.3079 |                    2 |\n",
      "| train_cifar_7f5ac_00006 | RUNNING    | 172.23.199.201:3668 |            4 |   64 |    4 | 0.00106146  | 1.80031 |     0.288  |                    1 |\n",
      "| train_cifar_7f5ac_00007 | RUNNING    | 172.23.199.201:3735 |            2 |  256 |    4 | 0.0127225   |         |            |                      |\n",
      "| train_cifar_7f5ac_00008 | RUNNING    | 172.23.199.201:3729 |            4 |  256 |    4 | 0.00447093  |         |            |                      |\n",
      "| train_cifar_7f5ac_00009 | PENDING    |                     |           16 |   64 |   64 | 0.000208139 |         |            |                      |\n",
      "| train_cifar_7f5ac_00000 | TERMINATED | 172.23.199.201:3668 |            8 |   32 |    8 | 0.000178243 | 2.30576 |     0.0977 |                    1 |\n",
      "| train_cifar_7f5ac_00001 | TERMINATED | 172.23.199.201:3729 |            8 |    8 |  256 | 0.000102774 | 2.07262 |     0.2231 |                    2 |\n",
      "| train_cifar_7f5ac_00004 | TERMINATED | 172.23.199.201:3735 |            8 |   32 |    8 | 0.0966673   | 2.3294  |     0.0992 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m [2,  4000] loss: 0.836\n",
      "\u001b[2m\u001b[36m(func pid=3729)\u001b[0m [1,  8000] loss: 0.441\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m [1, 16000] loss: 0.290\n",
      "== Status ==\n",
      "Current time: 2023-03-15 11:18:45 (running for 00:01:32.83)\n",
      "Memory usage on this node: 5.9/15.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=3\n",
      "Bracket: Iter 8.000: None | Iter 4.000: -1.5010592198371888 | Iter 2.000: -1.9317257415771485 | Iter 1.000: -2.24250205116272\n",
      "Resources requested: 12.0/12 CPUs, 0/0 GPUs, 0.0/8.34 GiB heap, 0.0/4.17 GiB objects\n",
      "Result logdir: /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12\n",
      "Number of trials: 10/10 (1 PENDING, 6 RUNNING, 3 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_7f5ac_00002 | RUNNING    | 172.23.199.201:3731 |           16 |   32 |   64 | 0.000617828 | 1.41391 |     0.4851 |                    5 |\n",
      "| train_cifar_7f5ac_00003 | RUNNING    | 172.23.199.201:3733 |            2 |   16 |  128 | 0.00224033  | 1.66019 |     0.4036 |                    1 |\n",
      "| train_cifar_7f5ac_00005 | RUNNING    | 172.23.199.201:3737 |            8 |  256 |   16 | 0.000194578 | 1.71061 |     0.3787 |                    3 |\n",
      "| train_cifar_7f5ac_00006 | RUNNING    | 172.23.199.201:3668 |            4 |   64 |    4 | 0.00106146  | 1.80031 |     0.288  |                    1 |\n",
      "| train_cifar_7f5ac_00007 | RUNNING    | 172.23.199.201:3735 |            2 |  256 |    4 | 0.0127225   |         |            |                      |\n",
      "| train_cifar_7f5ac_00008 | RUNNING    | 172.23.199.201:3729 |            4 |  256 |    4 | 0.00447093  |         |            |                      |\n",
      "| train_cifar_7f5ac_00009 | PENDING    |                     |           16 |   64 |   64 | 0.000208139 |         |            |                      |\n",
      "| train_cifar_7f5ac_00000 | TERMINATED | 172.23.199.201:3668 |            8 |   32 |    8 | 0.000178243 | 2.30576 |     0.0977 |                    1 |\n",
      "| train_cifar_7f5ac_00001 | TERMINATED | 172.23.199.201:3729 |            8 |    8 |  256 | 0.000102774 | 2.07262 |     0.2231 |                    2 |\n",
      "| train_cifar_7f5ac_00004 | TERMINATED | 172.23.199.201:3735 |            8 |   32 |    8 | 0.0966673   | 2.3294  |     0.0992 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m [2,  6000] loss: 0.534\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m [2,  2000] loss: 1.624\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m [4,  2000] loss: 1.675\n",
      "\u001b[2m\u001b[36m(func pid=3729)\u001b[0m [1, 10000] loss: 0.347\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m [1, 18000] loss: 0.257\n",
      "\u001b[2m\u001b[36m(func pid=3731)\u001b[0m [6,  2000] loss: 1.370\n",
      "== Status ==\n",
      "Current time: 2023-03-15 11:18:50 (running for 00:01:37.84)\n",
      "Memory usage on this node: 5.8/15.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=3\n",
      "Bracket: Iter 8.000: None | Iter 4.000: -1.5010592198371888 | Iter 2.000: -1.9317257415771485 | Iter 1.000: -2.24250205116272\n",
      "Resources requested: 12.0/12 CPUs, 0/0 GPUs, 0.0/8.34 GiB heap, 0.0/4.17 GiB objects\n",
      "Result logdir: /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12\n",
      "Number of trials: 10/10 (1 PENDING, 6 RUNNING, 3 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_7f5ac_00002 | RUNNING    | 172.23.199.201:3731 |           16 |   32 |   64 | 0.000617828 | 1.41391 |     0.4851 |                    5 |\n",
      "| train_cifar_7f5ac_00003 | RUNNING    | 172.23.199.201:3733 |            2 |   16 |  128 | 0.00224033  | 1.66019 |     0.4036 |                    1 |\n",
      "| train_cifar_7f5ac_00005 | RUNNING    | 172.23.199.201:3737 |            8 |  256 |   16 | 0.000194578 | 1.71061 |     0.3787 |                    3 |\n",
      "| train_cifar_7f5ac_00006 | RUNNING    | 172.23.199.201:3668 |            4 |   64 |    4 | 0.00106146  | 1.80031 |     0.288  |                    1 |\n",
      "| train_cifar_7f5ac_00007 | RUNNING    | 172.23.199.201:3735 |            2 |  256 |    4 | 0.0127225   |         |            |                      |\n",
      "| train_cifar_7f5ac_00008 | RUNNING    | 172.23.199.201:3729 |            4 |  256 |    4 | 0.00447093  |         |            |                      |\n",
      "| train_cifar_7f5ac_00009 | PENDING    |                     |           16 |   64 |   64 | 0.000208139 |         |            |                      |\n",
      "| train_cifar_7f5ac_00000 | TERMINATED | 172.23.199.201:3668 |            8 |   32 |    8 | 0.000178243 | 2.30576 |     0.0977 |                    1 |\n",
      "| train_cifar_7f5ac_00001 | TERMINATED | 172.23.199.201:3729 |            8 |    8 |  256 | 0.000102774 | 2.07262 |     0.2231 |                    2 |\n",
      "| train_cifar_7f5ac_00004 | TERMINATED | 172.23.199.201:3735 |            8 |   32 |    8 | 0.0966673   | 2.3294  |     0.0992 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 11:18:53,215\tWARNING tune.py:146 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2023-03-15 11:18:53,237\tERROR tune.py:794 -- Trials did not complete: [train_cifar_7f5ac_00002, train_cifar_7f5ac_00003, train_cifar_7f5ac_00005, train_cifar_7f5ac_00006, train_cifar_7f5ac_00007, train_cifar_7f5ac_00008, train_cifar_7f5ac_00009]\n",
      "2023-03-15 11:18:53,238\tINFO tune.py:798 -- Total run time: 100.37 seconds (100.34 seconds for the tuning loop).\n",
      "2023-03-15 11:18:53,240\tWARNING tune.py:804 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m 2023-03-15 11:18:53,253\tERROR worker.py:772 -- Worker exits with an exit code 1.\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1166, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1072, in ray._raylet.execute_task_with_cancellation_handler\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m   File \"python/ray/_raylet.pyx\", line 805, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m   File \"python/ray/_raylet.pyx\", line 850, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m   File \"python/ray/_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m   File \"python/ray/_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m   File \"python/ray/_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 365, in train\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 377, in step\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m     result = self._results_queue.get(\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/queue.py\", line 180, in get\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m     self.not_empty.wait(remaining)\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/threading.py\", line 324, in wait\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m     gotit = waiter.acquire(True, timeout)\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_private/worker.py\", line 769, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/multiprocessing/resource_sharer.py\", line 138, in _serve\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m     with self._listener.accept() as conn:\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 466, in accept\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m     answer_challenge(c, self._authkey)\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 757, in answer_challenge\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m     response = connection.recv_bytes(256)        # reject large message\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m     buf = self._recv_bytes(maxlength)\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m     buf = self._recv(4)\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m     chunk = read(handle, remaining)\n",
      "\u001b[2m\u001b[36m(func pid=3668)\u001b[0m ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/multiprocessing/resource_sharer.py\", line 138, in _serve\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m     with self._listener.accept() as conn:\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 466, in accept\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m     answer_challenge(c, self._authkey)\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 757, in answer_challenge\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m     response = connection.recv_bytes(256)        # reject large message\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m     buf = self._recv_bytes(maxlength)\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m     buf = self._recv(4)\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m     chunk = read(handle, remaining)\n",
      "\u001b[2m\u001b[36m(func pid=3733)\u001b[0m ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m [2023-03-15 11:18:53,294 C 3737 3737] core_worker.cc:717:  Check failed: _s.ok() Bad status: IOError: Broken pipe\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m *** StackTrace Information ***\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_raylet.so(+0xd508da) [0x7fe6d1a238da] ray::operator<<()\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_raylet.so(+0xd523c2) [0x7fe6d1a253c2] ray::SpdLogMessage::Flush()\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_raylet.so(_ZN3ray6RayLogD1Ev+0x37) [0x7fe6d1a256d7] ray::RayLog::~RayLog()\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_raylet.so(_ZN3ray4core10CoreWorker4ExitENS_3rpc14WorkerExitTypeERKSsRKSt10shared_ptrINS_17LocalMemoryBufferEE+0x1ab) [0x7fe6d132188b] ray::core::CoreWorker::Exit()\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_raylet.so(_ZN3ray4core10CoreWorker11ExecuteTaskERKNS_17TaskSpecificationERKSt10shared_ptrISt13unordered_mapISsSt6vectorISt4pairIldESaIS9_EESt4hashISsESt8equal_toISsESaIS8_IKSsSB_EEEEPS7_IS8_INS_8ObjectIDES5_INS_9RayObjectEEESaISQ_EEST_PN6google8protobuf16RepeatedPtrFieldINS_3rpc20ObjectReferenceCountEEEPbS11_+0x1c65) [0x7fe6d1334535] ray::core::CoreWorker::ExecuteTask()\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_raylet.so(_ZNSt17_Function_handlerIFN3ray6StatusERKNS0_17TaskSpecificationESt10shared_ptrISt13unordered_mapISsSt6vectorISt4pairIldESaIS9_EESt4hashISsESt8equal_toISsESaIS8_IKSsSB_EEEEPS7_IS8_INS0_8ObjectIDES5_INS0_9RayObjectEEESaISO_EESR_PN6google8protobuf16RepeatedPtrFieldINS0_3rpc20ObjectReferenceCountEEEPbSZ_ESt5_BindIFMNS0_4core10CoreWorkerEFS1_S4_RKSK_SR_SR_SY_SZ_SZ_EPS13_St12_PlaceholderILi1EES19_ILi2EES19_ILi3EES19_ILi4EES19_ILi5EES19_ILi6EES19_ILi7EEEEE9_M_invokeERKSt9_Any_dataS4_OSK_OSR_S1O_OSY_OSZ_S1Q_+0x54) [0x7fe6d1275a84] std::_Function_handler<>::_M_invoke()\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_raylet.so(+0x688602) [0x7fe6d135b602] ray::core::CoreWorkerDirectTaskReceiver::HandleTask()::{lambda()#1}::operator()()\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_raylet.so(+0x68948a) [0x7fe6d135c48a] std::_Function_handler<>::_M_invoke()\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_raylet.so(+0x69c5ee) [0x7fe6d136f5ee] ray::core::InboundRequest::Accept()\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_raylet.so(+0x6a1768) [0x7fe6d1374768] ray::core::ActorSchedulingQueue::ScheduleRequests()\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_raylet.so(_ZN3ray4core20ActorSchedulingQueue3AddEllSt8functionIFvS2_IFvNS_6StatusES2_IFvvEES5_EEEES9_S7_RKSsRKSt10shared_ptrINS_27FunctionDescriptorInterfaceEENS_6TaskIDERKSt6vectorINS_3rpc15ObjectReferenceESaISK_EE+0x596) [0x7fe6d13762c6] ray::core::ActorSchedulingQueue::Add()\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_raylet.so(_ZN3ray4core28CoreWorkerDirectTaskReceiver10HandleTaskENS_3rpc15PushTaskRequestEPNS2_13PushTaskReplyESt8functionIFvNS_6StatusES6_IFvvEES9_EE+0x1205) [0x7fe6d135b065] ray::core::CoreWorkerDirectTaskReceiver::HandleTask()\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_raylet.so(+0x63c79a) [0x7fe6d130f79a] std::_Function_handler<>::_M_invoke()\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_raylet.so(+0x90d206) [0x7fe6d15e0206] EventTracker::RecordExecution()\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_raylet.so(+0x8add3e) [0x7fe6d1580d3e] std::_Function_handler<>::_M_invoke()\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_raylet.so(+0x8ae296) [0x7fe6d1581296] boost::asio::detail::completion_handler<>::do_complete()\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_raylet.so(+0xd6135b) [0x7fe6d1a3435b] boost::asio::detail::scheduler::do_run_one()\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_raylet.so(+0xd625b1) [0x7fe6d1a355b1] boost::asio::detail::scheduler::run()\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_raylet.so(+0xd62820) [0x7fe6d1a35820] boost::asio::io_context::run()\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_raylet.so(_ZN3ray4core10CoreWorker20RunTaskExecutionLoopEv+0x1c) [0x7fe6d12fefcc] ray::core::CoreWorker::RunTaskExecutionLoop()\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_raylet.so(_ZN3ray4core21CoreWorkerProcessImpl26RunWorkerTaskExecutionLoopEv+0x8c) [0x7fe6d133ebdc] ray::core::CoreWorkerProcessImpl::RunWorkerTaskExecutionLoop()\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_raylet.so(_ZN3ray4core17CoreWorkerProcess20RunTaskExecutionLoopEv+0x1d) [0x7fe6d133ed8d] ray::core::CoreWorkerProcess::RunTaskExecutionLoop()\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /home/kingstar/anaconda3/envs/ml/lib/python3.10/site-packages/ray/_raylet.so(+0x4cd907) [0x7fe6d11a0907] __pyx_pw_3ray_7_raylet_10CoreWorker_7run_task_loop()\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m ray::ImplicitFunc() [0x503514] method_vectorcall_NOARGS\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m ray::ImplicitFunc(_PyEval_EvalFrameDefault+0x72e) [0x4f117e] _PyEval_EvalFrameDefault\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m ray::ImplicitFunc(_PyFunction_Vectorcall+0x6f) [0x500b2f] _PyFunction_Vectorcall\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m ray::ImplicitFunc(_PyEval_EvalFrameDefault+0x72e) [0x4f117e] _PyEval_EvalFrameDefault\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m ray::ImplicitFunc() [0x598dc2] _PyEval_Vector\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m ray::ImplicitFunc(PyEval_EvalCode+0x87) [0x598d07] PyEval_EvalCode\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m ray::ImplicitFunc() [0x5cb4b7] run_eval_code_obj\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m ray::ImplicitFunc() [0x5c62f0] run_mod\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m ray::ImplicitFunc() [0x45be54] pyrun_file.cold\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m ray::ImplicitFunc(_PyRun_SimpleFileObject+0x19f) [0x5c080f] _PyRun_SimpleFileObject\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m ray::ImplicitFunc(_PyRun_AnyFileObject+0x43) [0x5c0613] _PyRun_AnyFileObject\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m ray::ImplicitFunc(Py_RunMain+0x38d) [0x5bd42d] Py_RunMain\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m ray::ImplicitFunc(Py_BytesMain+0x39) [0x58c339] Py_BytesMain\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /lib/x86_64-linux-gnu/libc.so.6(+0x29d90) [0x7fe6d26b1d90]\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80) [0x7fe6d26b1e40] __libc_start_main\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m ray::ImplicitFunc() [0x58c1ee]\n",
      "\u001b[2m\u001b[36m(func pid=3737)\u001b[0m \n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/multiprocessing/resource_sharer.py\", line 138, in _serve\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m     with self._listener.accept() as conn:\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 466, in accept\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m     answer_challenge(c, self._authkey)\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 752, in answer_challenge\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m     message = connection.recv_bytes(256)         # reject large message\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m     buf = self._recv_bytes(maxlength)\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m     buf = self._recv(4)\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m   File \"/home/kingstar/anaconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m     chunk = read(handle, remaining)\n",
      "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-03-15 11:18:53 (running for 00:01:40.35)\n",
      "Memory usage on this node: 5.8/15.6 GiB \n",
      "Using AsyncHyperBand: num_stopped=3\n",
      "Bracket: Iter 8.000: None | Iter 4.000: -1.5010592198371888 | Iter 2.000: -1.9317257415771485 | Iter 1.000: -2.24250205116272\n",
      "Resources requested: 12.0/12 CPUs, 0/0 GPUs, 0.0/8.34 GiB heap, 0.0/4.17 GiB objects\n",
      "Result logdir: /home/kingstar/ray_results/train_cifar_2023-03-15_11-17-12\n",
      "Number of trials: 10/10 (1 PENDING, 6 RUNNING, 3 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_7f5ac_00002 | RUNNING    | 172.23.199.201:3731 |           16 |   32 |   64 | 0.000617828 | 1.39431 |     0.5002 |                    6 |\n",
      "| train_cifar_7f5ac_00003 | RUNNING    | 172.23.199.201:3733 |            2 |   16 |  128 | 0.00224033  | 1.66019 |     0.4036 |                    1 |\n",
      "| train_cifar_7f5ac_00005 | RUNNING    | 172.23.199.201:3737 |            8 |  256 |   16 | 0.000194578 | 1.71061 |     0.3787 |                    3 |\n",
      "| train_cifar_7f5ac_00006 | RUNNING    | 172.23.199.201:3668 |            4 |   64 |    4 | 0.00106146  | 1.80031 |     0.288  |                    1 |\n",
      "| train_cifar_7f5ac_00007 | RUNNING    | 172.23.199.201:3735 |            2 |  256 |    4 | 0.0127225   |         |            |                      |\n",
      "| train_cifar_7f5ac_00008 | RUNNING    | 172.23.199.201:3729 |            4 |  256 |    4 | 0.00447093  |         |            |                      |\n",
      "| train_cifar_7f5ac_00009 | PENDING    |                     |           16 |   64 |   64 | 0.000208139 |         |            |                      |\n",
      "| train_cifar_7f5ac_00000 | TERMINATED | 172.23.199.201:3668 |            8 |   32 |    8 | 0.000178243 | 2.30576 |     0.0977 |                    1 |\n",
      "| train_cifar_7f5ac_00001 | TERMINATED | 172.23.199.201:3729 |            8 |    8 |  256 | 0.000102774 | 2.07262 |     0.2231 |                    2 |\n",
      "| train_cifar_7f5ac_00004 | TERMINATED | 172.23.199.201:3735 |            8 |   32 |    8 | 0.0966673   | 2.3294  |     0.0992 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "Best trial config: {'l1': 32, 'l2': 64, 'lr': 0.0006178279487392563, 'batch_size': 16}\n",
      "Best trial final validation loss: 1.3943120020866393\n",
      "Best trial final validation accuracy: 0.5002\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_TrackedCheckpoint' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# You can change the number of GPUs per trial here:\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlogin(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0a25ae829bf4e2a6cd2acfdd4e65e6a26cd9927e\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpus_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 55\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(num_samples, max_num_epochs, gpus_per_trial)\u001b[0m\n\u001b[1;32m     52\u001b[0m         best_trained_model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDataParallel(best_trained_model)\n\u001b[1;32m     53\u001b[0m best_trained_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 55\u001b[0m best_checkpoint_dir \u001b[38;5;241m=\u001b[39m \u001b[43mbest_trial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\n\u001b[1;32m     56\u001b[0m model_state, optimizer_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     57\u001b[0m     best_checkpoint_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     58\u001b[0m best_trained_model\u001b[38;5;241m.\u001b[39mload_state_dict(model_state)\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_TrackedCheckpoint' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "\n",
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n",
    "    \n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    load_data(data_dir)\n",
    "    \n",
    "    # (중요도 1순위) config에 search space를 지정\n",
    "    config = {\n",
    "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([2, 4, 8, 16])\n",
    "    }\n",
    "    \n",
    "    # (중요도 2순위) 학습 스케쥴링 알고리즘 지정\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    \n",
    "    # 결과 출력 양식 지정\n",
    "    reporter = CLIReporter(\n",
    "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    \n",
    "    # (중요도 3순위) 병렬 처리 방식으로 학습 수행하도록 설정\n",
    "    # 위는 결국 전부 이 tune.run()을 실행하기 위한 과정들\n",
    "    result = tune.run(\n",
    "        partial(train_cifar, data_dir=data_dir),\n",
    "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "    best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    model_state, optimizer_state = torch.load(os.path.join(\n",
    "        best_checkpoint_dir, \"checkpoint\"))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # You can change the number of GPUs per trial here:\n",
    "    wandb.login(key=\"0a25ae829bf4e2a6cd2acfdd4e65e6a26cd9927e\")\n",
    "    main(num_samples=10, max_num_epochs=10, gpus_per_trial=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
